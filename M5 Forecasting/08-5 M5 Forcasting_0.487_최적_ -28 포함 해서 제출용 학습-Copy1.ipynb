{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "calendar = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "price = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "# df_test = pd.read_csv('m5-forecasting-accuracy/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30490/30490 [00:36<00:00, 841.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# # startpoints 찾아서 이전 데이터 지우기\n",
    "startpoints = np.zeros(df_train.shape[0])\n",
    "for idx in tqdm(range(df_train.shape[0])):\n",
    "    startpoints[idx]= np.where(df_train.iloc[idx,6:].values>0)[0].min().astype(int)\n",
    "start_dict = dict(zip(df_train['id'], startpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = []\n",
    "cat_cols = []\n",
    "drop_cols += ['date','d','id']\n",
    "tr_last = 1913\n",
    "# F_1~28 만들기  1914~1941 까지 \n",
    "for i in range(tr_last+1, tr_last+1+28):   df_train['d_%s'%i] = 0\n",
    "\n",
    "# # Unpivot\n",
    "df_train = pd.melt(df_train, id_vars=df_train.columns[:6], value_vars=df_train.columns[6:],\n",
    "       var_name = 'day', value_name = 'volume')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, calendar, left_on = 'day', right_on ='d')\n",
    "# snap 합치기\n",
    "snap = np.zeros(df_train.shape[0])\n",
    "snap[df_train[(df_train['state_id']=='CA')&(df_train['snap_CA']==1)].index] +=1\n",
    "snap[df_train[(df_train['state_id']=='TX')&(df_train['snap_TX']==1)].index] +=1\n",
    "snap[df_train[(df_train['state_id']=='WI')&(df_train['snap_WI']==1)].index] +=1\n",
    "df_train['snap'] = snap\n",
    "drop_cols += ['snap_CA','snap_TX','snap_WI']\n",
    "\n",
    "\n",
    "cat_cols += [ 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "#               'wday', 'month', 'year', # 이게 크리티컬?\n",
    "            'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap']\n",
    "\n",
    "\n",
    "# ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     46816555\n",
      "False       65122\n",
      "Name: startpoints, dtype: int64\n",
      "(46816555, 26)\n"
     ]
    }
   ],
   "source": [
    "# Sell price\n",
    "df_train.head()\n",
    "df_train = pd.merge(df_train, price)\n",
    "\n",
    "# # Start point 찾기 ::  0.1% 데이터를 날릴  수 있다. \n",
    "\n",
    "df_train['startpoint'] = df_train['id'].map(start_dict).astype(int)#.astype(str)\n",
    "df_train['startpoints'] = df_train['day'].str.slice(start=2).astype(int) >=df_train['startpoint']\n",
    "print(df_train['startpoints'].value_counts())\n",
    "df_train = df_train[df_train['startpoints']]\n",
    "print(df_train.shape)\n",
    "df_train.drop(['startpoint','startpoints'],axis =1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 왜 Shift를 해야할까?\n",
    "# Shift 28을 하지 않으면 예측값이 뒤로 가면갈 수록  F1->F28 예측 할 수 있는 변수가 줄어든게 된다.\n",
    "# 28일은 한달을 의미한다. 최근 한달간의 경향성을 보는 것으로 보면 되겠다.\n",
    "# 28일을 56일로 늘리면 안되나? - 최근 한달간의 경향성이 반영이 안되는 것이낙?\n",
    "# https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50\n",
    "\n",
    "\n",
    "# df_train['volume_1'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(1)\n",
    "# df_train['volume_2'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(2)\n",
    "# df_train['volume_3'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(3)\n",
    "\n",
    "\n",
    "df_train['volume_7'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(7)\n",
    "df_train['volume_28'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(28)\n",
    "\n",
    "print(\"mean\")\n",
    "\n",
    "df_train['rmean_7_7'] = df_train[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(7).mean())\n",
    "df_train['rmean_7_28'] = df_train[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(28).mean())\n",
    "df_train['rmean_7_50'] = df_train[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "df_train['rmean_28_7'] = df_train[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(7).mean())\n",
    "df_train['rmean_28_28'] = df_train[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(28).mean())\n",
    "df_train['rmean_28_50'] = df_train[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "# # print(\"std\")\n",
    "# # full_df['rstd_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7 ,min_periods=1).std())\n",
    "# # full_df['rstd_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28 ,min_periods=1).std())\n",
    "# # full_df['rstd_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(50 ,min_periods=1).std())\n",
    "\n",
    "# # print(\"max\")\n",
    "# # full_df['rmax_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7 ,min_periods=1).max())\n",
    "# # full_df['rmax_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28 ,min_periods=1).max())\n",
    "# # full_df['rmax_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(50 ,min_periods=1).max())\n",
    "\n",
    "# # print(\"min\")\n",
    "# # full_df['rmin_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7 ,min_periods=1).min())\n",
    "# # full_df['rmin_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28 ,min_periods=1).min())\n",
    "# # full_df['rmin_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(50 ,min_periods=1).min())\n",
    "\n",
    "# # print(\"count\")\n",
    "# # full_df['rcnt_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7).count() if x>0).fllna(0)\n",
    "# # full_df['rcnt_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28).count() if x>0).fllna(0)\n",
    "# # full_df['rcnt_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28).count() if x>0).fllna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] =  pd.to_datetime(df_train[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['week'] = getattr(df_train[\"date\"].dt, \"weekofyear\").astype(\"int16\")\n",
    "df_train['quarter'] = getattr(df_train[\"date\"].dt,\"quarter\").astype(\"int16\")\n",
    "df_train['mday'] = getattr(df_train[\"date\"].dt, \"day\").astype(\"int16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46816555, 35)\n"
     ]
    }
   ],
   "source": [
    "cols =['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "df_train[cols]= df_train[cols].fillna('NaN')\n",
    "print(df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44468825, 35)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'd', 'id', 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk', 'weekday']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols += [\"wm_yr_wk\", \"weekday\"]  ## 이게 문제?\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "tr_last = 1913\n",
    "testday = ['d_%s'% x for x in range(tr_last+1, tr_last+1+28)]\n",
    "train_id = df_train['id']\n",
    "df_test_id = df_train[df_train['day'].isin(testday)]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df_train.drop(drop_cols,axis =1 )\n",
    "# df_test =df_test.drop(drop_cols,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_7_50</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>rmean_28_50</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010975</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.90</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010976</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.66</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010977</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010978</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_81</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010979</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_82</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               item_id    dept_id   cat_id store_id state_id   day  volume  \\\n",
       "1010975  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_78       0   \n",
       "1010976  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_79       0   \n",
       "1010977  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_80       0   \n",
       "1010978  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_81       5   \n",
       "1010979  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_82      23   \n",
       "\n",
       "         wday  month  year  ... volume_28 rmean_7_7 rmean_7_28 rmean_7_50  \\\n",
       "1010975     1      4  2011  ...       0.0  6.857143   1.785714       1.88   \n",
       "1010976     2      4  2011  ...       0.0  4.000000   1.714286       1.68   \n",
       "1010977     3      4  2011  ...       0.0  5.142857   2.000000       1.76   \n",
       "1010978     4      4  2011  ...       0.0  5.714286   2.214286       1.80   \n",
       "1010979     5      4  2011  ...       0.0  4.714286   2.428571       1.88   \n",
       "\n",
       "         rmean_28_7  rmean_28_28  rmean_28_50  week  quarter  mday  \n",
       "1010975    0.285714     1.285714         2.90    15        2    16  \n",
       "1010976    0.000000     1.142857         2.66    15        2    17  \n",
       "1010977    0.000000     1.000000         2.36    16        2    18  \n",
       "1010978    0.000000     0.928571         2.36    16        2    19  \n",
       "1010979    0.000000     0.714286         2.36    16        2    20  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:06<00:00,  6.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Encoding\n",
    "for col in tqdm(cat_cols) :  # encoding -1이 문제?\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col]).astype(np.int8)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 분리하기\n",
    "testday = ['d_%s'% x for x in range(tr_last+1, tr_last+1+28)]\n",
    "df_test = df_train.copy()\n",
    "df_train = df_train[~df_train['day'].isin(testday)]\n",
    "# train_col = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day', 'volume', 'wday', 'month', 'year',\n",
    "#  'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap', 'sell_price', 'volume_7', 'volume_28',\n",
    "#  'rmean_7_7', 'rmean_7_28', 'rmean_7_50', 'rmean_28_7', 'rmean_28_28', 'rmean_28_50', 'week', 'quarter', 'mday']\n",
    "# df_train = df_train.loc[:,train_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_7_50</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>rmean_28_50</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010975</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.90</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010976</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.66</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010977</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010978</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_81</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010979</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_82</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  dept_id  cat_id  store_id  state_id   day  volume  wday  \\\n",
       "1010975      -92        3       1         0         0  d_78       0     1   \n",
       "1010976      -92        3       1         0         0  d_79       0     2   \n",
       "1010977      -92        3       1         0         0  d_80       0     3   \n",
       "1010978      -92        3       1         0         0  d_81       5     4   \n",
       "1010979      -92        3       1         0         0  d_82      23     5   \n",
       "\n",
       "         month  year  ...  volume_28  rmean_7_7  rmean_7_28  rmean_7_50  \\\n",
       "1010975      4  2011  ...        0.0   6.857143    1.785714        1.88   \n",
       "1010976      4  2011  ...        0.0   4.000000    1.714286        1.68   \n",
       "1010977      4  2011  ...        0.0   5.142857    2.000000        1.76   \n",
       "1010978      4  2011  ...        0.0   5.714286    2.214286        1.80   \n",
       "1010979      4  2011  ...        0.0   4.714286    2.428571        1.88   \n",
       "\n",
       "         rmean_28_7  rmean_28_28  rmean_28_50  week  quarter  mday  \n",
       "1010975    0.285714     1.285714         2.90    15        2    16  \n",
       "1010976    0.000000     1.142857         2.66    15        2    17  \n",
       "1010977    0.000000     1.000000         2.36    16        2    18  \n",
       "1010978    0.000000     0.928571         2.36    16        2    19  \n",
       "1010979    0.000000     0.714286         2.36    16        2    20  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train , x_valid = train_test_split(df_train, test_size =0.15, random_state = 99)\n",
    "# y_train, y_valid = x_train['volume'], x_valid['volume']\n",
    "\n",
    "# x_train = x_train.drop(['day','volume'], axis =1)\n",
    "# x_valid = x_valid.drop(['day','volume'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_7_50</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>rmean_28_50</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010975</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.90</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010976</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.66</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010977</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010978</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_81</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010979</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_82</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  dept_id  cat_id  store_id  state_id   day  volume  wday  \\\n",
       "1010975      -92        3       1         0         0  d_78       0     1   \n",
       "1010976      -92        3       1         0         0  d_79       0     2   \n",
       "1010977      -92        3       1         0         0  d_80       0     3   \n",
       "1010978      -92        3       1         0         0  d_81       5     4   \n",
       "1010979      -92        3       1         0         0  d_82      23     5   \n",
       "\n",
       "         month  year  ...  volume_28  rmean_7_7  rmean_7_28  rmean_7_50  \\\n",
       "1010975      4  2011  ...        0.0   6.857143    1.785714        1.88   \n",
       "1010976      4  2011  ...        0.0   4.000000    1.714286        1.68   \n",
       "1010977      4  2011  ...        0.0   5.142857    2.000000        1.76   \n",
       "1010978      4  2011  ...        0.0   5.714286    2.214286        1.80   \n",
       "1010979      4  2011  ...        0.0   4.714286    2.428571        1.88   \n",
       "\n",
       "         rmean_28_7  rmean_28_28  rmean_28_50  week  quarter  mday  \n",
       "1010975    0.285714     1.285714         2.90    15        2    16  \n",
       "1010976    0.000000     1.142857         2.66    15        2    17  \n",
       "1010977    0.000000     1.000000         2.36    16        2    18  \n",
       "1010978    0.000000     0.928571         2.36    16        2    19  \n",
       "1010979    0.000000     0.714286         2.36    16        2    20  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 마지막 28일 데이터 빼고 학습\n",
    "# cols=[f\"d_{i}\" for i in range(1913-28,1914)]\n",
    "# df_valid = df_train[df_train['day'].isin(cols)]\n",
    "# df_train = df_train[~df_train['day'].isin(cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [999,243]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"objective\" : \"poisson\",\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.075,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "        \"sub_row\" : 0.75,\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "#         \"nthread\" : 4\n",
    "        \"metric\": [\"rmse\"],\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 5000,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "        'n_jobs' :5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.59032\tvalid_1's rmse: 2.59313\n",
      "[100]\ttraining's rmse: 2.50193\tvalid_1's rmse: 2.5129\n",
      "[150]\ttraining's rmse: 2.47978\tvalid_1's rmse: 2.49436\n",
      "[200]\ttraining's rmse: 2.46036\tvalid_1's rmse: 2.47911\n",
      "[250]\ttraining's rmse: 2.44247\tvalid_1's rmse: 2.46628\n",
      "[300]\ttraining's rmse: 2.42794\tvalid_1's rmse: 2.45628\n",
      "[350]\ttraining's rmse: 2.4135\tvalid_1's rmse: 2.44699\n",
      "[400]\ttraining's rmse: 2.40116\tvalid_1's rmse: 2.43979\n",
      "[450]\ttraining's rmse: 2.39007\tvalid_1's rmse: 2.43372\n",
      "[500]\ttraining's rmse: 2.37994\tvalid_1's rmse: 2.42816\n",
      "[550]\ttraining's rmse: 2.37106\tvalid_1's rmse: 2.42428\n",
      "[600]\ttraining's rmse: 2.362\tvalid_1's rmse: 2.42039\n",
      "[650]\ttraining's rmse: 2.35373\tvalid_1's rmse: 2.41681\n",
      "[700]\ttraining's rmse: 2.34668\tvalid_1's rmse: 2.41382\n",
      "[750]\ttraining's rmse: 2.33994\tvalid_1's rmse: 2.41105\n",
      "[800]\ttraining's rmse: 2.33291\tvalid_1's rmse: 2.40876\n",
      "[850]\ttraining's rmse: 2.32608\tvalid_1's rmse: 2.40635\n",
      "[900]\ttraining's rmse: 2.31981\tvalid_1's rmse: 2.40376\n",
      "[950]\ttraining's rmse: 2.31396\tvalid_1's rmse: 2.40176\n",
      "[1000]\ttraining's rmse: 2.30866\tvalid_1's rmse: 2.40003\n",
      "[1050]\ttraining's rmse: 2.30294\tvalid_1's rmse: 2.3978\n",
      "[1100]\ttraining's rmse: 2.2976\tvalid_1's rmse: 2.39602\n",
      "[1150]\ttraining's rmse: 2.29299\tvalid_1's rmse: 2.39485\n",
      "[1200]\ttraining's rmse: 2.28845\tvalid_1's rmse: 2.39329\n",
      "[1250]\ttraining's rmse: 2.28321\tvalid_1's rmse: 2.3918\n",
      "[1300]\ttraining's rmse: 2.27811\tvalid_1's rmse: 2.39012\n",
      "[1350]\ttraining's rmse: 2.27417\tvalid_1's rmse: 2.38905\n",
      "[1400]\ttraining's rmse: 2.26981\tvalid_1's rmse: 2.38763\n",
      "[1450]\ttraining's rmse: 2.26516\tvalid_1's rmse: 2.38624\n",
      "[1500]\ttraining's rmse: 2.2612\tvalid_1's rmse: 2.38524\n",
      "[1550]\ttraining's rmse: 2.25725\tvalid_1's rmse: 2.38412\n",
      "[1600]\ttraining's rmse: 2.25348\tvalid_1's rmse: 2.38318\n",
      "[1650]\ttraining's rmse: 2.2491\tvalid_1's rmse: 2.38213\n",
      "[1700]\ttraining's rmse: 2.2451\tvalid_1's rmse: 2.38125\n",
      "[1750]\ttraining's rmse: 2.24121\tvalid_1's rmse: 2.37975\n",
      "[1800]\ttraining's rmse: 2.23751\tvalid_1's rmse: 2.37872\n",
      "[1850]\ttraining's rmse: 2.23373\tvalid_1's rmse: 2.37756\n",
      "[1900]\ttraining's rmse: 2.23063\tvalid_1's rmse: 2.37664\n",
      "[1950]\ttraining's rmse: 2.22736\tvalid_1's rmse: 2.37589\n",
      "[2000]\ttraining's rmse: 2.2245\tvalid_1's rmse: 2.37519\n",
      "[2050]\ttraining's rmse: 2.2216\tvalid_1's rmse: 2.37438\n",
      "[2100]\ttraining's rmse: 2.21789\tvalid_1's rmse: 2.37338\n",
      "[2150]\ttraining's rmse: 2.21478\tvalid_1's rmse: 2.37292\n",
      "[2200]\ttraining's rmse: 2.21183\tvalid_1's rmse: 2.37231\n",
      "[2250]\ttraining's rmse: 2.20927\tvalid_1's rmse: 2.37168\n",
      "[2300]\ttraining's rmse: 2.2063\tvalid_1's rmse: 2.37097\n",
      "[2350]\ttraining's rmse: 2.20345\tvalid_1's rmse: 2.37042\n",
      "[2400]\ttraining's rmse: 2.20018\tvalid_1's rmse: 2.36953\n",
      "[2450]\ttraining's rmse: 2.19666\tvalid_1's rmse: 2.3689\n",
      "[2500]\ttraining's rmse: 2.1939\tvalid_1's rmse: 2.36853\n",
      "[2550]\ttraining's rmse: 2.19119\tvalid_1's rmse: 2.36784\n",
      "[2600]\ttraining's rmse: 2.18866\tvalid_1's rmse: 2.36721\n",
      "[2650]\ttraining's rmse: 2.18641\tvalid_1's rmse: 2.36672\n",
      "[2700]\ttraining's rmse: 2.18368\tvalid_1's rmse: 2.36603\n",
      "[2750]\ttraining's rmse: 2.18127\tvalid_1's rmse: 2.36551\n",
      "[2800]\ttraining's rmse: 2.17863\tvalid_1's rmse: 2.3651\n",
      "[2850]\ttraining's rmse: 2.17632\tvalid_1's rmse: 2.36463\n",
      "[2900]\ttraining's rmse: 2.17387\tvalid_1's rmse: 2.36389\n",
      "[2950]\ttraining's rmse: 2.17168\tvalid_1's rmse: 2.36354\n",
      "[3000]\ttraining's rmse: 2.16884\tvalid_1's rmse: 2.36265\n",
      "[3050]\ttraining's rmse: 2.16666\tvalid_1's rmse: 2.36227\n",
      "[3100]\ttraining's rmse: 2.16407\tvalid_1's rmse: 2.36185\n",
      "[3150]\ttraining's rmse: 2.1612\tvalid_1's rmse: 2.36117\n",
      "[3200]\ttraining's rmse: 2.15869\tvalid_1's rmse: 2.3608\n",
      "[3250]\ttraining's rmse: 2.15607\tvalid_1's rmse: 2.36015\n",
      "[3300]\ttraining's rmse: 2.15366\tvalid_1's rmse: 2.35974\n",
      "[3350]\ttraining's rmse: 2.15185\tvalid_1's rmse: 2.35939\n",
      "[3400]\ttraining's rmse: 2.14906\tvalid_1's rmse: 2.35884\n",
      "[3450]\ttraining's rmse: 2.14672\tvalid_1's rmse: 2.35823\n",
      "[3500]\ttraining's rmse: 2.14477\tvalid_1's rmse: 2.35799\n",
      "[3550]\ttraining's rmse: 2.14247\tvalid_1's rmse: 2.35768\n",
      "[3600]\ttraining's rmse: 2.14028\tvalid_1's rmse: 2.35728\n",
      "[3650]\ttraining's rmse: 2.13837\tvalid_1's rmse: 2.3569\n",
      "[3700]\ttraining's rmse: 2.13642\tvalid_1's rmse: 2.35651\n",
      "[3750]\ttraining's rmse: 2.13457\tvalid_1's rmse: 2.35616\n",
      "[3800]\ttraining's rmse: 2.13216\tvalid_1's rmse: 2.35568\n",
      "[3850]\ttraining's rmse: 2.13025\tvalid_1's rmse: 2.3555\n",
      "[3900]\ttraining's rmse: 2.12809\tvalid_1's rmse: 2.35509\n",
      "[3950]\ttraining's rmse: 2.12585\tvalid_1's rmse: 2.35456\n",
      "[4000]\ttraining's rmse: 2.12385\tvalid_1's rmse: 2.35415\n",
      "[4050]\ttraining's rmse: 2.122\tvalid_1's rmse: 2.35368\n",
      "[4100]\ttraining's rmse: 2.12025\tvalid_1's rmse: 2.35346\n",
      "[4150]\ttraining's rmse: 2.11825\tvalid_1's rmse: 2.35305\n",
      "[4200]\ttraining's rmse: 2.11636\tvalid_1's rmse: 2.35251\n",
      "[4250]\ttraining's rmse: 2.11448\tvalid_1's rmse: 2.35221\n",
      "[4300]\ttraining's rmse: 2.11269\tvalid_1's rmse: 2.35197\n",
      "[4350]\ttraining's rmse: 2.11067\tvalid_1's rmse: 2.35153\n",
      "[4400]\ttraining's rmse: 2.10881\tvalid_1's rmse: 2.3511\n",
      "[4450]\ttraining's rmse: 2.10662\tvalid_1's rmse: 2.35056\n",
      "[4500]\ttraining's rmse: 2.10469\tvalid_1's rmse: 2.35018\n",
      "[4550]\ttraining's rmse: 2.10286\tvalid_1's rmse: 2.34992\n",
      "[4600]\ttraining's rmse: 2.10121\tvalid_1's rmse: 2.34969\n",
      "[4650]\ttraining's rmse: 2.09943\tvalid_1's rmse: 2.34924\n",
      "[4700]\ttraining's rmse: 2.0979\tvalid_1's rmse: 2.34901\n",
      "[4750]\ttraining's rmse: 2.09629\tvalid_1's rmse: 2.34866\n",
      "[4800]\ttraining's rmse: 2.09448\tvalid_1's rmse: 2.34834\n",
      "[4850]\ttraining's rmse: 2.09284\tvalid_1's rmse: 2.34801\n",
      "[4900]\ttraining's rmse: 2.09119\tvalid_1's rmse: 2.34783\n",
      "[4950]\ttraining's rmse: 2.08938\tvalid_1's rmse: 2.34755\n",
      "[5000]\ttraining's rmse: 2.08764\tvalid_1's rmse: 2.34712\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08764\tvalid_1's rmse: 2.34712\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.56656\tvalid_1's rmse: 2.59632\n",
      "[100]\ttraining's rmse: 2.49561\tvalid_1's rmse: 2.5287\n",
      "[150]\ttraining's rmse: 2.4734\tvalid_1's rmse: 2.51081\n",
      "[200]\ttraining's rmse: 2.45247\tvalid_1's rmse: 2.49493\n",
      "[250]\ttraining's rmse: 2.4335\tvalid_1's rmse: 2.48062\n",
      "[300]\ttraining's rmse: 2.41834\tvalid_1's rmse: 2.47067\n",
      "[350]\ttraining's rmse: 2.40367\tvalid_1's rmse: 2.46113\n",
      "[400]\ttraining's rmse: 2.39112\tvalid_1's rmse: 2.45339\n",
      "[450]\ttraining's rmse: 2.37826\tvalid_1's rmse: 2.44593\n",
      "[500]\ttraining's rmse: 2.3688\tvalid_1's rmse: 2.44113\n",
      "[550]\ttraining's rmse: 2.36059\tvalid_1's rmse: 2.43744\n",
      "[600]\ttraining's rmse: 2.35125\tvalid_1's rmse: 2.43334\n",
      "[650]\ttraining's rmse: 2.34466\tvalid_1's rmse: 2.43045\n",
      "[700]\ttraining's rmse: 2.33737\tvalid_1's rmse: 2.42715\n",
      "[750]\ttraining's rmse: 2.33143\tvalid_1's rmse: 2.4251\n",
      "[800]\ttraining's rmse: 2.32635\tvalid_1's rmse: 2.42342\n",
      "[850]\ttraining's rmse: 2.31951\tvalid_1's rmse: 2.42061\n",
      "[900]\ttraining's rmse: 2.31414\tvalid_1's rmse: 2.41849\n",
      "[950]\ttraining's rmse: 2.30811\tvalid_1's rmse: 2.41646\n",
      "[1000]\ttraining's rmse: 2.30183\tvalid_1's rmse: 2.41443\n",
      "[1050]\ttraining's rmse: 2.29777\tvalid_1's rmse: 2.41324\n",
      "[1100]\ttraining's rmse: 2.2924\tvalid_1's rmse: 2.41135\n",
      "[1150]\ttraining's rmse: 2.28744\tvalid_1's rmse: 2.40981\n",
      "[1200]\ttraining's rmse: 2.28323\tvalid_1's rmse: 2.40816\n",
      "[1250]\ttraining's rmse: 2.27865\tvalid_1's rmse: 2.4069\n",
      "[1300]\ttraining's rmse: 2.2744\tvalid_1's rmse: 2.40549\n",
      "[1350]\ttraining's rmse: 2.26981\tvalid_1's rmse: 2.40366\n",
      "[1400]\ttraining's rmse: 2.26611\tvalid_1's rmse: 2.40257\n",
      "[1450]\ttraining's rmse: 2.2617\tvalid_1's rmse: 2.40117\n",
      "[1500]\ttraining's rmse: 2.25724\tvalid_1's rmse: 2.39991\n",
      "[1550]\ttraining's rmse: 2.25354\tvalid_1's rmse: 2.399\n",
      "[1600]\ttraining's rmse: 2.25011\tvalid_1's rmse: 2.39807\n",
      "[1650]\ttraining's rmse: 2.24629\tvalid_1's rmse: 2.39701\n",
      "[1700]\ttraining's rmse: 2.24301\tvalid_1's rmse: 2.3961\n",
      "[1750]\ttraining's rmse: 2.23895\tvalid_1's rmse: 2.39498\n",
      "[1800]\ttraining's rmse: 2.23585\tvalid_1's rmse: 2.39424\n",
      "[1850]\ttraining's rmse: 2.23229\tvalid_1's rmse: 2.39342\n",
      "[1900]\ttraining's rmse: 2.22886\tvalid_1's rmse: 2.39232\n",
      "[1950]\ttraining's rmse: 2.22573\tvalid_1's rmse: 2.39174\n",
      "[2000]\ttraining's rmse: 2.22235\tvalid_1's rmse: 2.39101\n",
      "[2050]\ttraining's rmse: 2.21946\tvalid_1's rmse: 2.39009\n",
      "[2100]\ttraining's rmse: 2.2162\tvalid_1's rmse: 2.389\n",
      "[2150]\ttraining's rmse: 2.21345\tvalid_1's rmse: 2.38849\n",
      "[2200]\ttraining's rmse: 2.21069\tvalid_1's rmse: 2.38805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.20737\tvalid_1's rmse: 2.38731\n",
      "[2300]\ttraining's rmse: 2.20437\tvalid_1's rmse: 2.38683\n",
      "[2350]\ttraining's rmse: 2.20196\tvalid_1's rmse: 2.38635\n",
      "[2400]\ttraining's rmse: 2.19952\tvalid_1's rmse: 2.38594\n",
      "[2450]\ttraining's rmse: 2.19683\tvalid_1's rmse: 2.38539\n",
      "[2500]\ttraining's rmse: 2.19407\tvalid_1's rmse: 2.3847\n",
      "[2550]\ttraining's rmse: 2.19125\tvalid_1's rmse: 2.38417\n",
      "[2600]\ttraining's rmse: 2.18851\tvalid_1's rmse: 2.38359\n",
      "[2650]\ttraining's rmse: 2.18564\tvalid_1's rmse: 2.38275\n",
      "[2700]\ttraining's rmse: 2.18278\tvalid_1's rmse: 2.38223\n",
      "[2750]\ttraining's rmse: 2.18038\tvalid_1's rmse: 2.38179\n",
      "[2800]\ttraining's rmse: 2.17784\tvalid_1's rmse: 2.3813\n",
      "[2850]\ttraining's rmse: 2.17539\tvalid_1's rmse: 2.38082\n",
      "[2900]\ttraining's rmse: 2.17267\tvalid_1's rmse: 2.38027\n",
      "[2950]\ttraining's rmse: 2.17046\tvalid_1's rmse: 2.37979\n",
      "[3000]\ttraining's rmse: 2.16777\tvalid_1's rmse: 2.3791\n",
      "[3050]\ttraining's rmse: 2.16549\tvalid_1's rmse: 2.37883\n",
      "[3100]\ttraining's rmse: 2.16317\tvalid_1's rmse: 2.37829\n",
      "[3150]\ttraining's rmse: 2.1608\tvalid_1's rmse: 2.37768\n",
      "[3200]\ttraining's rmse: 2.15854\tvalid_1's rmse: 2.3772\n",
      "[3250]\ttraining's rmse: 2.15624\tvalid_1's rmse: 2.37675\n",
      "[3300]\ttraining's rmse: 2.15376\tvalid_1's rmse: 2.37613\n",
      "[3350]\ttraining's rmse: 2.15167\tvalid_1's rmse: 2.37579\n",
      "[3400]\ttraining's rmse: 2.1495\tvalid_1's rmse: 2.37531\n",
      "[3450]\ttraining's rmse: 2.14696\tvalid_1's rmse: 2.37478\n",
      "[3500]\ttraining's rmse: 2.14453\tvalid_1's rmse: 2.37425\n",
      "[3550]\ttraining's rmse: 2.14221\tvalid_1's rmse: 2.37384\n",
      "[3600]\ttraining's rmse: 2.14032\tvalid_1's rmse: 2.37339\n",
      "[3650]\ttraining's rmse: 2.13855\tvalid_1's rmse: 2.3731\n",
      "[3700]\ttraining's rmse: 2.13614\tvalid_1's rmse: 2.37249\n",
      "[3750]\ttraining's rmse: 2.13395\tvalid_1's rmse: 2.37202\n",
      "[3800]\ttraining's rmse: 2.13187\tvalid_1's rmse: 2.37165\n",
      "[3850]\ttraining's rmse: 2.12996\tvalid_1's rmse: 2.37115\n",
      "[3900]\ttraining's rmse: 2.12812\tvalid_1's rmse: 2.37074\n",
      "[3950]\ttraining's rmse: 2.12635\tvalid_1's rmse: 2.37037\n",
      "[4000]\ttraining's rmse: 2.12458\tvalid_1's rmse: 2.37017\n",
      "[4050]\ttraining's rmse: 2.12244\tvalid_1's rmse: 2.36981\n",
      "[4100]\ttraining's rmse: 2.12029\tvalid_1's rmse: 2.36927\n",
      "[4150]\ttraining's rmse: 2.11813\tvalid_1's rmse: 2.36873\n",
      "[4200]\ttraining's rmse: 2.11631\tvalid_1's rmse: 2.36838\n",
      "[4250]\ttraining's rmse: 2.11432\tvalid_1's rmse: 2.36793\n",
      "[4300]\ttraining's rmse: 2.11245\tvalid_1's rmse: 2.36762\n",
      "[4350]\ttraining's rmse: 2.11106\tvalid_1's rmse: 2.36745\n",
      "[4400]\ttraining's rmse: 2.10919\tvalid_1's rmse: 2.36714\n",
      "[4450]\ttraining's rmse: 2.10752\tvalid_1's rmse: 2.36688\n",
      "[4500]\ttraining's rmse: 2.10579\tvalid_1's rmse: 2.36662\n",
      "[4550]\ttraining's rmse: 2.10436\tvalid_1's rmse: 2.36637\n",
      "[4600]\ttraining's rmse: 2.10269\tvalid_1's rmse: 2.36612\n",
      "[4650]\ttraining's rmse: 2.10075\tvalid_1's rmse: 2.36574\n",
      "[4700]\ttraining's rmse: 2.09914\tvalid_1's rmse: 2.36548\n",
      "[4750]\ttraining's rmse: 2.09724\tvalid_1's rmse: 2.36514\n",
      "[4800]\ttraining's rmse: 2.09571\tvalid_1's rmse: 2.36489\n",
      "[4850]\ttraining's rmse: 2.09399\tvalid_1's rmse: 2.36452\n",
      "[4900]\ttraining's rmse: 2.09223\tvalid_1's rmse: 2.36417\n",
      "[4950]\ttraining's rmse: 2.09043\tvalid_1's rmse: 2.36374\n",
      "[5000]\ttraining's rmse: 2.08876\tvalid_1's rmse: 2.36343\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08876\tvalid_1's rmse: 2.36343\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.56716\tvalid_1's rmse: 2.57557\n",
      "[100]\ttraining's rmse: 2.49404\tvalid_1's rmse: 2.5091\n",
      "[150]\ttraining's rmse: 2.47134\tvalid_1's rmse: 2.49039\n",
      "[200]\ttraining's rmse: 2.45271\tvalid_1's rmse: 2.47621\n",
      "[250]\ttraining's rmse: 2.43392\tvalid_1's rmse: 2.46268\n",
      "[300]\ttraining's rmse: 2.42058\tvalid_1's rmse: 2.45422\n",
      "[350]\ttraining's rmse: 2.40779\tvalid_1's rmse: 2.44623\n",
      "[400]\ttraining's rmse: 2.39442\tvalid_1's rmse: 2.43865\n",
      "[450]\ttraining's rmse: 2.38238\tvalid_1's rmse: 2.43201\n",
      "[500]\ttraining's rmse: 2.37339\tvalid_1's rmse: 2.42724\n",
      "[550]\ttraining's rmse: 2.36456\tvalid_1's rmse: 2.42324\n",
      "[600]\ttraining's rmse: 2.35459\tvalid_1's rmse: 2.41844\n",
      "[650]\ttraining's rmse: 2.34718\tvalid_1's rmse: 2.4154\n",
      "[700]\ttraining's rmse: 2.34009\tvalid_1's rmse: 2.41224\n",
      "[750]\ttraining's rmse: 2.33302\tvalid_1's rmse: 2.40892\n",
      "[800]\ttraining's rmse: 2.32614\tvalid_1's rmse: 2.40637\n",
      "[850]\ttraining's rmse: 2.32001\tvalid_1's rmse: 2.4046\n",
      "[900]\ttraining's rmse: 2.31447\tvalid_1's rmse: 2.40276\n",
      "[950]\ttraining's rmse: 2.30767\tvalid_1's rmse: 2.4002\n",
      "[1000]\ttraining's rmse: 2.30232\tvalid_1's rmse: 2.39836\n",
      "[1050]\ttraining's rmse: 2.29687\tvalid_1's rmse: 2.39669\n",
      "[1100]\ttraining's rmse: 2.29111\tvalid_1's rmse: 2.39518\n",
      "[1150]\ttraining's rmse: 2.28558\tvalid_1's rmse: 2.39357\n",
      "[1200]\ttraining's rmse: 2.28089\tvalid_1's rmse: 2.39178\n",
      "[1250]\ttraining's rmse: 2.27633\tvalid_1's rmse: 2.39051\n",
      "[1300]\ttraining's rmse: 2.27158\tvalid_1's rmse: 2.38876\n",
      "[1350]\ttraining's rmse: 2.268\tvalid_1's rmse: 2.38766\n",
      "[1400]\ttraining's rmse: 2.26395\tvalid_1's rmse: 2.38638\n",
      "[1450]\ttraining's rmse: 2.25943\tvalid_1's rmse: 2.38522\n",
      "[1500]\ttraining's rmse: 2.25499\tvalid_1's rmse: 2.38392\n",
      "[1550]\ttraining's rmse: 2.25103\tvalid_1's rmse: 2.38286\n",
      "[1600]\ttraining's rmse: 2.24687\tvalid_1's rmse: 2.38146\n",
      "[1650]\ttraining's rmse: 2.24305\tvalid_1's rmse: 2.38039\n",
      "[1700]\ttraining's rmse: 2.2396\tvalid_1's rmse: 2.37933\n",
      "[1750]\ttraining's rmse: 2.23607\tvalid_1's rmse: 2.37853\n",
      "[1800]\ttraining's rmse: 2.23265\tvalid_1's rmse: 2.37772\n",
      "[1850]\ttraining's rmse: 2.22902\tvalid_1's rmse: 2.37648\n",
      "[1900]\ttraining's rmse: 2.22572\tvalid_1's rmse: 2.37558\n",
      "[1950]\ttraining's rmse: 2.22231\tvalid_1's rmse: 2.37466\n",
      "[2000]\ttraining's rmse: 2.21928\tvalid_1's rmse: 2.37386\n",
      "[2050]\ttraining's rmse: 2.21633\tvalid_1's rmse: 2.3732\n",
      "[2100]\ttraining's rmse: 2.21288\tvalid_1's rmse: 2.37233\n",
      "[2150]\ttraining's rmse: 2.20987\tvalid_1's rmse: 2.37144\n",
      "[2200]\ttraining's rmse: 2.20699\tvalid_1's rmse: 2.37064\n",
      "[2250]\ttraining's rmse: 2.20363\tvalid_1's rmse: 2.36982\n",
      "[2300]\ttraining's rmse: 2.20061\tvalid_1's rmse: 2.36919\n",
      "[2350]\ttraining's rmse: 2.19796\tvalid_1's rmse: 2.36874\n",
      "[2400]\ttraining's rmse: 2.19524\tvalid_1's rmse: 2.36813\n",
      "[2450]\ttraining's rmse: 2.19213\tvalid_1's rmse: 2.36762\n",
      "[2500]\ttraining's rmse: 2.18933\tvalid_1's rmse: 2.36705\n",
      "[2550]\ttraining's rmse: 2.18666\tvalid_1's rmse: 2.36634\n",
      "[2600]\ttraining's rmse: 2.1839\tvalid_1's rmse: 2.36576\n",
      "[2650]\ttraining's rmse: 2.18106\tvalid_1's rmse: 2.36518\n",
      "[2700]\ttraining's rmse: 2.17894\tvalid_1's rmse: 2.36492\n",
      "[2750]\ttraining's rmse: 2.17555\tvalid_1's rmse: 2.36386\n",
      "[2800]\ttraining's rmse: 2.17275\tvalid_1's rmse: 2.3633\n",
      "[2850]\ttraining's rmse: 2.17021\tvalid_1's rmse: 2.36273\n",
      "[2900]\ttraining's rmse: 2.16771\tvalid_1's rmse: 2.36235\n",
      "[2950]\ttraining's rmse: 2.16529\tvalid_1's rmse: 2.3618\n",
      "[3000]\ttraining's rmse: 2.16295\tvalid_1's rmse: 2.36125\n",
      "[3050]\ttraining's rmse: 2.1607\tvalid_1's rmse: 2.3607\n",
      "[3100]\ttraining's rmse: 2.1583\tvalid_1's rmse: 2.36037\n",
      "[3150]\ttraining's rmse: 2.15612\tvalid_1's rmse: 2.35997\n",
      "[3200]\ttraining's rmse: 2.15438\tvalid_1's rmse: 2.35954\n",
      "[3250]\ttraining's rmse: 2.15195\tvalid_1's rmse: 2.35882\n",
      "[3300]\ttraining's rmse: 2.15011\tvalid_1's rmse: 2.35836\n",
      "[3350]\ttraining's rmse: 2.1479\tvalid_1's rmse: 2.35796\n",
      "[3400]\ttraining's rmse: 2.14556\tvalid_1's rmse: 2.35758\n",
      "[3450]\ttraining's rmse: 2.14367\tvalid_1's rmse: 2.35708\n",
      "[3500]\ttraining's rmse: 2.14161\tvalid_1's rmse: 2.35692\n",
      "[3550]\ttraining's rmse: 2.13958\tvalid_1's rmse: 2.35654\n",
      "[3600]\ttraining's rmse: 2.13734\tvalid_1's rmse: 2.35598\n",
      "[3650]\ttraining's rmse: 2.1353\tvalid_1's rmse: 2.35563\n",
      "[3700]\ttraining's rmse: 2.13333\tvalid_1's rmse: 2.35522\n",
      "[3750]\ttraining's rmse: 2.13122\tvalid_1's rmse: 2.35476\n",
      "[3800]\ttraining's rmse: 2.12911\tvalid_1's rmse: 2.35432\n",
      "[3850]\ttraining's rmse: 2.12661\tvalid_1's rmse: 2.35362\n",
      "[3900]\ttraining's rmse: 2.12488\tvalid_1's rmse: 2.35327\n",
      "[3950]\ttraining's rmse: 2.12267\tvalid_1's rmse: 2.35284\n",
      "[4000]\ttraining's rmse: 2.12083\tvalid_1's rmse: 2.35244\n",
      "[4050]\ttraining's rmse: 2.11862\tvalid_1's rmse: 2.35195\n",
      "[4100]\ttraining's rmse: 2.11694\tvalid_1's rmse: 2.35158\n",
      "[4150]\ttraining's rmse: 2.11475\tvalid_1's rmse: 2.35105\n",
      "[4200]\ttraining's rmse: 2.11266\tvalid_1's rmse: 2.35073\n",
      "[4250]\ttraining's rmse: 2.11113\tvalid_1's rmse: 2.35041\n",
      "[4300]\ttraining's rmse: 2.10902\tvalid_1's rmse: 2.35011\n",
      "[4350]\ttraining's rmse: 2.10706\tvalid_1's rmse: 2.34993\n",
      "[4400]\ttraining's rmse: 2.10541\tvalid_1's rmse: 2.34968\n",
      "[4450]\ttraining's rmse: 2.10362\tvalid_1's rmse: 2.34938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500]\ttraining's rmse: 2.10214\tvalid_1's rmse: 2.3492\n",
      "[4550]\ttraining's rmse: 2.10047\tvalid_1's rmse: 2.34884\n",
      "[4600]\ttraining's rmse: 2.09896\tvalid_1's rmse: 2.34847\n",
      "[4650]\ttraining's rmse: 2.09698\tvalid_1's rmse: 2.34834\n",
      "[4700]\ttraining's rmse: 2.0953\tvalid_1's rmse: 2.34797\n",
      "[4750]\ttraining's rmse: 2.09343\tvalid_1's rmse: 2.34758\n",
      "[4800]\ttraining's rmse: 2.09152\tvalid_1's rmse: 2.34736\n",
      "[4850]\ttraining's rmse: 2.08953\tvalid_1's rmse: 2.34708\n",
      "[4900]\ttraining's rmse: 2.08791\tvalid_1's rmse: 2.34667\n",
      "[4950]\ttraining's rmse: 2.08585\tvalid_1's rmse: 2.34631\n",
      "[5000]\ttraining's rmse: 2.08429\tvalid_1's rmse: 2.34608\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08429\tvalid_1's rmse: 2.34608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.57264\tvalid_1's rmse: 2.59074\n",
      "[100]\ttraining's rmse: 2.49587\tvalid_1's rmse: 2.51562\n",
      "[150]\ttraining's rmse: 2.47296\tvalid_1's rmse: 2.49601\n",
      "[200]\ttraining's rmse: 2.45185\tvalid_1's rmse: 2.47886\n",
      "[250]\ttraining's rmse: 2.4342\tvalid_1's rmse: 2.46588\n",
      "[300]\ttraining's rmse: 2.41932\tvalid_1's rmse: 2.45518\n",
      "[350]\ttraining's rmse: 2.40344\tvalid_1's rmse: 2.44478\n",
      "[400]\ttraining's rmse: 2.39036\tvalid_1's rmse: 2.43681\n",
      "[450]\ttraining's rmse: 2.37848\tvalid_1's rmse: 2.43034\n",
      "[500]\ttraining's rmse: 2.36914\tvalid_1's rmse: 2.42582\n",
      "[550]\ttraining's rmse: 2.36032\tvalid_1's rmse: 2.42165\n",
      "[600]\ttraining's rmse: 2.35207\tvalid_1's rmse: 2.4178\n",
      "[650]\ttraining's rmse: 2.34448\tvalid_1's rmse: 2.41416\n",
      "[700]\ttraining's rmse: 2.33738\tvalid_1's rmse: 2.41094\n",
      "[750]\ttraining's rmse: 2.33085\tvalid_1's rmse: 2.40818\n",
      "[800]\ttraining's rmse: 2.32506\tvalid_1's rmse: 2.40631\n",
      "[850]\ttraining's rmse: 2.31852\tvalid_1's rmse: 2.40358\n",
      "[900]\ttraining's rmse: 2.31235\tvalid_1's rmse: 2.40087\n",
      "[950]\ttraining's rmse: 2.30766\tvalid_1's rmse: 2.39918\n",
      "[1000]\ttraining's rmse: 2.30216\tvalid_1's rmse: 2.39739\n",
      "[1050]\ttraining's rmse: 2.2962\tvalid_1's rmse: 2.39527\n",
      "[1100]\ttraining's rmse: 2.29084\tvalid_1's rmse: 2.394\n",
      "[1150]\ttraining's rmse: 2.28608\tvalid_1's rmse: 2.39224\n",
      "[1200]\ttraining's rmse: 2.2818\tvalid_1's rmse: 2.39086\n",
      "[1250]\ttraining's rmse: 2.27597\tvalid_1's rmse: 2.38882\n",
      "[1300]\ttraining's rmse: 2.27138\tvalid_1's rmse: 2.38739\n",
      "[1350]\ttraining's rmse: 2.26748\tvalid_1's rmse: 2.38614\n",
      "[1400]\ttraining's rmse: 2.2633\tvalid_1's rmse: 2.38478\n",
      "[1450]\ttraining's rmse: 2.25916\tvalid_1's rmse: 2.38382\n",
      "[1500]\ttraining's rmse: 2.25471\tvalid_1's rmse: 2.38216\n",
      "[1550]\ttraining's rmse: 2.25045\tvalid_1's rmse: 2.38077\n",
      "[1600]\ttraining's rmse: 2.24705\tvalid_1's rmse: 2.37962\n",
      "[1650]\ttraining's rmse: 2.24404\tvalid_1's rmse: 2.37877\n",
      "[1700]\ttraining's rmse: 2.24055\tvalid_1's rmse: 2.37797\n",
      "[1750]\ttraining's rmse: 2.23656\tvalid_1's rmse: 2.37686\n",
      "[1800]\ttraining's rmse: 2.23293\tvalid_1's rmse: 2.37595\n",
      "[1850]\ttraining's rmse: 2.22972\tvalid_1's rmse: 2.37508\n",
      "[1900]\ttraining's rmse: 2.22686\tvalid_1's rmse: 2.37446\n",
      "[1950]\ttraining's rmse: 2.22422\tvalid_1's rmse: 2.37382\n",
      "[2000]\ttraining's rmse: 2.22113\tvalid_1's rmse: 2.37315\n",
      "[2050]\ttraining's rmse: 2.2179\tvalid_1's rmse: 2.37241\n",
      "[2100]\ttraining's rmse: 2.21467\tvalid_1's rmse: 2.37173\n",
      "[2150]\ttraining's rmse: 2.21124\tvalid_1's rmse: 2.37096\n",
      "[2200]\ttraining's rmse: 2.20799\tvalid_1's rmse: 2.37013\n",
      "[2250]\ttraining's rmse: 2.20492\tvalid_1's rmse: 2.36939\n",
      "[2300]\ttraining's rmse: 2.2016\tvalid_1's rmse: 2.36851\n",
      "[2350]\ttraining's rmse: 2.1985\tvalid_1's rmse: 2.3677\n",
      "[2400]\ttraining's rmse: 2.19581\tvalid_1's rmse: 2.36721\n",
      "[2450]\ttraining's rmse: 2.19305\tvalid_1's rmse: 2.36648\n",
      "[2500]\ttraining's rmse: 2.1902\tvalid_1's rmse: 2.3657\n",
      "[2550]\ttraining's rmse: 2.18735\tvalid_1's rmse: 2.365\n",
      "[2600]\ttraining's rmse: 2.18449\tvalid_1's rmse: 2.3643\n",
      "[2650]\ttraining's rmse: 2.18167\tvalid_1's rmse: 2.36369\n",
      "[2700]\ttraining's rmse: 2.17875\tvalid_1's rmse: 2.36283\n",
      "[2750]\ttraining's rmse: 2.17635\tvalid_1's rmse: 2.36213\n",
      "[2800]\ttraining's rmse: 2.17344\tvalid_1's rmse: 2.36152\n",
      "[2850]\ttraining's rmse: 2.17108\tvalid_1's rmse: 2.36091\n",
      "[2900]\ttraining's rmse: 2.16851\tvalid_1's rmse: 2.36027\n",
      "[2950]\ttraining's rmse: 2.16572\tvalid_1's rmse: 2.35971\n",
      "[3000]\ttraining's rmse: 2.16277\tvalid_1's rmse: 2.35891\n",
      "[3050]\ttraining's rmse: 2.16017\tvalid_1's rmse: 2.35832\n",
      "[3100]\ttraining's rmse: 2.15801\tvalid_1's rmse: 2.35787\n",
      "[3150]\ttraining's rmse: 2.15599\tvalid_1's rmse: 2.35757\n",
      "[3200]\ttraining's rmse: 2.1538\tvalid_1's rmse: 2.35705\n",
      "[3250]\ttraining's rmse: 2.15147\tvalid_1's rmse: 2.35669\n",
      "[3300]\ttraining's rmse: 2.14979\tvalid_1's rmse: 2.35641\n",
      "[3350]\ttraining's rmse: 2.14746\tvalid_1's rmse: 2.35612\n",
      "[3400]\ttraining's rmse: 2.14527\tvalid_1's rmse: 2.35562\n",
      "[3450]\ttraining's rmse: 2.14339\tvalid_1's rmse: 2.35538\n",
      "[3500]\ttraining's rmse: 2.1416\tvalid_1's rmse: 2.35505\n",
      "[3550]\ttraining's rmse: 2.1398\tvalid_1's rmse: 2.35463\n",
      "[3600]\ttraining's rmse: 2.1379\tvalid_1's rmse: 2.35436\n",
      "[3650]\ttraining's rmse: 2.13547\tvalid_1's rmse: 2.35397\n",
      "[3700]\ttraining's rmse: 2.13343\tvalid_1's rmse: 2.35367\n",
      "[3750]\ttraining's rmse: 2.13138\tvalid_1's rmse: 2.3533\n",
      "[3800]\ttraining's rmse: 2.1294\tvalid_1's rmse: 2.35296\n",
      "[3850]\ttraining's rmse: 2.12702\tvalid_1's rmse: 2.35245\n",
      "[3900]\ttraining's rmse: 2.12488\tvalid_1's rmse: 2.35196\n",
      "[3950]\ttraining's rmse: 2.12298\tvalid_1's rmse: 2.35157\n",
      "[4000]\ttraining's rmse: 2.12143\tvalid_1's rmse: 2.35131\n",
      "[4050]\ttraining's rmse: 2.1194\tvalid_1's rmse: 2.35079\n",
      "[4100]\ttraining's rmse: 2.11752\tvalid_1's rmse: 2.35044\n",
      "[4150]\ttraining's rmse: 2.11567\tvalid_1's rmse: 2.34992\n",
      "[4200]\ttraining's rmse: 2.11366\tvalid_1's rmse: 2.34965\n",
      "[4250]\ttraining's rmse: 2.11171\tvalid_1's rmse: 2.34927\n",
      "[4300]\ttraining's rmse: 2.1101\tvalid_1's rmse: 2.34906\n",
      "[4350]\ttraining's rmse: 2.10826\tvalid_1's rmse: 2.34879\n",
      "[4400]\ttraining's rmse: 2.10632\tvalid_1's rmse: 2.34848\n",
      "[4450]\ttraining's rmse: 2.10454\tvalid_1's rmse: 2.3481\n",
      "[4500]\ttraining's rmse: 2.10277\tvalid_1's rmse: 2.34772\n",
      "[4550]\ttraining's rmse: 2.10094\tvalid_1's rmse: 2.34746\n",
      "[4600]\ttraining's rmse: 2.09942\tvalid_1's rmse: 2.34714\n",
      "[4650]\ttraining's rmse: 2.09759\tvalid_1's rmse: 2.34673\n",
      "[4700]\ttraining's rmse: 2.09574\tvalid_1's rmse: 2.3464\n",
      "[4750]\ttraining's rmse: 2.09386\tvalid_1's rmse: 2.34615\n",
      "[4800]\ttraining's rmse: 2.09239\tvalid_1's rmse: 2.34579\n",
      "[4850]\ttraining's rmse: 2.09074\tvalid_1's rmse: 2.34548\n",
      "[4900]\ttraining's rmse: 2.08905\tvalid_1's rmse: 2.34513\n",
      "[4950]\ttraining's rmse: 2.08747\tvalid_1's rmse: 2.34483\n",
      "[5000]\ttraining's rmse: 2.08622\tvalid_1's rmse: 2.34465\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08622\tvalid_1's rmse: 2.34465\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.56258\tvalid_1's rmse: 2.58572\n",
      "[100]\ttraining's rmse: 2.49227\tvalid_1's rmse: 2.52179\n",
      "[150]\ttraining's rmse: 2.47131\tvalid_1's rmse: 2.50513\n",
      "[200]\ttraining's rmse: 2.45069\tvalid_1's rmse: 2.48916\n",
      "[250]\ttraining's rmse: 2.43027\tvalid_1's rmse: 2.47415\n",
      "[300]\ttraining's rmse: 2.41428\tvalid_1's rmse: 2.46277\n",
      "[350]\ttraining's rmse: 2.39983\tvalid_1's rmse: 2.45317\n",
      "[400]\ttraining's rmse: 2.3894\tvalid_1's rmse: 2.44693\n",
      "[450]\ttraining's rmse: 2.37713\tvalid_1's rmse: 2.44014\n",
      "[500]\ttraining's rmse: 2.36765\tvalid_1's rmse: 2.43548\n",
      "[550]\ttraining's rmse: 2.35844\tvalid_1's rmse: 2.43096\n",
      "[600]\ttraining's rmse: 2.35062\tvalid_1's rmse: 2.42781\n",
      "[650]\ttraining's rmse: 2.34364\tvalid_1's rmse: 2.42493\n",
      "[700]\ttraining's rmse: 2.33657\tvalid_1's rmse: 2.42199\n",
      "[750]\ttraining's rmse: 2.32905\tvalid_1's rmse: 2.41915\n",
      "[800]\ttraining's rmse: 2.3218\tvalid_1's rmse: 2.41633\n",
      "[850]\ttraining's rmse: 2.31543\tvalid_1's rmse: 2.41365\n",
      "[900]\ttraining's rmse: 2.31007\tvalid_1's rmse: 2.41197\n",
      "[950]\ttraining's rmse: 2.30543\tvalid_1's rmse: 2.41016\n",
      "[1000]\ttraining's rmse: 2.3\tvalid_1's rmse: 2.40849\n",
      "[1050]\ttraining's rmse: 2.29451\tvalid_1's rmse: 2.40653\n",
      "[1100]\ttraining's rmse: 2.28918\tvalid_1's rmse: 2.40465\n",
      "[1150]\ttraining's rmse: 2.28412\tvalid_1's rmse: 2.4031\n",
      "[1200]\ttraining's rmse: 2.27941\tvalid_1's rmse: 2.40136\n",
      "[1250]\ttraining's rmse: 2.27555\tvalid_1's rmse: 2.40013\n",
      "[1300]\ttraining's rmse: 2.27047\tvalid_1's rmse: 2.39853\n",
      "[1350]\ttraining's rmse: 2.26561\tvalid_1's rmse: 2.39704\n",
      "[1400]\ttraining's rmse: 2.26082\tvalid_1's rmse: 2.39529\n",
      "[1450]\ttraining's rmse: 2.25712\tvalid_1's rmse: 2.39434\n",
      "[1500]\ttraining's rmse: 2.25287\tvalid_1's rmse: 2.39287\n",
      "[1550]\ttraining's rmse: 2.24997\tvalid_1's rmse: 2.39213\n",
      "[1600]\ttraining's rmse: 2.24627\tvalid_1's rmse: 2.39119\n",
      "[1650]\ttraining's rmse: 2.24239\tvalid_1's rmse: 2.38995\n",
      "[1700]\ttraining's rmse: 2.23854\tvalid_1's rmse: 2.38891\n",
      "[1750]\ttraining's rmse: 2.23491\tvalid_1's rmse: 2.38826\n",
      "[1800]\ttraining's rmse: 2.23164\tvalid_1's rmse: 2.38746\n",
      "[1850]\ttraining's rmse: 2.22888\tvalid_1's rmse: 2.38679\n",
      "[1900]\ttraining's rmse: 2.22548\tvalid_1's rmse: 2.3859\n",
      "[1950]\ttraining's rmse: 2.22252\tvalid_1's rmse: 2.38503\n",
      "[2000]\ttraining's rmse: 2.21953\tvalid_1's rmse: 2.38405\n",
      "[2050]\ttraining's rmse: 2.21604\tvalid_1's rmse: 2.38325\n",
      "[2100]\ttraining's rmse: 2.21316\tvalid_1's rmse: 2.38255\n",
      "[2150]\ttraining's rmse: 2.20964\tvalid_1's rmse: 2.38158\n",
      "[2200]\ttraining's rmse: 2.20637\tvalid_1's rmse: 2.38059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.20354\tvalid_1's rmse: 2.37991\n",
      "[2300]\ttraining's rmse: 2.20052\tvalid_1's rmse: 2.37924\n",
      "[2350]\ttraining's rmse: 2.19793\tvalid_1's rmse: 2.37851\n",
      "[2400]\ttraining's rmse: 2.19554\tvalid_1's rmse: 2.378\n",
      "[2450]\ttraining's rmse: 2.19268\tvalid_1's rmse: 2.37744\n",
      "[2500]\ttraining's rmse: 2.18992\tvalid_1's rmse: 2.37676\n",
      "[2550]\ttraining's rmse: 2.1871\tvalid_1's rmse: 2.37612\n",
      "[2600]\ttraining's rmse: 2.18414\tvalid_1's rmse: 2.37532\n",
      "[2650]\ttraining's rmse: 2.18132\tvalid_1's rmse: 2.3748\n",
      "[2700]\ttraining's rmse: 2.1786\tvalid_1's rmse: 2.37392\n",
      "[2750]\ttraining's rmse: 2.17578\tvalid_1's rmse: 2.3731\n",
      "[2800]\ttraining's rmse: 2.17389\tvalid_1's rmse: 2.37272\n",
      "[2850]\ttraining's rmse: 2.17121\tvalid_1's rmse: 2.37203\n",
      "[2900]\ttraining's rmse: 2.16869\tvalid_1's rmse: 2.3715\n",
      "[2950]\ttraining's rmse: 2.16586\tvalid_1's rmse: 2.37073\n",
      "[3000]\ttraining's rmse: 2.16341\tvalid_1's rmse: 2.37019\n",
      "[3050]\ttraining's rmse: 2.16095\tvalid_1's rmse: 2.36961\n",
      "[3100]\ttraining's rmse: 2.15856\tvalid_1's rmse: 2.36896\n",
      "[3150]\ttraining's rmse: 2.1565\tvalid_1's rmse: 2.36862\n",
      "[3200]\ttraining's rmse: 2.15417\tvalid_1's rmse: 2.36817\n",
      "[3250]\ttraining's rmse: 2.1522\tvalid_1's rmse: 2.36792\n",
      "[3300]\ttraining's rmse: 2.14984\tvalid_1's rmse: 2.3674\n",
      "[3350]\ttraining's rmse: 2.1478\tvalid_1's rmse: 2.36693\n",
      "[3400]\ttraining's rmse: 2.14581\tvalid_1's rmse: 2.36658\n",
      "[3450]\ttraining's rmse: 2.14375\tvalid_1's rmse: 2.36624\n",
      "[3500]\ttraining's rmse: 2.14169\tvalid_1's rmse: 2.36565\n",
      "[3550]\ttraining's rmse: 2.13961\tvalid_1's rmse: 2.36525\n",
      "[3600]\ttraining's rmse: 2.13743\tvalid_1's rmse: 2.36495\n",
      "[3650]\ttraining's rmse: 2.13477\tvalid_1's rmse: 2.3643\n",
      "[3700]\ttraining's rmse: 2.13289\tvalid_1's rmse: 2.36389\n",
      "[3750]\ttraining's rmse: 2.13095\tvalid_1's rmse: 2.3635\n",
      "[3800]\ttraining's rmse: 2.12881\tvalid_1's rmse: 2.36308\n",
      "[3850]\ttraining's rmse: 2.12694\tvalid_1's rmse: 2.36282\n",
      "[3900]\ttraining's rmse: 2.12456\tvalid_1's rmse: 2.36239\n",
      "[3950]\ttraining's rmse: 2.12281\tvalid_1's rmse: 2.362\n",
      "[4000]\ttraining's rmse: 2.12101\tvalid_1's rmse: 2.36175\n",
      "[4050]\ttraining's rmse: 2.11896\tvalid_1's rmse: 2.36152\n",
      "[4100]\ttraining's rmse: 2.117\tvalid_1's rmse: 2.36102\n",
      "[4150]\ttraining's rmse: 2.1155\tvalid_1's rmse: 2.36064\n",
      "[4200]\ttraining's rmse: 2.11371\tvalid_1's rmse: 2.36024\n",
      "[4250]\ttraining's rmse: 2.11155\tvalid_1's rmse: 2.35974\n",
      "[4300]\ttraining's rmse: 2.10981\tvalid_1's rmse: 2.35944\n",
      "[4350]\ttraining's rmse: 2.10805\tvalid_1's rmse: 2.35899\n",
      "[4400]\ttraining's rmse: 2.10608\tvalid_1's rmse: 2.35854\n",
      "[4450]\ttraining's rmse: 2.10455\tvalid_1's rmse: 2.35821\n",
      "[4500]\ttraining's rmse: 2.1027\tvalid_1's rmse: 2.35782\n",
      "[4550]\ttraining's rmse: 2.10127\tvalid_1's rmse: 2.35759\n",
      "[4600]\ttraining's rmse: 2.09962\tvalid_1's rmse: 2.35732\n",
      "[4650]\ttraining's rmse: 2.0979\tvalid_1's rmse: 2.35706\n",
      "[4700]\ttraining's rmse: 2.09598\tvalid_1's rmse: 2.35669\n",
      "[4750]\ttraining's rmse: 2.09461\tvalid_1's rmse: 2.35648\n",
      "[4800]\ttraining's rmse: 2.09313\tvalid_1's rmse: 2.35622\n",
      "[4850]\ttraining's rmse: 2.09118\tvalid_1's rmse: 2.35571\n",
      "[4900]\ttraining's rmse: 2.08983\tvalid_1's rmse: 2.35556\n",
      "[4950]\ttraining's rmse: 2.08813\tvalid_1's rmse: 2.35541\n",
      "[5000]\ttraining's rmse: 2.08674\tvalid_1's rmse: 2.35523\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08674\tvalid_1's rmse: 2.35523\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.58624\tvalid_1's rmse: 2.58989\n",
      "[100]\ttraining's rmse: 2.50256\tvalid_1's rmse: 2.5114\n",
      "[150]\ttraining's rmse: 2.4802\tvalid_1's rmse: 2.493\n",
      "[200]\ttraining's rmse: 2.45938\tvalid_1's rmse: 2.47693\n",
      "[250]\ttraining's rmse: 2.43984\tvalid_1's rmse: 2.46377\n",
      "[300]\ttraining's rmse: 2.42383\tvalid_1's rmse: 2.45286\n",
      "[350]\ttraining's rmse: 2.40934\tvalid_1's rmse: 2.4442\n",
      "[400]\ttraining's rmse: 2.39597\tvalid_1's rmse: 2.43666\n",
      "[450]\ttraining's rmse: 2.38566\tvalid_1's rmse: 2.43147\n",
      "[500]\ttraining's rmse: 2.37619\tvalid_1's rmse: 2.42708\n",
      "[550]\ttraining's rmse: 2.3675\tvalid_1's rmse: 2.42287\n",
      "[600]\ttraining's rmse: 2.35862\tvalid_1's rmse: 2.41906\n",
      "[650]\ttraining's rmse: 2.35083\tvalid_1's rmse: 2.41579\n",
      "[700]\ttraining's rmse: 2.34323\tvalid_1's rmse: 2.41249\n",
      "[750]\ttraining's rmse: 2.33599\tvalid_1's rmse: 2.40934\n",
      "[800]\ttraining's rmse: 2.32828\tvalid_1's rmse: 2.40625\n",
      "[850]\ttraining's rmse: 2.32233\tvalid_1's rmse: 2.40368\n",
      "[900]\ttraining's rmse: 2.31582\tvalid_1's rmse: 2.4014\n",
      "[950]\ttraining's rmse: 2.31071\tvalid_1's rmse: 2.39971\n",
      "[1000]\ttraining's rmse: 2.30612\tvalid_1's rmse: 2.39811\n",
      "[1050]\ttraining's rmse: 2.3004\tvalid_1's rmse: 2.39621\n",
      "[1100]\ttraining's rmse: 2.29501\tvalid_1's rmse: 2.39411\n",
      "[1150]\ttraining's rmse: 2.29023\tvalid_1's rmse: 2.39286\n",
      "[1200]\ttraining's rmse: 2.28476\tvalid_1's rmse: 2.39083\n",
      "[1250]\ttraining's rmse: 2.28001\tvalid_1's rmse: 2.38939\n",
      "[1300]\ttraining's rmse: 2.27537\tvalid_1's rmse: 2.38788\n",
      "[1350]\ttraining's rmse: 2.27062\tvalid_1's rmse: 2.38628\n",
      "[1400]\ttraining's rmse: 2.26653\tvalid_1's rmse: 2.3852\n",
      "[1450]\ttraining's rmse: 2.26207\tvalid_1's rmse: 2.38382\n",
      "[1500]\ttraining's rmse: 2.25792\tvalid_1's rmse: 2.38253\n",
      "[1550]\ttraining's rmse: 2.25401\tvalid_1's rmse: 2.38123\n",
      "[1600]\ttraining's rmse: 2.25038\tvalid_1's rmse: 2.38046\n",
      "[1650]\ttraining's rmse: 2.24739\tvalid_1's rmse: 2.37979\n",
      "[1700]\ttraining's rmse: 2.24377\tvalid_1's rmse: 2.3785\n",
      "[1750]\ttraining's rmse: 2.24033\tvalid_1's rmse: 2.37775\n",
      "[1800]\ttraining's rmse: 2.23711\tvalid_1's rmse: 2.37699\n",
      "[1850]\ttraining's rmse: 2.23394\tvalid_1's rmse: 2.37614\n",
      "[1900]\ttraining's rmse: 2.23033\tvalid_1's rmse: 2.37519\n",
      "[1950]\ttraining's rmse: 2.22744\tvalid_1's rmse: 2.37458\n",
      "[2000]\ttraining's rmse: 2.2242\tvalid_1's rmse: 2.37392\n",
      "[2050]\ttraining's rmse: 2.22031\tvalid_1's rmse: 2.37314\n",
      "[2100]\ttraining's rmse: 2.21663\tvalid_1's rmse: 2.37229\n",
      "[2150]\ttraining's rmse: 2.2132\tvalid_1's rmse: 2.3714\n",
      "[2200]\ttraining's rmse: 2.21034\tvalid_1's rmse: 2.37092\n",
      "[2250]\ttraining's rmse: 2.20707\tvalid_1's rmse: 2.37013\n",
      "[2300]\ttraining's rmse: 2.20382\tvalid_1's rmse: 2.3694\n",
      "[2350]\ttraining's rmse: 2.20096\tvalid_1's rmse: 2.36867\n",
      "[2400]\ttraining's rmse: 2.19791\tvalid_1's rmse: 2.36787\n",
      "[2450]\ttraining's rmse: 2.19513\tvalid_1's rmse: 2.36701\n",
      "[2500]\ttraining's rmse: 2.19246\tvalid_1's rmse: 2.36643\n",
      "[2550]\ttraining's rmse: 2.18948\tvalid_1's rmse: 2.36551\n",
      "[2600]\ttraining's rmse: 2.18689\tvalid_1's rmse: 2.36494\n",
      "[2650]\ttraining's rmse: 2.18445\tvalid_1's rmse: 2.36436\n",
      "[2700]\ttraining's rmse: 2.18198\tvalid_1's rmse: 2.36387\n",
      "[2750]\ttraining's rmse: 2.17976\tvalid_1's rmse: 2.36333\n",
      "[2800]\ttraining's rmse: 2.17718\tvalid_1's rmse: 2.36271\n",
      "[2850]\ttraining's rmse: 2.17455\tvalid_1's rmse: 2.36201\n",
      "[2900]\ttraining's rmse: 2.1721\tvalid_1's rmse: 2.3616\n",
      "[2950]\ttraining's rmse: 2.16965\tvalid_1's rmse: 2.36091\n",
      "[3000]\ttraining's rmse: 2.1671\tvalid_1's rmse: 2.36031\n",
      "[3050]\ttraining's rmse: 2.16505\tvalid_1's rmse: 2.35989\n",
      "[3100]\ttraining's rmse: 2.16228\tvalid_1's rmse: 2.35915\n",
      "[3150]\ttraining's rmse: 2.15988\tvalid_1's rmse: 2.35831\n",
      "[3200]\ttraining's rmse: 2.15714\tvalid_1's rmse: 2.35771\n",
      "[3250]\ttraining's rmse: 2.15505\tvalid_1's rmse: 2.35727\n",
      "[3300]\ttraining's rmse: 2.15285\tvalid_1's rmse: 2.35686\n",
      "[3350]\ttraining's rmse: 2.15066\tvalid_1's rmse: 2.35644\n",
      "[3400]\ttraining's rmse: 2.14837\tvalid_1's rmse: 2.35581\n",
      "[3450]\ttraining's rmse: 2.14649\tvalid_1's rmse: 2.35552\n",
      "[3500]\ttraining's rmse: 2.14448\tvalid_1's rmse: 2.35506\n",
      "[3550]\ttraining's rmse: 2.14213\tvalid_1's rmse: 2.35451\n",
      "[3600]\ttraining's rmse: 2.13959\tvalid_1's rmse: 2.35373\n",
      "[3650]\ttraining's rmse: 2.13746\tvalid_1's rmse: 2.35328\n",
      "[3700]\ttraining's rmse: 2.13553\tvalid_1's rmse: 2.35281\n",
      "[3750]\ttraining's rmse: 2.13339\tvalid_1's rmse: 2.35236\n",
      "[3800]\ttraining's rmse: 2.13122\tvalid_1's rmse: 2.35199\n",
      "[3850]\ttraining's rmse: 2.12898\tvalid_1's rmse: 2.35142\n",
      "[3900]\ttraining's rmse: 2.12729\tvalid_1's rmse: 2.35115\n",
      "[3950]\ttraining's rmse: 2.12511\tvalid_1's rmse: 2.35077\n",
      "[4000]\ttraining's rmse: 2.12333\tvalid_1's rmse: 2.35036\n",
      "[4050]\ttraining's rmse: 2.12162\tvalid_1's rmse: 2.3502\n",
      "[4100]\ttraining's rmse: 2.11953\tvalid_1's rmse: 2.34979\n",
      "[4150]\ttraining's rmse: 2.11773\tvalid_1's rmse: 2.34948\n",
      "[4200]\ttraining's rmse: 2.11595\tvalid_1's rmse: 2.34925\n",
      "[4250]\ttraining's rmse: 2.11437\tvalid_1's rmse: 2.34899\n",
      "[4300]\ttraining's rmse: 2.11248\tvalid_1's rmse: 2.3486\n",
      "[4350]\ttraining's rmse: 2.11048\tvalid_1's rmse: 2.34825\n",
      "[4400]\ttraining's rmse: 2.10886\tvalid_1's rmse: 2.34795\n",
      "[4450]\ttraining's rmse: 2.10692\tvalid_1's rmse: 2.34754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500]\ttraining's rmse: 2.10519\tvalid_1's rmse: 2.3471\n",
      "[4550]\ttraining's rmse: 2.10299\tvalid_1's rmse: 2.34655\n",
      "[4600]\ttraining's rmse: 2.10049\tvalid_1's rmse: 2.34591\n",
      "[4650]\ttraining's rmse: 2.09866\tvalid_1's rmse: 2.34551\n",
      "[4700]\ttraining's rmse: 2.09687\tvalid_1's rmse: 2.34527\n",
      "[4750]\ttraining's rmse: 2.09509\tvalid_1's rmse: 2.3449\n",
      "[4800]\ttraining's rmse: 2.09295\tvalid_1's rmse: 2.34447\n",
      "[4850]\ttraining's rmse: 2.09098\tvalid_1's rmse: 2.34397\n",
      "[4900]\ttraining's rmse: 2.08952\tvalid_1's rmse: 2.34369\n",
      "[4950]\ttraining's rmse: 2.08784\tvalid_1's rmse: 2.34345\n",
      "[5000]\ttraining's rmse: 2.08595\tvalid_1's rmse: 2.34327\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08595\tvalid_1's rmse: 2.34327\n",
      "Wall time: 9h 37min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folds = 3\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pickle\n",
    "\n",
    "for state in states:\n",
    "    \n",
    "    seed = state\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "    for idx, (train_index, test_index) in enumerate(skf.split(df_train.index,df_train['volume'])):\n",
    "        print(idx)\n",
    "        x_train = df_train.iloc[train_index].drop(['day','volume'], axis =1)\n",
    "        y_train = df_train.iloc[train_index]['volume']\n",
    "        x_valid = df_train.iloc[test_index].drop(['day','volume'], axis =1)\n",
    "        y_valid = df_train.iloc[test_index]['volume'] \n",
    "\n",
    "        # Modeling\n",
    "        lgb_train = lgb.Dataset(x_train, y_train,categorical_feature=cat_cols)\n",
    "        lgb_eval = lgb.Dataset(x_valid, y_valid,categorical_feature=cat_cols)\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "    #                     num_boost_round=1000, \n",
    "                        valid_sets=(lgb_train, lgb_eval),\n",
    "                        early_stopping_rounds= 50,#100,\n",
    "                        verbose_eval=50) #100)\n",
    "\n",
    "        pickle.dump(gbm,open( \"20200508_model_%s_full_seed%s.pkl\"%(idx,seed), \"wb\" ))\n",
    "\n",
    "        del gbm\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1914\n",
      "rolling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n",
      "rolling\n",
      "mean\n",
      "1916\n",
      "rolling\n",
      "mean\n",
      "1917\n",
      "rolling\n",
      "mean\n",
      "1918\n",
      "rolling\n",
      "mean\n",
      "1919\n",
      "rolling\n",
      "mean\n",
      "1920\n",
      "rolling\n",
      "mean\n",
      "1921\n",
      "rolling\n",
      "mean\n",
      "1922\n",
      "rolling\n",
      "mean\n",
      "1923\n",
      "rolling\n",
      "mean\n",
      "1924\n",
      "rolling\n",
      "mean\n",
      "1925\n",
      "rolling\n",
      "mean\n",
      "1926\n",
      "rolling\n",
      "mean\n",
      "1927\n",
      "rolling\n",
      "mean\n",
      "1928\n",
      "rolling\n",
      "mean\n",
      "1929\n",
      "rolling\n",
      "mean\n",
      "1930\n",
      "rolling\n",
      "mean\n",
      "1931\n",
      "rolling\n",
      "mean\n",
      "1932\n",
      "rolling\n",
      "mean\n",
      "1933\n",
      "rolling\n",
      "mean\n",
      "1934\n",
      "rolling\n",
      "mean\n",
      "1935\n",
      "rolling\n",
      "mean\n",
      "1936\n",
      "rolling\n",
      "mean\n",
      "1937\n",
      "rolling\n",
      "mean\n",
      "1938\n",
      "rolling\n",
      "mean\n",
      "1939\n",
      "rolling\n",
      "mean\n",
      "1940\n",
      "rolling\n",
      "mean\n",
      "1941\n",
      "rolling\n",
      "mean\n",
      "243\n",
      "1914\n",
      "rolling\n",
      "mean\n",
      "1915\n",
      "rolling\n",
      "mean\n",
      "1916\n",
      "rolling\n",
      "mean\n",
      "1917\n",
      "rolling\n",
      "mean\n",
      "1918\n",
      "rolling\n",
      "mean\n",
      "1919\n",
      "rolling\n",
      "mean\n",
      "1920\n",
      "rolling\n",
      "mean\n",
      "1921\n",
      "rolling\n",
      "mean\n",
      "1922\n",
      "rolling\n",
      "mean\n",
      "1923\n",
      "rolling\n",
      "mean\n",
      "1924\n",
      "rolling\n",
      "mean\n",
      "1925\n",
      "rolling\n",
      "mean\n",
      "1926\n",
      "rolling\n",
      "mean\n",
      "1927\n",
      "rolling\n",
      "mean\n",
      "1928\n",
      "rolling\n",
      "mean\n",
      "1929\n",
      "rolling\n",
      "mean\n",
      "1930\n",
      "rolling\n",
      "mean\n",
      "1931\n",
      "rolling\n",
      "mean\n",
      "1932\n",
      "rolling\n",
      "mean\n",
      "1933\n",
      "rolling\n",
      "mean\n",
      "1934\n",
      "rolling\n",
      "mean\n",
      "1935\n",
      "rolling\n",
      "mean\n",
      "1936\n",
      "rolling\n",
      "mean\n",
      "1937\n",
      "rolling\n",
      "mean\n",
      "1938\n",
      "rolling\n",
      "mean\n",
      "1939\n",
      "rolling\n",
      "mean\n",
      "1940\n",
      "rolling\n",
      "mean\n",
      "1941\n",
      "rolling\n",
      "mean\n",
      "Wall time: 1h 35min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test['id'] = train_id\n",
    "max_lag  = 120\n",
    "trn_lst = 1913\n",
    "for state in states:\n",
    "    print(state)\n",
    "#     if state == 999: continue\n",
    "    #df_test 값이 Reset이 안되어 있다 !!!!!  - df test를 유지하는 것으로 하자\n",
    "    tmp_test = df_test.copy() # 추가 !!\n",
    "    for tdelta in range(1, 29):\n",
    "        f_day = trn_lst+tdelta\n",
    "        print(f_day)\n",
    "        days = [f\"d_{i}\" for i in range(trn_lst-max_lag+tdelta,f_day+1)]\n",
    "        tst = tmp_test[tmp_test['day'].isin(days)]\n",
    "\n",
    "        print(\"rolling\")\n",
    "#         tst['volume_1'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(1)\n",
    "#         tst['volume_2'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(2)\n",
    "#         tst['volume_3'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(3)\n",
    "        \n",
    "        \n",
    "        tst['volume_7'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(7)\n",
    "        tst['volume_28'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(28)\n",
    "\n",
    "        print(\"mean\")\n",
    "        tst['rmean_7_7'] = tst[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(7).mean())\n",
    "        tst['rmean_7_28'] = tst[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(28).mean())\n",
    "        tst['rmean_7_50'] = tst[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "        tst['rmean_28_7'] = tst[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(7).mean())\n",
    "        tst['rmean_28_28'] = tst[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(28).mean())\n",
    "        tst['rmean_28_50'] = tst[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "\n",
    "        tst = tst[tst['day'] == \"d_%s\"%(f_day)]\n",
    "        t_id,t_volume,t_day = tst['id'],tst['volume'],tst['day']\n",
    "        tst = tst.drop(['id','volume','day'], axis =1)\n",
    "\n",
    "        # Crossvalidation \n",
    "        for idx in range(folds):\n",
    "            gbm = pickle.load(open( \"20200508_model_%s_full_seed%s.pkl\"%(idx,state), \"rb\" ))\n",
    "            tmp_test.loc[tmp_test.day==\"d_%s\"%(f_day),'volume'] += 1.028*gbm.predict(tst) / folds\n",
    "    \n",
    "    cols = [f\"d_{i}\" for i in range(1914,1942)]\n",
    "\n",
    "    sub = tmp_test[tmp_test['day'].isin(cols)].loc[:,['id','volume']]\n",
    "    sub['F']= [f\"F{rank}\" for rank in sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "    sub = sub.set_index([\"id\", \"F\" ]).unstack()[\"volume\"].reset_index()\n",
    "    sub.sort_values(\"id\", inplace = True)\n",
    "    sub.reset_index(drop=True, inplace = True)                                                   \n",
    "    sub =sub[['id']+[\"F%s\"% x for x in range(1,29)]]\n",
    "\n",
    "    sub = sub.fillna(0)\n",
    "\n",
    "    sub2 = sub.copy()\n",
    "    sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "    sub.to_csv(\"submission_20200508_full_seed%s.csv\"%state,index=False)\n",
    "    \n",
    "    del tmp_test\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [f\"d_{i}\" for i in range(1914,1942)]\n",
    "\n",
    "# sub = df_test[df_test['day'].isin(cols)].loc[:,['id','volume']]\n",
    "# sub['F']= [f\"F{rank}\" for rank in sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "# sub = sub.set_index([\"id\", \"F\" ]).unstack()[\"volume\"].reset_index()\n",
    "# sub.sort_values(\"id\", inplace = True)\n",
    "# sub.reset_index(drop=True, inplace = True)                                                   \n",
    "# sub =sub[['id']+[\"F%s\"% x for x in range(1,29)]]\n",
    "\n",
    "# sub = sub.fillna(0)\n",
    "\n",
    "# sub2 = sub.copy()\n",
    "# sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "# sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "# sub.to_csv(\"submission_20200505_0.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F\n",
       "id     0\n",
       "F1     0\n",
       "F2     0\n",
       "F3     0\n",
       "F4     0\n",
       "F5     0\n",
       "F6     0\n",
       "F7     0\n",
       "F8     0\n",
       "F9     0\n",
       "F10    0\n",
       "F11    0\n",
       "F12    0\n",
       "F13    0\n",
       "F14    0\n",
       "F15    0\n",
       "F16    0\n",
       "F17    0\n",
       "F18    0\n",
       "F19    0\n",
       "F20    0\n",
       "F21    0\n",
       "F22    0\n",
       "F23    0\n",
       "F24    0\n",
       "F25    0\n",
       "F26    0\n",
       "F27    0\n",
       "F28    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Union\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# class WRMSSEEvaluator(object):\n",
    "\n",
    "#     def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame, tst):\n",
    "#         train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "#         train_target_columns = train_y.columns.tolist()\n",
    "#         weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "# #         train_id = train_id\n",
    "#         train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "#         id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "#         valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "#         if not all([c in valid_df.columns for c in id_columns]):\n",
    "#             valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "#         self.train_df = train_df\n",
    "#         self.valid_df = valid_df\n",
    "#         self.calendar = calendar\n",
    "#         self.prices = prices\n",
    "#         self.tst = tst\n",
    "#         self.weight_columns = weight_columns\n",
    "#         self.id_columns = id_columns\n",
    "#         self.valid_target_columns = valid_target_columns\n",
    "\n",
    "#         weight_df = self.get_weight_df()\n",
    "\n",
    "#         self.group_ids = (\n",
    "#             'all_id',\n",
    "#             'state_id',\n",
    "#             'store_id',\n",
    "#             'cat_id',\n",
    "#             'dept_id',\n",
    "#             ['state_id', 'cat_id'],\n",
    "#             ['state_id', 'dept_id'],\n",
    "#             ['store_id', 'cat_id'],\n",
    "#             ['store_id', 'dept_id'],\n",
    "#             'item_id',\n",
    "#             ['item_id', 'state_id'],\n",
    "#             ['item_id', 'store_id']\n",
    "#         )\n",
    "\n",
    "#         for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "#             train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "#             scale = []\n",
    "#             for _, row in train_y.iterrows():\n",
    "#                 series = row.values[np.argmax(row.values != 0):]\n",
    "#                 scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "#             setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "#             setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "#             setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "#             lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "#             setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "#     def get_weight_df(self) -> pd.DataFrame:\n",
    "#         day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "#         weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "#         weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "#         weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "#         weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "#         weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "#         weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "#         weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "#         weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "#         return weight_df\n",
    "\n",
    "#     def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "#         valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "#         score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "#         scale = getattr(self, f'lv{lv}_scale')\n",
    "#         scale = np.where(scale != 0 , scale, 1)\n",
    "#         return (score / scale).map(np.sqrt)\n",
    "\n",
    "#     def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "#         assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "#         if isinstance(valid_preds, np.ndarray):\n",
    "#             valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "#         valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "#         all_scores = []\n",
    "#         for i, group_id in enumerate(self.group_ids):\n",
    "#             lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "#             weight = getattr(self, f'lv{i + 1}_weight')\n",
    "#             lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "#             all_scores.append(lv_scores.sum())\n",
    "\n",
    "#         return np.mean(all_scores)\n",
    "    \n",
    "    \n",
    "# class WRMSSEForLightGBM(WRMSSEEvaluator):\n",
    "\n",
    "#     def feval(self, preds, dtrain):\n",
    "# #         print(preds.shape, self.tst.shape)\n",
    "# #         tst= self.df[self.df['day'].isin(valid_target_columns)]\n",
    "# #         tst['id'] = train_id.loc[tst.index]\n",
    "#         tmp = self.tst.copy()\n",
    "#         tmp['preds'] = preds\n",
    "#         tmp=  tmp.set_index(['id',\"day\"]).unstack()[\"preds\"].reset_index()\n",
    "#         tmp =  tmp.fillna(0)\n",
    "\n",
    "#         val = pd.DataFrame()\n",
    "#         val['id'] = self.train_df['id']\n",
    "#         pred = pd.merge(val, tmp, how = 'left')\n",
    "#         pred = pred.fillna(0)\n",
    "# #         print(pred.columns)\n",
    "# #         print(self.valid_target_columns)\n",
    "#         pred = pred.loc[:,self.valid_target_columns ]\n",
    "# #         cv_scores.append(evaluator.score(pred))\n",
    "# #         preds = preds.reshape(self.valid_df[self.valid_target_columns].shape)\n",
    "#         score = self.score(pred)\n",
    "#         return 'WRMSSE', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 마지막 28일 데이터로 검증\n",
    "# cv_scores =[]\n",
    "# cols = [\"d_%s\"%x for x in range(1,1914)]\n",
    "# days = list(df_train['day'].unique()) # 날짜 섞기\n",
    "# cols = [x for x in cols if x in days]\n",
    "\n",
    "# d_valid=[f\"d_{i}\" for i in range(1913-28,1914)]\n",
    "# d_train = [x for x in cols if x not in d_valid]\n",
    "# tmp = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "# tmp_train = pd.concat([tmp.iloc[:,:6],tmp.loc[:, d_train]], axis =1)\n",
    "# tmp_valid = tmp.loc[:, d_valid]\n",
    "# #     evaluator = WRMSSEEvaluator(tmp_train, tmp_valid, calendar, price)\n",
    "# tst= df_valid[df_valid['day'].isin(d_valid)]\n",
    "# tst['id'] = train_id.loc[tst.index]\n",
    "\n",
    "\n",
    "# evaluator = WRMSSEEvaluator(tmp_train, tmp_valid, calendar, price,tst)\n",
    "# for state in states:\n",
    "#     print(state)\n",
    "#     for idx in range(folds):\n",
    "\n",
    "#         # gbm = pickle.dump(gbm,open( \"20200502_model_%s_r1_5000.pkl\"%idx, \"wb\" ))\n",
    "#         gbm = pickle.load(open( \"20200505_model_%s_best_seed%s.pkl\"%(idx,state), \"rb\" ))\n",
    "#         preds= gbm.predict(df_valid.drop(['day','volume'], axis =1))\n",
    "#         tst= df_valid[df_valid['day'].isin(d_valid)]\n",
    "#         tst['id'] = train_id.loc[tst.index]\n",
    "#         tst['preds'] = preds\n",
    "#         tst= tst.set_index(['id',\"day\"]).unstack()[\"preds\"].reset_index()\n",
    "#         tst = tst.fillna(0)\n",
    "\n",
    "#         val = pd.DataFrame()\n",
    "#         val['id'] = tmp_train['id']\n",
    "#         pred = pd.merge(val,tst, how = 'left')\n",
    "#         pred = pred.fillna(0)\n",
    "#         pred = pred.loc[:,d_valid]\n",
    "#         cv_scores.append(evaluator.score(pred))\n",
    "#         print(state, idx,\"WRMSSE :\", evaluator.score(pred))\n",
    "#     print(state, \"CV score\" , np.mean(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
