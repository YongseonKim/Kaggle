{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "calendar = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "price = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "# df_test = pd.read_csv('m5-forecasting-accuracy/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30490/30490 [00:36<00:00, 841.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# # startpoints 찾아서 이전 데이터 지우기\n",
    "startpoints = np.zeros(df_train.shape[0])\n",
    "for idx in tqdm(range(df_train.shape[0])):\n",
    "    startpoints[idx]= np.where(df_train.iloc[idx,6:].values>0)[0].min().astype(int)\n",
    "start_dict = dict(zip(df_train['id'], startpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = []\n",
    "cat_cols = []\n",
    "drop_cols += ['date','d','id']\n",
    "tr_last = 1913\n",
    "# F_1~28 만들기  1914~1941 까지 \n",
    "for i in range(tr_last+1, tr_last+1+28):   df_train['d_%s'%i] = 0\n",
    "\n",
    "# # Unpivot\n",
    "df_train = pd.melt(df_train, id_vars=df_train.columns[:6], value_vars=df_train.columns[6:],\n",
    "       var_name = 'day', value_name = 'volume')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, calendar, left_on = 'day', right_on ='d')\n",
    "# snap 합치기\n",
    "snap = np.zeros(df_train.shape[0])\n",
    "snap[df_train[(df_train['state_id']=='CA')&(df_train['snap_CA']==1)].index] +=1\n",
    "snap[df_train[(df_train['state_id']=='TX')&(df_train['snap_TX']==1)].index] +=1\n",
    "snap[df_train[(df_train['state_id']=='WI')&(df_train['snap_WI']==1)].index] +=1\n",
    "df_train['snap'] = snap\n",
    "drop_cols += ['snap_CA','snap_TX','snap_WI']\n",
    "\n",
    "\n",
    "cat_cols += [ 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "#               'wday', 'month', 'year', # 이게 크리티컬?\n",
    "            'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap']\n",
    "\n",
    "\n",
    "# ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     46816555\n",
      "False       65122\n",
      "Name: startpoints, dtype: int64\n",
      "(46816555, 26)\n"
     ]
    }
   ],
   "source": [
    "# Sell price\n",
    "df_train.head()\n",
    "df_train = pd.merge(df_train, price)\n",
    "\n",
    "# # Start point 찾기 ::  0.1% 데이터를 날릴  수 있다. \n",
    "\n",
    "df_train['startpoint'] = df_train['id'].map(start_dict).astype(int)#.astype(str)\n",
    "df_train['startpoints'] = df_train['day'].str.slice(start=2).astype(int) >=df_train['startpoint']\n",
    "print(df_train['startpoints'].value_counts())\n",
    "df_train = df_train[df_train['startpoints']]\n",
    "print(df_train.shape)\n",
    "df_train.drop(['startpoint','startpoints'],axis =1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 왜 Shift를 해야할까?\n",
    "# Shift 28을 하지 않으면 예측값이 뒤로 가면갈 수록  F1->F28 예측 할 수 있는 변수가 줄어든게 된다.\n",
    "# 28일은 한달을 의미한다. 최근 한달간의 경향성을 보는 것으로 보면 되겠다.\n",
    "# 28일을 56일로 늘리면 안되나? - 최근 한달간의 경향성이 반영이 안되는 것이낙?\n",
    "# https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50\n",
    "\n",
    "df_train['volume_7'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(7)\n",
    "df_train['volume_28'] = df_train[['id','volume']].groupby(\"id\")['volume'].shift(28)\n",
    "\n",
    "print(\"mean\")\n",
    "\n",
    "df_train['rmean_7_7'] = df_train[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(7).mean())\n",
    "df_train['rmean_7_28'] = df_train[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(28).mean())\n",
    "df_train['rmean_7_50'] = df_train[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "df_train['rmean_28_7'] = df_train[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(7).mean())\n",
    "df_train['rmean_28_28'] = df_train[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(28).mean())\n",
    "df_train['rmean_28_50'] = df_train[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "# # print(\"std\")\n",
    "# # full_df['rstd_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7 ,min_periods=1).std())\n",
    "# # full_df['rstd_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28 ,min_periods=1).std())\n",
    "# # full_df['rstd_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(50 ,min_periods=1).std())\n",
    "\n",
    "# # print(\"max\")\n",
    "# # full_df['rmax_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7 ,min_periods=1).max())\n",
    "# # full_df['rmax_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28 ,min_periods=1).max())\n",
    "# # full_df['rmax_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(50 ,min_periods=1).max())\n",
    "\n",
    "# # print(\"min\")\n",
    "# # full_df['rmin_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7 ,min_periods=1).min())\n",
    "# # full_df['rmin_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28 ,min_periods=1).min())\n",
    "# # full_df['rmin_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(50 ,min_periods=1).min())\n",
    "\n",
    "# # print(\"count\")\n",
    "# # full_df['rcnt_7'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(7).count() if x>0).fllna(0)\n",
    "# # full_df['rcnt_28'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28).count() if x>0).fllna(0)\n",
    "# # full_df['rcnt_50'] = full_df[['id','volume']].groupby(\"id\")['volume'].transform(lambda x: x.rolling(28).count() if x>0).fllna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] =  pd.to_datetime(df_train[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['week'] = getattr(df_train[\"date\"].dt, \"weekofyear\").astype(\"int16\")\n",
    "df_train['quarter'] = getattr(df_train[\"date\"].dt,\"quarter\").astype(\"int16\")\n",
    "df_train['mday'] = getattr(df_train[\"date\"].dt, \"day\").astype(\"int16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46816555, 35)\n"
     ]
    }
   ],
   "source": [
    "cols =['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "df_train[cols]= df_train[cols].fillna('NaN')\n",
    "print(df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44468825, 35)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'd', 'id', 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk', 'weekday']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols += [\"wm_yr_wk\", \"weekday\"]  ## 이게 문제?\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "tr_last = 1913\n",
    "testday = ['d_%s'% x for x in range(tr_last+1, tr_last+1+28)]\n",
    "train_id = df_train['id']\n",
    "df_test_id = df_train[df_train['day'].isin(testday)]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df_train.drop(drop_cols,axis =1 )\n",
    "# df_test =df_test.drop(drop_cols,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_7_50</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>rmean_28_50</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010975</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.90</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010976</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.66</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010977</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010978</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_81</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010979</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_82</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               item_id    dept_id   cat_id store_id state_id   day  volume  \\\n",
       "1010975  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_78       0   \n",
       "1010976  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_79       0   \n",
       "1010977  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_80       0   \n",
       "1010978  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_81       5   \n",
       "1010979  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1       CA  d_82      23   \n",
       "\n",
       "         wday  month  year  ... volume_28 rmean_7_7 rmean_7_28 rmean_7_50  \\\n",
       "1010975     1      4  2011  ...       0.0  6.857143   1.785714       1.88   \n",
       "1010976     2      4  2011  ...       0.0  4.000000   1.714286       1.68   \n",
       "1010977     3      4  2011  ...       0.0  5.142857   2.000000       1.76   \n",
       "1010978     4      4  2011  ...       0.0  5.714286   2.214286       1.80   \n",
       "1010979     5      4  2011  ...       0.0  4.714286   2.428571       1.88   \n",
       "\n",
       "         rmean_28_7  rmean_28_28  rmean_28_50  week  quarter  mday  \n",
       "1010975    0.285714     1.285714         2.90    15        2    16  \n",
       "1010976    0.000000     1.142857         2.66    15        2    17  \n",
       "1010977    0.000000     1.000000         2.36    16        2    18  \n",
       "1010978    0.000000     0.928571         2.36    16        2    19  \n",
       "1010979    0.000000     0.714286         2.36    16        2    20  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:03<00:00,  6.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Encoding\n",
    "for col in tqdm(cat_cols) :  # encoding -1이 문제?\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col]).astype(np.int8)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 분리하기\n",
    "testday = ['d_%s'% x for x in range(tr_last+1, tr_last+1+28)]\n",
    "df_test = df_train.copy()\n",
    "df_train = df_train[~df_train['day'].isin(testday)]\n",
    "# train_col = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day', 'volume', 'wday', 'month', 'year',\n",
    "#  'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap', 'sell_price', 'volume_7', 'volume_28',\n",
    "#  'rmean_7_7', 'rmean_7_28', 'rmean_7_50', 'rmean_28_7', 'rmean_28_28', 'rmean_28_50', 'week', 'quarter', 'mday']\n",
    "# df_train = df_train.loc[:,train_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_7_50</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>rmean_28_50</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010975</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.90</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010976</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.66</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010977</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010978</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_81</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010979</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_82</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  dept_id  cat_id  store_id  state_id   day  volume  wday  \\\n",
       "1010975      -92        3       1         0         0  d_78       0     1   \n",
       "1010976      -92        3       1         0         0  d_79       0     2   \n",
       "1010977      -92        3       1         0         0  d_80       0     3   \n",
       "1010978      -92        3       1         0         0  d_81       5     4   \n",
       "1010979      -92        3       1         0         0  d_82      23     5   \n",
       "\n",
       "         month  year  ...  volume_28  rmean_7_7  rmean_7_28  rmean_7_50  \\\n",
       "1010975      4  2011  ...        0.0   6.857143    1.785714        1.88   \n",
       "1010976      4  2011  ...        0.0   4.000000    1.714286        1.68   \n",
       "1010977      4  2011  ...        0.0   5.142857    2.000000        1.76   \n",
       "1010978      4  2011  ...        0.0   5.714286    2.214286        1.80   \n",
       "1010979      4  2011  ...        0.0   4.714286    2.428571        1.88   \n",
       "\n",
       "         rmean_28_7  rmean_28_28  rmean_28_50  week  quarter  mday  \n",
       "1010975    0.285714     1.285714         2.90    15        2    16  \n",
       "1010976    0.000000     1.142857         2.66    15        2    17  \n",
       "1010977    0.000000     1.000000         2.36    16        2    18  \n",
       "1010978    0.000000     0.928571         2.36    16        2    19  \n",
       "1010979    0.000000     0.714286         2.36    16        2    20  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train , x_valid = train_test_split(df_train, test_size =0.15, random_state = 99)\n",
    "# y_train, y_valid = x_train['volume'], x_valid['volume']\n",
    "\n",
    "# x_train = x_train.drop(['day','volume'], axis =1)\n",
    "# x_valid = x_valid.drop(['day','volume'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_7_50</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>rmean_28_50</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010975</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.90</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010976</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.66</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010977</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010978</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_81</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010979</th>\n",
       "      <td>-92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_82</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  dept_id  cat_id  store_id  state_id   day  volume  wday  \\\n",
       "1010975      -92        3       1         0         0  d_78       0     1   \n",
       "1010976      -92        3       1         0         0  d_79       0     2   \n",
       "1010977      -92        3       1         0         0  d_80       0     3   \n",
       "1010978      -92        3       1         0         0  d_81       5     4   \n",
       "1010979      -92        3       1         0         0  d_82      23     5   \n",
       "\n",
       "         month  year  ...  volume_28  rmean_7_7  rmean_7_28  rmean_7_50  \\\n",
       "1010975      4  2011  ...        0.0   6.857143    1.785714        1.88   \n",
       "1010976      4  2011  ...        0.0   4.000000    1.714286        1.68   \n",
       "1010977      4  2011  ...        0.0   5.142857    2.000000        1.76   \n",
       "1010978      4  2011  ...        0.0   5.714286    2.214286        1.80   \n",
       "1010979      4  2011  ...        0.0   4.714286    2.428571        1.88   \n",
       "\n",
       "         rmean_28_7  rmean_28_28  rmean_28_50  week  quarter  mday  \n",
       "1010975    0.285714     1.285714         2.90    15        2    16  \n",
       "1010976    0.000000     1.142857         2.66    15        2    17  \n",
       "1010977    0.000000     1.000000         2.36    16        2    18  \n",
       "1010978    0.000000     0.928571         2.36    16        2    19  \n",
       "1010979    0.000000     0.714286         2.36    16        2    20  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 28일 데이터 빼고 학습\n",
    "cols=[f\"d_{i}\" for i in range(1913-28,1914)]\n",
    "df_valid = df_train[df_train['day'].isin(cols)]\n",
    "df_train = df_train[~df_train['day'].isin(cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [999,243,498,45,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"objective\" : \"poisson\",\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.075,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "        \"sub_row\" : 0.75,\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "#         \"nthread\" : 4\n",
    "        \"metric\": [\"rmse\"],\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 5000,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "        'n_jobs' :10 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.59317\tvalid_1's rmse: 2.62731\n",
      "[100]\ttraining's rmse: 2.5079\tvalid_1's rmse: 2.54336\n",
      "[150]\ttraining's rmse: 2.48386\tvalid_1's rmse: 2.52345\n",
      "[200]\ttraining's rmse: 2.46293\tvalid_1's rmse: 2.50677\n",
      "[250]\ttraining's rmse: 2.44412\tvalid_1's rmse: 2.49258\n",
      "[300]\ttraining's rmse: 2.42672\tvalid_1's rmse: 2.47989\n",
      "[350]\ttraining's rmse: 2.40967\tvalid_1's rmse: 2.46834\n",
      "[400]\ttraining's rmse: 2.39722\tvalid_1's rmse: 2.46067\n",
      "[450]\ttraining's rmse: 2.38676\tvalid_1's rmse: 2.45517\n",
      "[500]\ttraining's rmse: 2.37641\tvalid_1's rmse: 2.44957\n",
      "[550]\ttraining's rmse: 2.36641\tvalid_1's rmse: 2.4444\n",
      "[600]\ttraining's rmse: 2.3582\tvalid_1's rmse: 2.44006\n",
      "[650]\ttraining's rmse: 2.35078\tvalid_1's rmse: 2.43665\n",
      "[700]\ttraining's rmse: 2.34326\tvalid_1's rmse: 2.43341\n",
      "[750]\ttraining's rmse: 2.33641\tvalid_1's rmse: 2.43059\n",
      "[800]\ttraining's rmse: 2.33008\tvalid_1's rmse: 2.42786\n",
      "[850]\ttraining's rmse: 2.32284\tvalid_1's rmse: 2.4248\n",
      "[900]\ttraining's rmse: 2.31813\tvalid_1's rmse: 2.42304\n",
      "[950]\ttraining's rmse: 2.31279\tvalid_1's rmse: 2.42083\n",
      "[1000]\ttraining's rmse: 2.30822\tvalid_1's rmse: 2.41926\n",
      "[1050]\ttraining's rmse: 2.30338\tvalid_1's rmse: 2.4173\n",
      "[1100]\ttraining's rmse: 2.29779\tvalid_1's rmse: 2.41563\n",
      "[1150]\ttraining's rmse: 2.29338\tvalid_1's rmse: 2.41415\n",
      "[1200]\ttraining's rmse: 2.28876\tvalid_1's rmse: 2.41268\n",
      "[1250]\ttraining's rmse: 2.28458\tvalid_1's rmse: 2.41166\n",
      "[1300]\ttraining's rmse: 2.28001\tvalid_1's rmse: 2.41025\n",
      "[1350]\ttraining's rmse: 2.27553\tvalid_1's rmse: 2.40892\n",
      "[1400]\ttraining's rmse: 2.27116\tvalid_1's rmse: 2.40734\n",
      "[1450]\ttraining's rmse: 2.26641\tvalid_1's rmse: 2.40604\n",
      "[1500]\ttraining's rmse: 2.26215\tvalid_1's rmse: 2.40463\n",
      "[1550]\ttraining's rmse: 2.25842\tvalid_1's rmse: 2.40335\n",
      "[1600]\ttraining's rmse: 2.25463\tvalid_1's rmse: 2.40205\n",
      "[1650]\ttraining's rmse: 2.25047\tvalid_1's rmse: 2.40086\n",
      "[1700]\ttraining's rmse: 2.24683\tvalid_1's rmse: 2.39981\n",
      "[1750]\ttraining's rmse: 2.24238\tvalid_1's rmse: 2.39859\n",
      "[1800]\ttraining's rmse: 2.2383\tvalid_1's rmse: 2.39757\n",
      "[1850]\ttraining's rmse: 2.23532\tvalid_1's rmse: 2.39657\n",
      "[1900]\ttraining's rmse: 2.23158\tvalid_1's rmse: 2.39564\n",
      "[1950]\ttraining's rmse: 2.22853\tvalid_1's rmse: 2.395\n",
      "[2000]\ttraining's rmse: 2.22561\tvalid_1's rmse: 2.39445\n",
      "[2050]\ttraining's rmse: 2.22274\tvalid_1's rmse: 2.39376\n",
      "[2100]\ttraining's rmse: 2.21987\tvalid_1's rmse: 2.39287\n",
      "[2150]\ttraining's rmse: 2.21656\tvalid_1's rmse: 2.39194\n",
      "[2200]\ttraining's rmse: 2.21359\tvalid_1's rmse: 2.39107\n",
      "[2250]\ttraining's rmse: 2.21066\tvalid_1's rmse: 2.39018\n",
      "[2300]\ttraining's rmse: 2.20813\tvalid_1's rmse: 2.3897\n",
      "[2350]\ttraining's rmse: 2.20502\tvalid_1's rmse: 2.38895\n",
      "[2400]\ttraining's rmse: 2.20181\tvalid_1's rmse: 2.38807\n",
      "[2450]\ttraining's rmse: 2.19896\tvalid_1's rmse: 2.38752\n",
      "[2500]\ttraining's rmse: 2.19606\tvalid_1's rmse: 2.38677\n",
      "[2550]\ttraining's rmse: 2.19364\tvalid_1's rmse: 2.38613\n",
      "[2600]\ttraining's rmse: 2.19097\tvalid_1's rmse: 2.38574\n",
      "[2650]\ttraining's rmse: 2.18861\tvalid_1's rmse: 2.38517\n",
      "[2700]\ttraining's rmse: 2.18606\tvalid_1's rmse: 2.38465\n",
      "[2750]\ttraining's rmse: 2.18389\tvalid_1's rmse: 2.38404\n",
      "[2800]\ttraining's rmse: 2.18072\tvalid_1's rmse: 2.3832\n",
      "[2850]\ttraining's rmse: 2.17848\tvalid_1's rmse: 2.38256\n",
      "[2900]\ttraining's rmse: 2.17552\tvalid_1's rmse: 2.38186\n",
      "[2950]\ttraining's rmse: 2.17313\tvalid_1's rmse: 2.38111\n",
      "[3000]\ttraining's rmse: 2.17029\tvalid_1's rmse: 2.38039\n",
      "[3050]\ttraining's rmse: 2.1681\tvalid_1's rmse: 2.38015\n",
      "[3100]\ttraining's rmse: 2.16589\tvalid_1's rmse: 2.37968\n",
      "[3150]\ttraining's rmse: 2.16355\tvalid_1's rmse: 2.37924\n",
      "[3200]\ttraining's rmse: 2.16111\tvalid_1's rmse: 2.37863\n",
      "[3250]\ttraining's rmse: 2.15879\tvalid_1's rmse: 2.37819\n",
      "[3300]\ttraining's rmse: 2.15616\tvalid_1's rmse: 2.37774\n",
      "[3350]\ttraining's rmse: 2.15415\tvalid_1's rmse: 2.37744\n",
      "[3400]\ttraining's rmse: 2.15174\tvalid_1's rmse: 2.37687\n",
      "[3450]\ttraining's rmse: 2.14966\tvalid_1's rmse: 2.37659\n",
      "[3500]\ttraining's rmse: 2.14751\tvalid_1's rmse: 2.37607\n",
      "[3550]\ttraining's rmse: 2.14569\tvalid_1's rmse: 2.37572\n",
      "[3600]\ttraining's rmse: 2.14371\tvalid_1's rmse: 2.37536\n",
      "[3650]\ttraining's rmse: 2.14202\tvalid_1's rmse: 2.37505\n",
      "[3700]\ttraining's rmse: 2.13999\tvalid_1's rmse: 2.37473\n",
      "[3750]\ttraining's rmse: 2.13805\tvalid_1's rmse: 2.37442\n",
      "[3800]\ttraining's rmse: 2.1357\tvalid_1's rmse: 2.37383\n",
      "[3850]\ttraining's rmse: 2.13312\tvalid_1's rmse: 2.37337\n",
      "[3900]\ttraining's rmse: 2.13086\tvalid_1's rmse: 2.37303\n",
      "[3950]\ttraining's rmse: 2.12905\tvalid_1's rmse: 2.37282\n",
      "[4000]\ttraining's rmse: 2.12688\tvalid_1's rmse: 2.3724\n",
      "[4050]\ttraining's rmse: 2.12499\tvalid_1's rmse: 2.37193\n",
      "[4100]\ttraining's rmse: 2.123\tvalid_1's rmse: 2.37148\n",
      "[4150]\ttraining's rmse: 2.12108\tvalid_1's rmse: 2.37126\n",
      "[4200]\ttraining's rmse: 2.11895\tvalid_1's rmse: 2.37093\n",
      "[4250]\ttraining's rmse: 2.1168\tvalid_1's rmse: 2.37046\n",
      "[4300]\ttraining's rmse: 2.11506\tvalid_1's rmse: 2.37038\n",
      "[4350]\ttraining's rmse: 2.11335\tvalid_1's rmse: 2.37\n",
      "[4400]\ttraining's rmse: 2.11164\tvalid_1's rmse: 2.36972\n",
      "[4450]\ttraining's rmse: 2.1103\tvalid_1's rmse: 2.36933\n",
      "[4500]\ttraining's rmse: 2.10856\tvalid_1's rmse: 2.36901\n",
      "[4550]\ttraining's rmse: 2.10703\tvalid_1's rmse: 2.36863\n",
      "[4600]\ttraining's rmse: 2.10501\tvalid_1's rmse: 2.36822\n",
      "[4650]\ttraining's rmse: 2.10346\tvalid_1's rmse: 2.3679\n",
      "[4700]\ttraining's rmse: 2.10169\tvalid_1's rmse: 2.36751\n",
      "[4750]\ttraining's rmse: 2.10005\tvalid_1's rmse: 2.36715\n",
      "[4800]\ttraining's rmse: 2.09812\tvalid_1's rmse: 2.36667\n",
      "[4850]\ttraining's rmse: 2.09648\tvalid_1's rmse: 2.36648\n",
      "[4900]\ttraining's rmse: 2.09466\tvalid_1's rmse: 2.36632\n",
      "[4950]\ttraining's rmse: 2.09314\tvalid_1's rmse: 2.36604\n",
      "[5000]\ttraining's rmse: 2.09149\tvalid_1's rmse: 2.36587\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09149\tvalid_1's rmse: 2.36587\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.58072\tvalid_1's rmse: 2.58479\n",
      "[100]\ttraining's rmse: 2.50907\tvalid_1's rmse: 2.51778\n",
      "[150]\ttraining's rmse: 2.48887\tvalid_1's rmse: 2.50053\n",
      "[200]\ttraining's rmse: 2.46878\tvalid_1's rmse: 2.48388\n",
      "[250]\ttraining's rmse: 2.45125\tvalid_1's rmse: 2.47096\n",
      "[300]\ttraining's rmse: 2.43474\tvalid_1's rmse: 2.45902\n",
      "[350]\ttraining's rmse: 2.41911\tvalid_1's rmse: 2.44799\n",
      "[400]\ttraining's rmse: 2.40624\tvalid_1's rmse: 2.43963\n",
      "[450]\ttraining's rmse: 2.39581\tvalid_1's rmse: 2.4339\n",
      "[500]\ttraining's rmse: 2.38592\tvalid_1's rmse: 2.42831\n",
      "[550]\ttraining's rmse: 2.37843\tvalid_1's rmse: 2.4249\n",
      "[600]\ttraining's rmse: 2.37001\tvalid_1's rmse: 2.42084\n",
      "[650]\ttraining's rmse: 2.36219\tvalid_1's rmse: 2.4177\n",
      "[700]\ttraining's rmse: 2.35501\tvalid_1's rmse: 2.41459\n",
      "[750]\ttraining's rmse: 2.34821\tvalid_1's rmse: 2.41153\n",
      "[800]\ttraining's rmse: 2.34199\tvalid_1's rmse: 2.40915\n",
      "[850]\ttraining's rmse: 2.33571\tvalid_1's rmse: 2.40668\n",
      "[900]\ttraining's rmse: 2.3288\tvalid_1's rmse: 2.40404\n",
      "[950]\ttraining's rmse: 2.32352\tvalid_1's rmse: 2.40236\n",
      "[1000]\ttraining's rmse: 2.31654\tvalid_1's rmse: 2.3997\n",
      "[1050]\ttraining's rmse: 2.31104\tvalid_1's rmse: 2.39812\n",
      "[1100]\ttraining's rmse: 2.30556\tvalid_1's rmse: 2.39608\n",
      "[1150]\ttraining's rmse: 2.30023\tvalid_1's rmse: 2.39414\n",
      "[1200]\ttraining's rmse: 2.2946\tvalid_1's rmse: 2.39233\n",
      "[1250]\ttraining's rmse: 2.29027\tvalid_1's rmse: 2.39107\n",
      "[1300]\ttraining's rmse: 2.28561\tvalid_1's rmse: 2.38937\n",
      "[1350]\ttraining's rmse: 2.28086\tvalid_1's rmse: 2.3879\n",
      "[1400]\ttraining's rmse: 2.27693\tvalid_1's rmse: 2.38705\n",
      "[1450]\ttraining's rmse: 2.27271\tvalid_1's rmse: 2.38583\n",
      "[1500]\ttraining's rmse: 2.26849\tvalid_1's rmse: 2.38487\n",
      "[1550]\ttraining's rmse: 2.26494\tvalid_1's rmse: 2.38388\n",
      "[1600]\ttraining's rmse: 2.26069\tvalid_1's rmse: 2.38272\n",
      "[1650]\ttraining's rmse: 2.25608\tvalid_1's rmse: 2.38149\n",
      "[1700]\ttraining's rmse: 2.25202\tvalid_1's rmse: 2.38081\n",
      "[1750]\ttraining's rmse: 2.24841\tvalid_1's rmse: 2.37992\n",
      "[1800]\ttraining's rmse: 2.24519\tvalid_1's rmse: 2.37919\n",
      "[1850]\ttraining's rmse: 2.24118\tvalid_1's rmse: 2.37785\n",
      "[1900]\ttraining's rmse: 2.23783\tvalid_1's rmse: 2.37687\n",
      "[1950]\ttraining's rmse: 2.23396\tvalid_1's rmse: 2.37583\n",
      "[2000]\ttraining's rmse: 2.22987\tvalid_1's rmse: 2.37478\n",
      "[2050]\ttraining's rmse: 2.22629\tvalid_1's rmse: 2.37374\n",
      "[2100]\ttraining's rmse: 2.22222\tvalid_1's rmse: 2.37268\n",
      "[2150]\ttraining's rmse: 2.2195\tvalid_1's rmse: 2.37218\n",
      "[2200]\ttraining's rmse: 2.21626\tvalid_1's rmse: 2.37144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.21243\tvalid_1's rmse: 2.37037\n",
      "[2300]\ttraining's rmse: 2.20985\tvalid_1's rmse: 2.36984\n",
      "[2350]\ttraining's rmse: 2.20758\tvalid_1's rmse: 2.36934\n",
      "[2400]\ttraining's rmse: 2.20465\tvalid_1's rmse: 2.36881\n",
      "[2450]\ttraining's rmse: 2.20185\tvalid_1's rmse: 2.36827\n",
      "[2500]\ttraining's rmse: 2.19953\tvalid_1's rmse: 2.36758\n",
      "[2550]\ttraining's rmse: 2.19716\tvalid_1's rmse: 2.36714\n",
      "[2600]\ttraining's rmse: 2.19441\tvalid_1's rmse: 2.36664\n",
      "[2650]\ttraining's rmse: 2.19238\tvalid_1's rmse: 2.36598\n",
      "[2700]\ttraining's rmse: 2.18928\tvalid_1's rmse: 2.36554\n",
      "[2750]\ttraining's rmse: 2.18654\tvalid_1's rmse: 2.36509\n",
      "[2800]\ttraining's rmse: 2.18374\tvalid_1's rmse: 2.3646\n",
      "[2850]\ttraining's rmse: 2.18119\tvalid_1's rmse: 2.36403\n",
      "[2900]\ttraining's rmse: 2.17908\tvalid_1's rmse: 2.36371\n",
      "Early stopping, best iteration is:\n",
      "[2888]\ttraining's rmse: 2.17955\tvalid_1's rmse: 2.36359\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.61761\tvalid_1's rmse: 2.63063\n",
      "[100]\ttraining's rmse: 2.51906\tvalid_1's rmse: 2.538\n",
      "[150]\ttraining's rmse: 2.49391\tvalid_1's rmse: 2.51693\n",
      "[200]\ttraining's rmse: 2.47339\tvalid_1's rmse: 2.50177\n",
      "[250]\ttraining's rmse: 2.4535\tvalid_1's rmse: 2.48745\n",
      "[300]\ttraining's rmse: 2.43731\tvalid_1's rmse: 2.47615\n",
      "[350]\ttraining's rmse: 2.42222\tvalid_1's rmse: 2.46689\n",
      "[400]\ttraining's rmse: 2.40756\tvalid_1's rmse: 2.45797\n",
      "[450]\ttraining's rmse: 2.39688\tvalid_1's rmse: 2.45167\n",
      "[500]\ttraining's rmse: 2.38602\tvalid_1's rmse: 2.44647\n",
      "[550]\ttraining's rmse: 2.37663\tvalid_1's rmse: 2.44209\n",
      "[600]\ttraining's rmse: 2.36718\tvalid_1's rmse: 2.43725\n",
      "[650]\ttraining's rmse: 2.36043\tvalid_1's rmse: 2.43412\n",
      "[700]\ttraining's rmse: 2.35308\tvalid_1's rmse: 2.43045\n",
      "[750]\ttraining's rmse: 2.34547\tvalid_1's rmse: 2.42762\n",
      "[800]\ttraining's rmse: 2.33978\tvalid_1's rmse: 2.42525\n",
      "[850]\ttraining's rmse: 2.33386\tvalid_1's rmse: 2.42298\n",
      "[900]\ttraining's rmse: 2.32759\tvalid_1's rmse: 2.42043\n",
      "[950]\ttraining's rmse: 2.32135\tvalid_1's rmse: 2.41782\n",
      "[1000]\ttraining's rmse: 2.31656\tvalid_1's rmse: 2.41576\n",
      "[1050]\ttraining's rmse: 2.31142\tvalid_1's rmse: 2.41418\n",
      "[1100]\ttraining's rmse: 2.30545\tvalid_1's rmse: 2.41204\n",
      "[1150]\ttraining's rmse: 2.2994\tvalid_1's rmse: 2.41016\n",
      "[1200]\ttraining's rmse: 2.2943\tvalid_1's rmse: 2.40848\n",
      "[1250]\ttraining's rmse: 2.28971\tvalid_1's rmse: 2.40709\n",
      "[1300]\ttraining's rmse: 2.28531\tvalid_1's rmse: 2.40574\n",
      "[1350]\ttraining's rmse: 2.28092\tvalid_1's rmse: 2.40443\n",
      "[1400]\ttraining's rmse: 2.27647\tvalid_1's rmse: 2.40313\n",
      "[1450]\ttraining's rmse: 2.27284\tvalid_1's rmse: 2.40234\n",
      "[1500]\ttraining's rmse: 2.26807\tvalid_1's rmse: 2.401\n",
      "[1550]\ttraining's rmse: 2.26395\tvalid_1's rmse: 2.3998\n",
      "[1600]\ttraining's rmse: 2.26059\tvalid_1's rmse: 2.39896\n",
      "[1650]\ttraining's rmse: 2.25655\tvalid_1's rmse: 2.39779\n",
      "[1700]\ttraining's rmse: 2.25258\tvalid_1's rmse: 2.39654\n",
      "[1750]\ttraining's rmse: 2.24855\tvalid_1's rmse: 2.39542\n",
      "[1800]\ttraining's rmse: 2.2444\tvalid_1's rmse: 2.39414\n",
      "[1850]\ttraining's rmse: 2.2403\tvalid_1's rmse: 2.39289\n",
      "[1900]\ttraining's rmse: 2.23676\tvalid_1's rmse: 2.39216\n",
      "[1950]\ttraining's rmse: 2.23359\tvalid_1's rmse: 2.39127\n",
      "[2000]\ttraining's rmse: 2.23015\tvalid_1's rmse: 2.39034\n",
      "[2050]\ttraining's rmse: 2.22615\tvalid_1's rmse: 2.38945\n",
      "[2100]\ttraining's rmse: 2.22268\tvalid_1's rmse: 2.38863\n",
      "[2150]\ttraining's rmse: 2.21949\tvalid_1's rmse: 2.38797\n",
      "[2200]\ttraining's rmse: 2.21655\tvalid_1's rmse: 2.38726\n",
      "[2250]\ttraining's rmse: 2.21342\tvalid_1's rmse: 2.38683\n",
      "[2300]\ttraining's rmse: 2.21078\tvalid_1's rmse: 2.38626\n",
      "[2350]\ttraining's rmse: 2.20755\tvalid_1's rmse: 2.38527\n",
      "[2400]\ttraining's rmse: 2.20449\tvalid_1's rmse: 2.38458\n",
      "[2450]\ttraining's rmse: 2.20173\tvalid_1's rmse: 2.38421\n",
      "[2500]\ttraining's rmse: 2.19852\tvalid_1's rmse: 2.38358\n",
      "[2550]\ttraining's rmse: 2.19563\tvalid_1's rmse: 2.38298\n",
      "[2600]\ttraining's rmse: 2.19297\tvalid_1's rmse: 2.38209\n",
      "[2650]\ttraining's rmse: 2.19063\tvalid_1's rmse: 2.38164\n",
      "[2700]\ttraining's rmse: 2.18776\tvalid_1's rmse: 2.38114\n",
      "[2750]\ttraining's rmse: 2.18478\tvalid_1's rmse: 2.38038\n",
      "[2800]\ttraining's rmse: 2.18278\tvalid_1's rmse: 2.37999\n",
      "[2850]\ttraining's rmse: 2.18056\tvalid_1's rmse: 2.37957\n",
      "[2900]\ttraining's rmse: 2.17833\tvalid_1's rmse: 2.37899\n",
      "[2950]\ttraining's rmse: 2.17628\tvalid_1's rmse: 2.37857\n",
      "[3000]\ttraining's rmse: 2.17341\tvalid_1's rmse: 2.37781\n",
      "[3050]\ttraining's rmse: 2.17061\tvalid_1's rmse: 2.37719\n",
      "[3100]\ttraining's rmse: 2.1679\tvalid_1's rmse: 2.37682\n",
      "[3150]\ttraining's rmse: 2.16555\tvalid_1's rmse: 2.3763\n",
      "[3200]\ttraining's rmse: 2.16315\tvalid_1's rmse: 2.37579\n",
      "[3250]\ttraining's rmse: 2.1608\tvalid_1's rmse: 2.37537\n",
      "[3300]\ttraining's rmse: 2.15844\tvalid_1's rmse: 2.37491\n",
      "[3350]\ttraining's rmse: 2.15619\tvalid_1's rmse: 2.37462\n",
      "[3400]\ttraining's rmse: 2.15342\tvalid_1's rmse: 2.3739\n",
      "[3450]\ttraining's rmse: 2.15079\tvalid_1's rmse: 2.37324\n",
      "[3500]\ttraining's rmse: 2.14855\tvalid_1's rmse: 2.37272\n",
      "[3550]\ttraining's rmse: 2.14648\tvalid_1's rmse: 2.37244\n",
      "[3600]\ttraining's rmse: 2.14436\tvalid_1's rmse: 2.372\n",
      "[3650]\ttraining's rmse: 2.14232\tvalid_1's rmse: 2.37151\n",
      "[3700]\ttraining's rmse: 2.14001\tvalid_1's rmse: 2.37096\n",
      "[3750]\ttraining's rmse: 2.13818\tvalid_1's rmse: 2.37067\n",
      "[3800]\ttraining's rmse: 2.13577\tvalid_1's rmse: 2.37015\n",
      "[3850]\ttraining's rmse: 2.13399\tvalid_1's rmse: 2.3698\n",
      "[3900]\ttraining's rmse: 2.13215\tvalid_1's rmse: 2.36943\n",
      "[3950]\ttraining's rmse: 2.13004\tvalid_1's rmse: 2.36897\n",
      "[4000]\ttraining's rmse: 2.12778\tvalid_1's rmse: 2.36865\n",
      "[4050]\ttraining's rmse: 2.12549\tvalid_1's rmse: 2.36829\n",
      "[4100]\ttraining's rmse: 2.12338\tvalid_1's rmse: 2.36779\n",
      "[4150]\ttraining's rmse: 2.12152\tvalid_1's rmse: 2.36752\n",
      "[4200]\ttraining's rmse: 2.11981\tvalid_1's rmse: 2.36712\n",
      "[4250]\ttraining's rmse: 2.11764\tvalid_1's rmse: 2.36675\n",
      "[4300]\ttraining's rmse: 2.11563\tvalid_1's rmse: 2.36658\n",
      "[4350]\ttraining's rmse: 2.11352\tvalid_1's rmse: 2.36616\n",
      "[4400]\ttraining's rmse: 2.11135\tvalid_1's rmse: 2.36572\n",
      "[4450]\ttraining's rmse: 2.1096\tvalid_1's rmse: 2.36542\n",
      "[4500]\ttraining's rmse: 2.10803\tvalid_1's rmse: 2.36522\n",
      "[4550]\ttraining's rmse: 2.10612\tvalid_1's rmse: 2.36507\n",
      "[4600]\ttraining's rmse: 2.10419\tvalid_1's rmse: 2.36462\n",
      "[4650]\ttraining's rmse: 2.10233\tvalid_1's rmse: 2.3643\n",
      "[4700]\ttraining's rmse: 2.10059\tvalid_1's rmse: 2.36401\n",
      "[4750]\ttraining's rmse: 2.09885\tvalid_1's rmse: 2.36379\n",
      "[4800]\ttraining's rmse: 2.09713\tvalid_1's rmse: 2.3635\n",
      "[4850]\ttraining's rmse: 2.0955\tvalid_1's rmse: 2.36325\n",
      "[4900]\ttraining's rmse: 2.0937\tvalid_1's rmse: 2.36304\n",
      "[4950]\ttraining's rmse: 2.09219\tvalid_1's rmse: 2.36286\n",
      "[5000]\ttraining's rmse: 2.09071\tvalid_1's rmse: 2.36267\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09071\tvalid_1's rmse: 2.36267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.58327\tvalid_1's rmse: 2.6003\n",
      "[100]\ttraining's rmse: 2.50188\tvalid_1's rmse: 2.51902\n",
      "[150]\ttraining's rmse: 2.47887\tvalid_1's rmse: 2.50075\n",
      "[200]\ttraining's rmse: 2.45797\tvalid_1's rmse: 2.48531\n",
      "[250]\ttraining's rmse: 2.43974\tvalid_1's rmse: 2.47287\n",
      "[300]\ttraining's rmse: 2.42336\tvalid_1's rmse: 2.46277\n",
      "[350]\ttraining's rmse: 2.40995\tvalid_1's rmse: 2.45462\n",
      "[400]\ttraining's rmse: 2.39671\tvalid_1's rmse: 2.44765\n",
      "[450]\ttraining's rmse: 2.38405\tvalid_1's rmse: 2.4409\n",
      "[500]\ttraining's rmse: 2.37412\tvalid_1's rmse: 2.43625\n",
      "[550]\ttraining's rmse: 2.36559\tvalid_1's rmse: 2.43223\n",
      "[600]\ttraining's rmse: 2.3585\tvalid_1's rmse: 2.42877\n",
      "[650]\ttraining's rmse: 2.34927\tvalid_1's rmse: 2.42474\n",
      "[700]\ttraining's rmse: 2.3431\tvalid_1's rmse: 2.42252\n",
      "[750]\ttraining's rmse: 2.33648\tvalid_1's rmse: 2.42007\n",
      "[800]\ttraining's rmse: 2.32922\tvalid_1's rmse: 2.41742\n",
      "[850]\ttraining's rmse: 2.32357\tvalid_1's rmse: 2.41561\n",
      "[900]\ttraining's rmse: 2.31781\tvalid_1's rmse: 2.41353\n",
      "[950]\ttraining's rmse: 2.31204\tvalid_1's rmse: 2.41208\n",
      "[1000]\ttraining's rmse: 2.30667\tvalid_1's rmse: 2.41041\n",
      "[1050]\ttraining's rmse: 2.30176\tvalid_1's rmse: 2.40857\n",
      "[1100]\ttraining's rmse: 2.29767\tvalid_1's rmse: 2.40705\n",
      "[1150]\ttraining's rmse: 2.2923\tvalid_1's rmse: 2.40516\n",
      "[1200]\ttraining's rmse: 2.28742\tvalid_1's rmse: 2.40364\n",
      "[1250]\ttraining's rmse: 2.28332\tvalid_1's rmse: 2.40226\n",
      "[1300]\ttraining's rmse: 2.27778\tvalid_1's rmse: 2.40063\n",
      "[1350]\ttraining's rmse: 2.27349\tvalid_1's rmse: 2.3994\n",
      "[1400]\ttraining's rmse: 2.26901\tvalid_1's rmse: 2.39793\n",
      "[1450]\ttraining's rmse: 2.26489\tvalid_1's rmse: 2.39685\n",
      "[1500]\ttraining's rmse: 2.26023\tvalid_1's rmse: 2.3955\n",
      "[1550]\ttraining's rmse: 2.2569\tvalid_1's rmse: 2.39452\n",
      "[1600]\ttraining's rmse: 2.25299\tvalid_1's rmse: 2.39356\n",
      "[1650]\ttraining's rmse: 2.24904\tvalid_1's rmse: 2.39233\n",
      "[1700]\ttraining's rmse: 2.24543\tvalid_1's rmse: 2.39139\n",
      "[1750]\ttraining's rmse: 2.24134\tvalid_1's rmse: 2.39045\n",
      "[1800]\ttraining's rmse: 2.23843\tvalid_1's rmse: 2.38957\n",
      "[1850]\ttraining's rmse: 2.2349\tvalid_1's rmse: 2.38865\n",
      "[1900]\ttraining's rmse: 2.23136\tvalid_1's rmse: 2.38741\n",
      "[1950]\ttraining's rmse: 2.22802\tvalid_1's rmse: 2.38668\n",
      "[2000]\ttraining's rmse: 2.22467\tvalid_1's rmse: 2.38558\n",
      "[2050]\ttraining's rmse: 2.22218\tvalid_1's rmse: 2.385\n",
      "[2100]\ttraining's rmse: 2.21916\tvalid_1's rmse: 2.38412\n",
      "[2150]\ttraining's rmse: 2.21627\tvalid_1's rmse: 2.38332\n",
      "[2200]\ttraining's rmse: 2.21313\tvalid_1's rmse: 2.38231\n",
      "[2250]\ttraining's rmse: 2.20981\tvalid_1's rmse: 2.38133\n",
      "[2300]\ttraining's rmse: 2.20672\tvalid_1's rmse: 2.38081\n",
      "[2350]\ttraining's rmse: 2.20395\tvalid_1's rmse: 2.38019\n",
      "[2400]\ttraining's rmse: 2.20106\tvalid_1's rmse: 2.37935\n",
      "[2450]\ttraining's rmse: 2.19801\tvalid_1's rmse: 2.3787\n",
      "[2500]\ttraining's rmse: 2.1951\tvalid_1's rmse: 2.3778\n",
      "[2550]\ttraining's rmse: 2.19272\tvalid_1's rmse: 2.3774\n",
      "[2600]\ttraining's rmse: 2.19001\tvalid_1's rmse: 2.37673\n",
      "[2650]\ttraining's rmse: 2.18788\tvalid_1's rmse: 2.37633\n",
      "[2700]\ttraining's rmse: 2.18586\tvalid_1's rmse: 2.3759\n",
      "[2750]\ttraining's rmse: 2.18342\tvalid_1's rmse: 2.37532\n",
      "[2800]\ttraining's rmse: 2.18088\tvalid_1's rmse: 2.3748\n",
      "[2850]\ttraining's rmse: 2.17798\tvalid_1's rmse: 2.37402\n",
      "[2900]\ttraining's rmse: 2.1752\tvalid_1's rmse: 2.37362\n",
      "[2950]\ttraining's rmse: 2.1721\tvalid_1's rmse: 2.37259\n",
      "[3000]\ttraining's rmse: 2.16956\tvalid_1's rmse: 2.37185\n",
      "[3050]\ttraining's rmse: 2.16715\tvalid_1's rmse: 2.37153\n",
      "[3100]\ttraining's rmse: 2.16452\tvalid_1's rmse: 2.37097\n",
      "[3150]\ttraining's rmse: 2.16212\tvalid_1's rmse: 2.3704\n",
      "[3200]\ttraining's rmse: 2.15996\tvalid_1's rmse: 2.36995\n",
      "[3250]\ttraining's rmse: 2.15764\tvalid_1's rmse: 2.36937\n",
      "[3300]\ttraining's rmse: 2.1552\tvalid_1's rmse: 2.36878\n",
      "[3350]\ttraining's rmse: 2.1531\tvalid_1's rmse: 2.36821\n",
      "[3400]\ttraining's rmse: 2.15083\tvalid_1's rmse: 2.36776\n",
      "[3450]\ttraining's rmse: 2.1488\tvalid_1's rmse: 2.36747\n",
      "[3500]\ttraining's rmse: 2.1467\tvalid_1's rmse: 2.36697\n",
      "[3550]\ttraining's rmse: 2.1446\tvalid_1's rmse: 2.36665\n",
      "[3600]\ttraining's rmse: 2.14244\tvalid_1's rmse: 2.36626\n",
      "[3650]\ttraining's rmse: 2.14061\tvalid_1's rmse: 2.36609\n",
      "[3700]\ttraining's rmse: 2.13868\tvalid_1's rmse: 2.36578\n",
      "[3750]\ttraining's rmse: 2.13696\tvalid_1's rmse: 2.3654\n",
      "[3800]\ttraining's rmse: 2.13486\tvalid_1's rmse: 2.36484\n",
      "[3850]\ttraining's rmse: 2.13333\tvalid_1's rmse: 2.36462\n",
      "[3900]\ttraining's rmse: 2.13139\tvalid_1's rmse: 2.36426\n",
      "[3950]\ttraining's rmse: 2.12978\tvalid_1's rmse: 2.36397\n",
      "[4000]\ttraining's rmse: 2.12741\tvalid_1's rmse: 2.36349\n",
      "[4050]\ttraining's rmse: 2.12535\tvalid_1's rmse: 2.36311\n",
      "[4100]\ttraining's rmse: 2.12316\tvalid_1's rmse: 2.3628\n",
      "[4150]\ttraining's rmse: 2.1212\tvalid_1's rmse: 2.36239\n",
      "[4200]\ttraining's rmse: 2.11938\tvalid_1's rmse: 2.36206\n",
      "[4250]\ttraining's rmse: 2.11753\tvalid_1's rmse: 2.36188\n",
      "[4300]\ttraining's rmse: 2.11555\tvalid_1's rmse: 2.36155\n",
      "[4350]\ttraining's rmse: 2.11341\tvalid_1's rmse: 2.36134\n",
      "[4400]\ttraining's rmse: 2.11176\tvalid_1's rmse: 2.36104\n",
      "[4450]\ttraining's rmse: 2.11001\tvalid_1's rmse: 2.36069\n",
      "[4500]\ttraining's rmse: 2.10828\tvalid_1's rmse: 2.36065\n",
      "[4550]\ttraining's rmse: 2.10613\tvalid_1's rmse: 2.36021\n",
      "[4600]\ttraining's rmse: 2.10415\tvalid_1's rmse: 2.35976\n",
      "[4650]\ttraining's rmse: 2.10237\tvalid_1's rmse: 2.35945\n",
      "[4700]\ttraining's rmse: 2.10086\tvalid_1's rmse: 2.35924\n",
      "[4750]\ttraining's rmse: 2.09913\tvalid_1's rmse: 2.35906\n",
      "[4800]\ttraining's rmse: 2.09722\tvalid_1's rmse: 2.35875\n",
      "[4850]\ttraining's rmse: 2.09555\tvalid_1's rmse: 2.35857\n",
      "[4900]\ttraining's rmse: 2.09385\tvalid_1's rmse: 2.3582\n",
      "[4950]\ttraining's rmse: 2.09248\tvalid_1's rmse: 2.35804\n",
      "[5000]\ttraining's rmse: 2.0907\tvalid_1's rmse: 2.35773\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.0907\tvalid_1's rmse: 2.35773\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.57525\tvalid_1's rmse: 2.58901\n",
      "[100]\ttraining's rmse: 2.50564\tvalid_1's rmse: 2.52286\n",
      "[150]\ttraining's rmse: 2.4823\tvalid_1's rmse: 2.50352\n",
      "[200]\ttraining's rmse: 2.46329\tvalid_1's rmse: 2.48914\n",
      "[250]\ttraining's rmse: 2.44319\tvalid_1's rmse: 2.47451\n",
      "[300]\ttraining's rmse: 2.42735\tvalid_1's rmse: 2.46421\n",
      "[350]\ttraining's rmse: 2.41198\tvalid_1's rmse: 2.45414\n",
      "[400]\ttraining's rmse: 2.39971\tvalid_1's rmse: 2.4473\n",
      "[450]\ttraining's rmse: 2.38731\tvalid_1's rmse: 2.44015\n",
      "[500]\ttraining's rmse: 2.37714\tvalid_1's rmse: 2.43478\n",
      "[550]\ttraining's rmse: 2.36877\tvalid_1's rmse: 2.43083\n",
      "[600]\ttraining's rmse: 2.3598\tvalid_1's rmse: 2.42671\n",
      "[650]\ttraining's rmse: 2.35216\tvalid_1's rmse: 2.42363\n",
      "[700]\ttraining's rmse: 2.34552\tvalid_1's rmse: 2.42107\n",
      "[750]\ttraining's rmse: 2.3391\tvalid_1's rmse: 2.41876\n",
      "[800]\ttraining's rmse: 2.33249\tvalid_1's rmse: 2.41641\n",
      "[850]\ttraining's rmse: 2.32617\tvalid_1's rmse: 2.41401\n",
      "[900]\ttraining's rmse: 2.31985\tvalid_1's rmse: 2.41173\n",
      "[950]\ttraining's rmse: 2.31427\tvalid_1's rmse: 2.40977\n",
      "[1000]\ttraining's rmse: 2.30832\tvalid_1's rmse: 2.40752\n",
      "[1050]\ttraining's rmse: 2.3031\tvalid_1's rmse: 2.40609\n",
      "[1100]\ttraining's rmse: 2.29831\tvalid_1's rmse: 2.40456\n",
      "[1150]\ttraining's rmse: 2.29293\tvalid_1's rmse: 2.40296\n",
      "[1200]\ttraining's rmse: 2.2881\tvalid_1's rmse: 2.4014\n",
      "[1250]\ttraining's rmse: 2.28361\tvalid_1's rmse: 2.40001\n",
      "[1300]\ttraining's rmse: 2.27926\tvalid_1's rmse: 2.39885\n",
      "[1350]\ttraining's rmse: 2.27469\tvalid_1's rmse: 2.39768\n",
      "[1400]\ttraining's rmse: 2.27061\tvalid_1's rmse: 2.39629\n",
      "[1450]\ttraining's rmse: 2.26631\tvalid_1's rmse: 2.39512\n",
      "[1500]\ttraining's rmse: 2.26174\tvalid_1's rmse: 2.39355\n",
      "[1550]\ttraining's rmse: 2.25825\tvalid_1's rmse: 2.39289\n",
      "[1600]\ttraining's rmse: 2.25412\tvalid_1's rmse: 2.39145\n",
      "[1650]\ttraining's rmse: 2.25051\tvalid_1's rmse: 2.3906\n",
      "[1700]\ttraining's rmse: 2.24624\tvalid_1's rmse: 2.38937\n",
      "[1750]\ttraining's rmse: 2.24232\tvalid_1's rmse: 2.38815\n",
      "[1800]\ttraining's rmse: 2.23873\tvalid_1's rmse: 2.38728\n",
      "[1850]\ttraining's rmse: 2.23542\tvalid_1's rmse: 2.3863\n",
      "[1900]\ttraining's rmse: 2.23197\tvalid_1's rmse: 2.38563\n",
      "[1950]\ttraining's rmse: 2.2287\tvalid_1's rmse: 2.38474\n",
      "[2000]\ttraining's rmse: 2.2255\tvalid_1's rmse: 2.38371\n",
      "[2050]\ttraining's rmse: 2.22205\tvalid_1's rmse: 2.38289\n",
      "[2100]\ttraining's rmse: 2.21929\tvalid_1's rmse: 2.38232\n",
      "[2150]\ttraining's rmse: 2.21592\tvalid_1's rmse: 2.38155\n",
      "[2200]\ttraining's rmse: 2.21302\tvalid_1's rmse: 2.38101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.21008\tvalid_1's rmse: 2.38039\n",
      "[2300]\ttraining's rmse: 2.20728\tvalid_1's rmse: 2.37968\n",
      "[2350]\ttraining's rmse: 2.20451\tvalid_1's rmse: 2.37909\n",
      "[2400]\ttraining's rmse: 2.2013\tvalid_1's rmse: 2.37828\n",
      "[2450]\ttraining's rmse: 2.19823\tvalid_1's rmse: 2.37752\n",
      "[2500]\ttraining's rmse: 2.1954\tvalid_1's rmse: 2.37685\n",
      "[2550]\ttraining's rmse: 2.19211\tvalid_1's rmse: 2.376\n",
      "[2600]\ttraining's rmse: 2.18942\tvalid_1's rmse: 2.37539\n",
      "[2650]\ttraining's rmse: 2.18701\tvalid_1's rmse: 2.37498\n",
      "[2700]\ttraining's rmse: 2.18384\tvalid_1's rmse: 2.37399\n",
      "[2750]\ttraining's rmse: 2.18115\tvalid_1's rmse: 2.37334\n",
      "[2800]\ttraining's rmse: 2.17869\tvalid_1's rmse: 2.37293\n",
      "[2850]\ttraining's rmse: 2.17673\tvalid_1's rmse: 2.37238\n",
      "[2900]\ttraining's rmse: 2.17417\tvalid_1's rmse: 2.3717\n",
      "[2950]\ttraining's rmse: 2.17142\tvalid_1's rmse: 2.37121\n",
      "[3000]\ttraining's rmse: 2.169\tvalid_1's rmse: 2.37075\n",
      "[3050]\ttraining's rmse: 2.1664\tvalid_1's rmse: 2.37016\n",
      "[3100]\ttraining's rmse: 2.1638\tvalid_1's rmse: 2.36976\n",
      "[3150]\ttraining's rmse: 2.16165\tvalid_1's rmse: 2.36942\n",
      "[3200]\ttraining's rmse: 2.15919\tvalid_1's rmse: 2.36886\n",
      "[3250]\ttraining's rmse: 2.15699\tvalid_1's rmse: 2.36846\n",
      "[3300]\ttraining's rmse: 2.15491\tvalid_1's rmse: 2.36796\n",
      "[3350]\ttraining's rmse: 2.15263\tvalid_1's rmse: 2.36737\n",
      "[3400]\ttraining's rmse: 2.15073\tvalid_1's rmse: 2.3668\n",
      "[3450]\ttraining's rmse: 2.14833\tvalid_1's rmse: 2.36627\n",
      "[3500]\ttraining's rmse: 2.14607\tvalid_1's rmse: 2.366\n",
      "[3550]\ttraining's rmse: 2.1438\tvalid_1's rmse: 2.36564\n",
      "[3600]\ttraining's rmse: 2.14127\tvalid_1's rmse: 2.36493\n",
      "[3650]\ttraining's rmse: 2.13925\tvalid_1's rmse: 2.36455\n",
      "[3700]\ttraining's rmse: 2.13706\tvalid_1's rmse: 2.36409\n",
      "[3750]\ttraining's rmse: 2.13526\tvalid_1's rmse: 2.36375\n",
      "[3800]\ttraining's rmse: 2.13326\tvalid_1's rmse: 2.36333\n",
      "[3850]\ttraining's rmse: 2.13112\tvalid_1's rmse: 2.36285\n",
      "[3900]\ttraining's rmse: 2.12931\tvalid_1's rmse: 2.36248\n",
      "[3950]\ttraining's rmse: 2.12706\tvalid_1's rmse: 2.36208\n",
      "[4000]\ttraining's rmse: 2.12491\tvalid_1's rmse: 2.36185\n",
      "[4050]\ttraining's rmse: 2.12269\tvalid_1's rmse: 2.36141\n",
      "[4100]\ttraining's rmse: 2.12086\tvalid_1's rmse: 2.36106\n",
      "[4150]\ttraining's rmse: 2.11904\tvalid_1's rmse: 2.36066\n",
      "[4200]\ttraining's rmse: 2.11732\tvalid_1's rmse: 2.36046\n",
      "[4250]\ttraining's rmse: 2.11523\tvalid_1's rmse: 2.36004\n",
      "[4300]\ttraining's rmse: 2.11316\tvalid_1's rmse: 2.3596\n",
      "[4350]\ttraining's rmse: 2.11085\tvalid_1's rmse: 2.35915\n",
      "[4400]\ttraining's rmse: 2.10863\tvalid_1's rmse: 2.35857\n",
      "[4450]\ttraining's rmse: 2.10685\tvalid_1's rmse: 2.35817\n",
      "[4500]\ttraining's rmse: 2.10497\tvalid_1's rmse: 2.35779\n",
      "[4550]\ttraining's rmse: 2.10365\tvalid_1's rmse: 2.35763\n",
      "[4600]\ttraining's rmse: 2.10215\tvalid_1's rmse: 2.35728\n",
      "[4650]\ttraining's rmse: 2.10037\tvalid_1's rmse: 2.35707\n",
      "[4700]\ttraining's rmse: 2.09847\tvalid_1's rmse: 2.35653\n",
      "[4750]\ttraining's rmse: 2.09673\tvalid_1's rmse: 2.35613\n",
      "[4800]\ttraining's rmse: 2.09487\tvalid_1's rmse: 2.35584\n",
      "[4850]\ttraining's rmse: 2.09316\tvalid_1's rmse: 2.35569\n",
      "[4900]\ttraining's rmse: 2.09187\tvalid_1's rmse: 2.35547\n",
      "[4950]\ttraining's rmse: 2.09038\tvalid_1's rmse: 2.35522\n",
      "[5000]\ttraining's rmse: 2.08847\tvalid_1's rmse: 2.35483\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08847\tvalid_1's rmse: 2.35483\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.59099\tvalid_1's rmse: 2.6033\n",
      "[100]\ttraining's rmse: 2.51233\tvalid_1's rmse: 2.53047\n",
      "[150]\ttraining's rmse: 2.49019\tvalid_1's rmse: 2.51176\n",
      "[200]\ttraining's rmse: 2.47123\tvalid_1's rmse: 2.49652\n",
      "[250]\ttraining's rmse: 2.44916\tvalid_1's rmse: 2.48019\n",
      "[300]\ttraining's rmse: 2.43272\tvalid_1's rmse: 2.46907\n",
      "[350]\ttraining's rmse: 2.41725\tvalid_1's rmse: 2.45965\n",
      "[400]\ttraining's rmse: 2.40306\tvalid_1's rmse: 2.4512\n",
      "[450]\ttraining's rmse: 2.39166\tvalid_1's rmse: 2.44541\n",
      "[500]\ttraining's rmse: 2.38132\tvalid_1's rmse: 2.4403\n",
      "[550]\ttraining's rmse: 2.37284\tvalid_1's rmse: 2.43654\n",
      "[600]\ttraining's rmse: 2.36425\tvalid_1's rmse: 2.43298\n",
      "[650]\ttraining's rmse: 2.35589\tvalid_1's rmse: 2.42951\n",
      "[700]\ttraining's rmse: 2.34838\tvalid_1's rmse: 2.42699\n",
      "[750]\ttraining's rmse: 2.34208\tvalid_1's rmse: 2.42511\n",
      "[800]\ttraining's rmse: 2.33503\tvalid_1's rmse: 2.42223\n",
      "[850]\ttraining's rmse: 2.32729\tvalid_1's rmse: 2.41892\n",
      "[900]\ttraining's rmse: 2.32154\tvalid_1's rmse: 2.41721\n",
      "[950]\ttraining's rmse: 2.31622\tvalid_1's rmse: 2.41554\n",
      "[1000]\ttraining's rmse: 2.31081\tvalid_1's rmse: 2.41384\n",
      "[1050]\ttraining's rmse: 2.3058\tvalid_1's rmse: 2.41231\n",
      "[1100]\ttraining's rmse: 2.29991\tvalid_1's rmse: 2.41017\n",
      "[1150]\ttraining's rmse: 2.29542\tvalid_1's rmse: 2.4088\n",
      "[1200]\ttraining's rmse: 2.29101\tvalid_1's rmse: 2.40755\n",
      "[1250]\ttraining's rmse: 2.28586\tvalid_1's rmse: 2.40608\n",
      "[1300]\ttraining's rmse: 2.28062\tvalid_1's rmse: 2.40419\n",
      "[1350]\ttraining's rmse: 2.27668\tvalid_1's rmse: 2.40287\n",
      "[1400]\ttraining's rmse: 2.27201\tvalid_1's rmse: 2.40116\n",
      "[1450]\ttraining's rmse: 2.26811\tvalid_1's rmse: 2.40008\n",
      "[1500]\ttraining's rmse: 2.26427\tvalid_1's rmse: 2.39897\n",
      "[1550]\ttraining's rmse: 2.26057\tvalid_1's rmse: 2.398\n",
      "[1600]\ttraining's rmse: 2.25647\tvalid_1's rmse: 2.39702\n",
      "[1650]\ttraining's rmse: 2.25312\tvalid_1's rmse: 2.39617\n",
      "[1700]\ttraining's rmse: 2.24907\tvalid_1's rmse: 2.39498\n",
      "[1750]\ttraining's rmse: 2.24503\tvalid_1's rmse: 2.39392\n",
      "[1800]\ttraining's rmse: 2.24132\tvalid_1's rmse: 2.39302\n",
      "[1850]\ttraining's rmse: 2.23791\tvalid_1's rmse: 2.39227\n",
      "[1900]\ttraining's rmse: 2.23491\tvalid_1's rmse: 2.39171\n",
      "[1950]\ttraining's rmse: 2.23146\tvalid_1's rmse: 2.39066\n",
      "[2000]\ttraining's rmse: 2.22805\tvalid_1's rmse: 2.38976\n",
      "[2050]\ttraining's rmse: 2.22472\tvalid_1's rmse: 2.38906\n",
      "[2100]\ttraining's rmse: 2.22029\tvalid_1's rmse: 2.38766\n",
      "[2150]\ttraining's rmse: 2.2169\tvalid_1's rmse: 2.38703\n",
      "[2200]\ttraining's rmse: 2.21389\tvalid_1's rmse: 2.38624\n",
      "[2250]\ttraining's rmse: 2.21001\tvalid_1's rmse: 2.38494\n",
      "[2300]\ttraining's rmse: 2.20718\tvalid_1's rmse: 2.38421\n",
      "[2350]\ttraining's rmse: 2.20457\tvalid_1's rmse: 2.38342\n",
      "[2400]\ttraining's rmse: 2.20166\tvalid_1's rmse: 2.38276\n",
      "[2450]\ttraining's rmse: 2.19848\tvalid_1's rmse: 2.38225\n",
      "[2500]\ttraining's rmse: 2.19571\tvalid_1's rmse: 2.38187\n",
      "[2550]\ttraining's rmse: 2.19277\tvalid_1's rmse: 2.38114\n",
      "[2600]\ttraining's rmse: 2.19069\tvalid_1's rmse: 2.38078\n",
      "[2650]\ttraining's rmse: 2.18759\tvalid_1's rmse: 2.38022\n",
      "[2700]\ttraining's rmse: 2.18487\tvalid_1's rmse: 2.37953\n",
      "[2750]\ttraining's rmse: 2.18194\tvalid_1's rmse: 2.37877\n",
      "[2800]\ttraining's rmse: 2.17911\tvalid_1's rmse: 2.37792\n",
      "[2850]\ttraining's rmse: 2.17651\tvalid_1's rmse: 2.37745\n",
      "[2900]\ttraining's rmse: 2.17382\tvalid_1's rmse: 2.37668\n",
      "[2950]\ttraining's rmse: 2.17146\tvalid_1's rmse: 2.37633\n",
      "[3000]\ttraining's rmse: 2.16895\tvalid_1's rmse: 2.37587\n",
      "[3050]\ttraining's rmse: 2.16667\tvalid_1's rmse: 2.37534\n",
      "[3100]\ttraining's rmse: 2.16447\tvalid_1's rmse: 2.37485\n",
      "[3150]\ttraining's rmse: 2.162\tvalid_1's rmse: 2.37454\n",
      "[3200]\ttraining's rmse: 2.15969\tvalid_1's rmse: 2.3741\n",
      "[3250]\ttraining's rmse: 2.15711\tvalid_1's rmse: 2.37344\n",
      "[3300]\ttraining's rmse: 2.15438\tvalid_1's rmse: 2.37306\n",
      "[3350]\ttraining's rmse: 2.1517\tvalid_1's rmse: 2.37243\n",
      "[3400]\ttraining's rmse: 2.1494\tvalid_1's rmse: 2.37193\n",
      "[3450]\ttraining's rmse: 2.14738\tvalid_1's rmse: 2.37153\n",
      "[3500]\ttraining's rmse: 2.14535\tvalid_1's rmse: 2.37108\n",
      "[3550]\ttraining's rmse: 2.14349\tvalid_1's rmse: 2.37081\n",
      "[3600]\ttraining's rmse: 2.14134\tvalid_1's rmse: 2.37033\n",
      "[3650]\ttraining's rmse: 2.13871\tvalid_1's rmse: 2.36987\n",
      "[3700]\ttraining's rmse: 2.1369\tvalid_1's rmse: 2.36947\n",
      "[3750]\ttraining's rmse: 2.13468\tvalid_1's rmse: 2.36899\n",
      "[3800]\ttraining's rmse: 2.13309\tvalid_1's rmse: 2.36864\n",
      "[3850]\ttraining's rmse: 2.13121\tvalid_1's rmse: 2.36827\n",
      "[3900]\ttraining's rmse: 2.129\tvalid_1's rmse: 2.36779\n",
      "[3950]\ttraining's rmse: 2.12656\tvalid_1's rmse: 2.36713\n",
      "[4000]\ttraining's rmse: 2.12448\tvalid_1's rmse: 2.3669\n",
      "[4050]\ttraining's rmse: 2.12252\tvalid_1's rmse: 2.36652\n",
      "[4100]\ttraining's rmse: 2.12114\tvalid_1's rmse: 2.3663\n",
      "[4150]\ttraining's rmse: 2.11968\tvalid_1's rmse: 2.36604\n",
      "[4200]\ttraining's rmse: 2.11758\tvalid_1's rmse: 2.36569\n",
      "[4250]\ttraining's rmse: 2.1152\tvalid_1's rmse: 2.36514\n",
      "[4300]\ttraining's rmse: 2.11333\tvalid_1's rmse: 2.3648\n",
      "[4350]\ttraining's rmse: 2.1119\tvalid_1's rmse: 2.36455\n",
      "[4400]\ttraining's rmse: 2.10996\tvalid_1's rmse: 2.3643\n",
      "[4450]\ttraining's rmse: 2.10827\tvalid_1's rmse: 2.36402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500]\ttraining's rmse: 2.10654\tvalid_1's rmse: 2.36363\n",
      "[4550]\ttraining's rmse: 2.10448\tvalid_1's rmse: 2.36338\n",
      "[4600]\ttraining's rmse: 2.10279\tvalid_1's rmse: 2.36298\n",
      "[4650]\ttraining's rmse: 2.10072\tvalid_1's rmse: 2.36259\n",
      "[4700]\ttraining's rmse: 2.0988\tvalid_1's rmse: 2.36233\n",
      "[4750]\ttraining's rmse: 2.09733\tvalid_1's rmse: 2.36216\n",
      "[4800]\ttraining's rmse: 2.09572\tvalid_1's rmse: 2.36195\n",
      "[4850]\ttraining's rmse: 2.0937\tvalid_1's rmse: 2.36128\n",
      "[4900]\ttraining's rmse: 2.0917\tvalid_1's rmse: 2.36094\n",
      "[4950]\ttraining's rmse: 2.0898\tvalid_1's rmse: 2.36062\n",
      "[5000]\ttraining's rmse: 2.08823\tvalid_1's rmse: 2.36025\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08823\tvalid_1's rmse: 2.36025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.58164\tvalid_1's rmse: 2.60206\n",
      "[100]\ttraining's rmse: 2.50851\tvalid_1's rmse: 2.53485\n",
      "[150]\ttraining's rmse: 2.48533\tvalid_1's rmse: 2.51526\n",
      "[200]\ttraining's rmse: 2.46268\tvalid_1's rmse: 2.49723\n",
      "[250]\ttraining's rmse: 2.443\tvalid_1's rmse: 2.48259\n",
      "[300]\ttraining's rmse: 2.42439\tvalid_1's rmse: 2.47005\n",
      "[350]\ttraining's rmse: 2.40991\tvalid_1's rmse: 2.46092\n",
      "[400]\ttraining's rmse: 2.3986\tvalid_1's rmse: 2.45459\n",
      "[450]\ttraining's rmse: 2.387\tvalid_1's rmse: 2.44863\n",
      "[500]\ttraining's rmse: 2.37697\tvalid_1's rmse: 2.44392\n",
      "[550]\ttraining's rmse: 2.36805\tvalid_1's rmse: 2.4399\n",
      "[600]\ttraining's rmse: 2.36017\tvalid_1's rmse: 2.43642\n",
      "[650]\ttraining's rmse: 2.3531\tvalid_1's rmse: 2.43336\n",
      "[700]\ttraining's rmse: 2.34578\tvalid_1's rmse: 2.43046\n",
      "[750]\ttraining's rmse: 2.33847\tvalid_1's rmse: 2.42732\n",
      "[800]\ttraining's rmse: 2.33255\tvalid_1's rmse: 2.42531\n",
      "[850]\ttraining's rmse: 2.32729\tvalid_1's rmse: 2.42319\n",
      "[900]\ttraining's rmse: 2.32057\tvalid_1's rmse: 2.42055\n",
      "[950]\ttraining's rmse: 2.31551\tvalid_1's rmse: 2.41898\n",
      "[1000]\ttraining's rmse: 2.31077\tvalid_1's rmse: 2.41748\n",
      "[1050]\ttraining's rmse: 2.30644\tvalid_1's rmse: 2.4166\n",
      "[1100]\ttraining's rmse: 2.30131\tvalid_1's rmse: 2.4146\n",
      "[1150]\ttraining's rmse: 2.29582\tvalid_1's rmse: 2.413\n",
      "[1200]\ttraining's rmse: 2.29042\tvalid_1's rmse: 2.41117\n",
      "[1250]\ttraining's rmse: 2.28564\tvalid_1's rmse: 2.40966\n",
      "[1300]\ttraining's rmse: 2.28118\tvalid_1's rmse: 2.40806\n",
      "[1350]\ttraining's rmse: 2.27674\tvalid_1's rmse: 2.40644\n",
      "[1400]\ttraining's rmse: 2.27228\tvalid_1's rmse: 2.40518\n",
      "[1450]\ttraining's rmse: 2.26783\tvalid_1's rmse: 2.40403\n",
      "[1500]\ttraining's rmse: 2.2635\tvalid_1's rmse: 2.40251\n",
      "[1550]\ttraining's rmse: 2.25906\tvalid_1's rmse: 2.40132\n",
      "[1600]\ttraining's rmse: 2.25537\tvalid_1's rmse: 2.40015\n",
      "[1650]\ttraining's rmse: 2.25102\tvalid_1's rmse: 2.39872\n",
      "[1700]\ttraining's rmse: 2.24773\tvalid_1's rmse: 2.39796\n",
      "[1750]\ttraining's rmse: 2.24433\tvalid_1's rmse: 2.39723\n",
      "[1800]\ttraining's rmse: 2.24135\tvalid_1's rmse: 2.3966\n",
      "[1850]\ttraining's rmse: 2.23783\tvalid_1's rmse: 2.39556\n",
      "[1900]\ttraining's rmse: 2.23409\tvalid_1's rmse: 2.39453\n",
      "[1950]\ttraining's rmse: 2.23081\tvalid_1's rmse: 2.39373\n",
      "[2000]\ttraining's rmse: 2.22746\tvalid_1's rmse: 2.39282\n",
      "[2050]\ttraining's rmse: 2.2239\tvalid_1's rmse: 2.39204\n",
      "[2100]\ttraining's rmse: 2.22063\tvalid_1's rmse: 2.39152\n",
      "[2150]\ttraining's rmse: 2.21756\tvalid_1's rmse: 2.39087\n",
      "[2200]\ttraining's rmse: 2.21441\tvalid_1's rmse: 2.39011\n",
      "[2250]\ttraining's rmse: 2.21076\tvalid_1's rmse: 2.38919\n",
      "[2300]\ttraining's rmse: 2.20789\tvalid_1's rmse: 2.3885\n",
      "[2350]\ttraining's rmse: 2.20478\tvalid_1's rmse: 2.3876\n",
      "[2400]\ttraining's rmse: 2.2019\tvalid_1's rmse: 2.38675\n",
      "[2450]\ttraining's rmse: 2.1996\tvalid_1's rmse: 2.38632\n",
      "[2500]\ttraining's rmse: 2.19734\tvalid_1's rmse: 2.38581\n",
      "[2550]\ttraining's rmse: 2.19521\tvalid_1's rmse: 2.38545\n",
      "[2600]\ttraining's rmse: 2.19223\tvalid_1's rmse: 2.38493\n",
      "[2650]\ttraining's rmse: 2.18957\tvalid_1's rmse: 2.38444\n",
      "[2700]\ttraining's rmse: 2.18632\tvalid_1's rmse: 2.38376\n",
      "[2750]\ttraining's rmse: 2.18376\tvalid_1's rmse: 2.38346\n",
      "[2800]\ttraining's rmse: 2.18146\tvalid_1's rmse: 2.38301\n",
      "[2850]\ttraining's rmse: 2.17881\tvalid_1's rmse: 2.3823\n",
      "[2900]\ttraining's rmse: 2.17628\tvalid_1's rmse: 2.38157\n",
      "[2950]\ttraining's rmse: 2.17415\tvalid_1's rmse: 2.38113\n",
      "[3000]\ttraining's rmse: 2.1718\tvalid_1's rmse: 2.38056\n",
      "[3050]\ttraining's rmse: 2.16926\tvalid_1's rmse: 2.37998\n",
      "[3100]\ttraining's rmse: 2.16691\tvalid_1's rmse: 2.37913\n",
      "[3150]\ttraining's rmse: 2.16481\tvalid_1's rmse: 2.3787\n",
      "[3200]\ttraining's rmse: 2.16277\tvalid_1's rmse: 2.37823\n",
      "[3250]\ttraining's rmse: 2.16017\tvalid_1's rmse: 2.37774\n",
      "[3300]\ttraining's rmse: 2.15795\tvalid_1's rmse: 2.37753\n",
      "[3350]\ttraining's rmse: 2.15572\tvalid_1's rmse: 2.37705\n",
      "[3400]\ttraining's rmse: 2.15378\tvalid_1's rmse: 2.37676\n",
      "[3450]\ttraining's rmse: 2.15154\tvalid_1's rmse: 2.37618\n",
      "[3500]\ttraining's rmse: 2.14932\tvalid_1's rmse: 2.37576\n",
      "[3550]\ttraining's rmse: 2.14731\tvalid_1's rmse: 2.37547\n",
      "[3600]\ttraining's rmse: 2.14523\tvalid_1's rmse: 2.37501\n",
      "[3650]\ttraining's rmse: 2.14296\tvalid_1's rmse: 2.37461\n",
      "[3700]\ttraining's rmse: 2.14106\tvalid_1's rmse: 2.37425\n",
      "[3750]\ttraining's rmse: 2.13839\tvalid_1's rmse: 2.3738\n",
      "[3800]\ttraining's rmse: 2.13623\tvalid_1's rmse: 2.37345\n",
      "[3850]\ttraining's rmse: 2.13434\tvalid_1's rmse: 2.37301\n",
      "[3900]\ttraining's rmse: 2.13223\tvalid_1's rmse: 2.37255\n",
      "[3950]\ttraining's rmse: 2.13022\tvalid_1's rmse: 2.37229\n",
      "[4000]\ttraining's rmse: 2.12815\tvalid_1's rmse: 2.37186\n",
      "[4050]\ttraining's rmse: 2.12629\tvalid_1's rmse: 2.37179\n",
      "[4100]\ttraining's rmse: 2.12459\tvalid_1's rmse: 2.37146\n",
      "[4150]\ttraining's rmse: 2.12283\tvalid_1's rmse: 2.37123\n",
      "[4200]\ttraining's rmse: 2.121\tvalid_1's rmse: 2.37091\n",
      "[4250]\ttraining's rmse: 2.11917\tvalid_1's rmse: 2.37074\n",
      "[4300]\ttraining's rmse: 2.11719\tvalid_1's rmse: 2.37012\n",
      "[4350]\ttraining's rmse: 2.11523\tvalid_1's rmse: 2.36983\n",
      "[4400]\ttraining's rmse: 2.11352\tvalid_1's rmse: 2.36965\n",
      "[4450]\ttraining's rmse: 2.11135\tvalid_1's rmse: 2.36922\n",
      "[4500]\ttraining's rmse: 2.10944\tvalid_1's rmse: 2.36876\n",
      "[4550]\ttraining's rmse: 2.10741\tvalid_1's rmse: 2.36861\n",
      "[4600]\ttraining's rmse: 2.10534\tvalid_1's rmse: 2.3683\n",
      "[4650]\ttraining's rmse: 2.10374\tvalid_1's rmse: 2.36804\n",
      "[4700]\ttraining's rmse: 2.1019\tvalid_1's rmse: 2.36757\n",
      "[4750]\ttraining's rmse: 2.10019\tvalid_1's rmse: 2.36729\n",
      "[4800]\ttraining's rmse: 2.09854\tvalid_1's rmse: 2.36704\n",
      "[4850]\ttraining's rmse: 2.09691\tvalid_1's rmse: 2.36671\n",
      "[4900]\ttraining's rmse: 2.09538\tvalid_1's rmse: 2.36636\n",
      "[4950]\ttraining's rmse: 2.09352\tvalid_1's rmse: 2.36603\n",
      "[5000]\ttraining's rmse: 2.09183\tvalid_1's rmse: 2.36566\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09183\tvalid_1's rmse: 2.36566\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.57741\tvalid_1's rmse: 2.59505\n",
      "[100]\ttraining's rmse: 2.50095\tvalid_1's rmse: 2.52749\n",
      "[150]\ttraining's rmse: 2.48007\tvalid_1's rmse: 2.51007\n",
      "[200]\ttraining's rmse: 2.46053\tvalid_1's rmse: 2.49445\n",
      "[250]\ttraining's rmse: 2.44197\tvalid_1's rmse: 2.48016\n",
      "[300]\ttraining's rmse: 2.42701\tvalid_1's rmse: 2.46988\n",
      "[350]\ttraining's rmse: 2.41233\tvalid_1's rmse: 2.45968\n",
      "[400]\ttraining's rmse: 2.39849\tvalid_1's rmse: 2.4511\n",
      "[450]\ttraining's rmse: 2.3866\tvalid_1's rmse: 2.4442\n",
      "[500]\ttraining's rmse: 2.37737\tvalid_1's rmse: 2.43911\n",
      "[550]\ttraining's rmse: 2.36917\tvalid_1's rmse: 2.43474\n",
      "[600]\ttraining's rmse: 2.36006\tvalid_1's rmse: 2.43067\n",
      "[650]\ttraining's rmse: 2.35269\tvalid_1's rmse: 2.42717\n",
      "[700]\ttraining's rmse: 2.34517\tvalid_1's rmse: 2.42321\n",
      "[750]\ttraining's rmse: 2.33853\tvalid_1's rmse: 2.42057\n",
      "[800]\ttraining's rmse: 2.33202\tvalid_1's rmse: 2.41787\n",
      "[850]\ttraining's rmse: 2.32613\tvalid_1's rmse: 2.41571\n",
      "[900]\ttraining's rmse: 2.32042\tvalid_1's rmse: 2.41363\n",
      "[950]\ttraining's rmse: 2.3141\tvalid_1's rmse: 2.41095\n",
      "[1000]\ttraining's rmse: 2.3082\tvalid_1's rmse: 2.40869\n",
      "[1050]\ttraining's rmse: 2.30308\tvalid_1's rmse: 2.4069\n",
      "[1100]\ttraining's rmse: 2.29829\tvalid_1's rmse: 2.40535\n",
      "[1150]\ttraining's rmse: 2.29238\tvalid_1's rmse: 2.40336\n",
      "[1200]\ttraining's rmse: 2.28713\tvalid_1's rmse: 2.40169\n",
      "[1250]\ttraining's rmse: 2.28184\tvalid_1's rmse: 2.39952\n",
      "[1300]\ttraining's rmse: 2.27693\tvalid_1's rmse: 2.39779\n",
      "[1350]\ttraining's rmse: 2.27288\tvalid_1's rmse: 2.39658\n",
      "[1400]\ttraining's rmse: 2.26919\tvalid_1's rmse: 2.39553\n",
      "[1450]\ttraining's rmse: 2.26488\tvalid_1's rmse: 2.39433\n",
      "[1500]\ttraining's rmse: 2.2604\tvalid_1's rmse: 2.39308\n",
      "[1550]\ttraining's rmse: 2.25569\tvalid_1's rmse: 2.39159\n",
      "[1600]\ttraining's rmse: 2.25199\tvalid_1's rmse: 2.39066\n",
      "[1650]\ttraining's rmse: 2.24791\tvalid_1's rmse: 2.38927\n",
      "[1700]\ttraining's rmse: 2.24411\tvalid_1's rmse: 2.38787\n",
      "[1750]\ttraining's rmse: 2.24034\tvalid_1's rmse: 2.38672\n",
      "[1800]\ttraining's rmse: 2.23724\tvalid_1's rmse: 2.38577\n",
      "[1850]\ttraining's rmse: 2.23352\tvalid_1's rmse: 2.38471\n",
      "[1900]\ttraining's rmse: 2.23007\tvalid_1's rmse: 2.38398\n",
      "[1950]\ttraining's rmse: 2.22674\tvalid_1's rmse: 2.38301\n",
      "[2000]\ttraining's rmse: 2.22337\tvalid_1's rmse: 2.38208\n",
      "[2050]\ttraining's rmse: 2.22026\tvalid_1's rmse: 2.38148\n",
      "[2100]\ttraining's rmse: 2.2173\tvalid_1's rmse: 2.3807\n",
      "[2150]\ttraining's rmse: 2.21393\tvalid_1's rmse: 2.37982\n",
      "[2200]\ttraining's rmse: 2.21103\tvalid_1's rmse: 2.37909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.20775\tvalid_1's rmse: 2.37817\n",
      "[2300]\ttraining's rmse: 2.20485\tvalid_1's rmse: 2.37745\n",
      "[2350]\ttraining's rmse: 2.20215\tvalid_1's rmse: 2.37705\n",
      "[2400]\ttraining's rmse: 2.1989\tvalid_1's rmse: 2.37599\n",
      "[2450]\ttraining's rmse: 2.19617\tvalid_1's rmse: 2.37526\n",
      "[2500]\ttraining's rmse: 2.19311\tvalid_1's rmse: 2.37461\n",
      "[2550]\ttraining's rmse: 2.18988\tvalid_1's rmse: 2.37376\n",
      "[2600]\ttraining's rmse: 2.18734\tvalid_1's rmse: 2.37307\n",
      "[2650]\ttraining's rmse: 2.18457\tvalid_1's rmse: 2.37241\n",
      "[2700]\ttraining's rmse: 2.18245\tvalid_1's rmse: 2.37192\n",
      "[2750]\ttraining's rmse: 2.17996\tvalid_1's rmse: 2.37141\n",
      "[2800]\ttraining's rmse: 2.17775\tvalid_1's rmse: 2.37093\n",
      "[2850]\ttraining's rmse: 2.17547\tvalid_1's rmse: 2.37067\n",
      "[2900]\ttraining's rmse: 2.17261\tvalid_1's rmse: 2.36995\n",
      "[2950]\ttraining's rmse: 2.16975\tvalid_1's rmse: 2.36926\n",
      "[3000]\ttraining's rmse: 2.16702\tvalid_1's rmse: 2.36854\n",
      "[3050]\ttraining's rmse: 2.1642\tvalid_1's rmse: 2.3679\n",
      "[3100]\ttraining's rmse: 2.16164\tvalid_1's rmse: 2.36752\n",
      "[3150]\ttraining's rmse: 2.15901\tvalid_1's rmse: 2.36681\n",
      "[3200]\ttraining's rmse: 2.15635\tvalid_1's rmse: 2.36622\n",
      "[3250]\ttraining's rmse: 2.15414\tvalid_1's rmse: 2.36563\n",
      "[3300]\ttraining's rmse: 2.15189\tvalid_1's rmse: 2.36499\n",
      "[3350]\ttraining's rmse: 2.14963\tvalid_1's rmse: 2.36445\n",
      "[3400]\ttraining's rmse: 2.14775\tvalid_1's rmse: 2.36406\n",
      "[3450]\ttraining's rmse: 2.14538\tvalid_1's rmse: 2.36369\n",
      "[3500]\ttraining's rmse: 2.14335\tvalid_1's rmse: 2.36316\n",
      "[3550]\ttraining's rmse: 2.14128\tvalid_1's rmse: 2.36287\n",
      "[3600]\ttraining's rmse: 2.13876\tvalid_1's rmse: 2.36223\n",
      "[3650]\ttraining's rmse: 2.1366\tvalid_1's rmse: 2.3618\n",
      "[3700]\ttraining's rmse: 2.13437\tvalid_1's rmse: 2.36121\n",
      "[3750]\ttraining's rmse: 2.13248\tvalid_1's rmse: 2.36088\n",
      "[3800]\ttraining's rmse: 2.13017\tvalid_1's rmse: 2.3604\n",
      "[3850]\ttraining's rmse: 2.12784\tvalid_1's rmse: 2.36004\n",
      "[3900]\ttraining's rmse: 2.12569\tvalid_1's rmse: 2.35971\n",
      "[3950]\ttraining's rmse: 2.12398\tvalid_1's rmse: 2.35916\n",
      "[4000]\ttraining's rmse: 2.12205\tvalid_1's rmse: 2.35888\n",
      "[4050]\ttraining's rmse: 2.12047\tvalid_1's rmse: 2.35859\n",
      "[4100]\ttraining's rmse: 2.11884\tvalid_1's rmse: 2.35831\n",
      "[4150]\ttraining's rmse: 2.11687\tvalid_1's rmse: 2.35812\n",
      "[4200]\ttraining's rmse: 2.11499\tvalid_1's rmse: 2.35786\n",
      "[4250]\ttraining's rmse: 2.11338\tvalid_1's rmse: 2.35755\n",
      "[4300]\ttraining's rmse: 2.11144\tvalid_1's rmse: 2.35719\n",
      "[4350]\ttraining's rmse: 2.11016\tvalid_1's rmse: 2.35729\n",
      "Early stopping, best iteration is:\n",
      "[4322]\ttraining's rmse: 2.11083\tvalid_1's rmse: 2.35714\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.58448\tvalid_1's rmse: 2.60119\n",
      "[100]\ttraining's rmse: 2.50801\tvalid_1's rmse: 2.52166\n",
      "[150]\ttraining's rmse: 2.4848\tvalid_1's rmse: 2.50157\n",
      "[200]\ttraining's rmse: 2.46427\tvalid_1's rmse: 2.48498\n",
      "[250]\ttraining's rmse: 2.44625\tvalid_1's rmse: 2.47131\n",
      "[300]\ttraining's rmse: 2.43036\tvalid_1's rmse: 2.4606\n",
      "[350]\ttraining's rmse: 2.41548\tvalid_1's rmse: 2.45117\n",
      "[400]\ttraining's rmse: 2.40139\tvalid_1's rmse: 2.44304\n",
      "[450]\ttraining's rmse: 2.39031\tvalid_1's rmse: 2.4369\n",
      "[500]\ttraining's rmse: 2.37852\tvalid_1's rmse: 2.43054\n",
      "[550]\ttraining's rmse: 2.36904\tvalid_1's rmse: 2.42567\n",
      "[600]\ttraining's rmse: 2.36113\tvalid_1's rmse: 2.4222\n",
      "[650]\ttraining's rmse: 2.3534\tvalid_1's rmse: 2.41886\n",
      "[700]\ttraining's rmse: 2.34643\tvalid_1's rmse: 2.41631\n",
      "[750]\ttraining's rmse: 2.3384\tvalid_1's rmse: 2.41297\n",
      "[800]\ttraining's rmse: 2.33164\tvalid_1's rmse: 2.41042\n",
      "[850]\ttraining's rmse: 2.32527\tvalid_1's rmse: 2.40818\n",
      "[900]\ttraining's rmse: 2.31912\tvalid_1's rmse: 2.40582\n",
      "[950]\ttraining's rmse: 2.31411\tvalid_1's rmse: 2.40417\n",
      "[1000]\ttraining's rmse: 2.30894\tvalid_1's rmse: 2.40245\n",
      "[1050]\ttraining's rmse: 2.30393\tvalid_1's rmse: 2.40088\n",
      "[1100]\ttraining's rmse: 2.29919\tvalid_1's rmse: 2.39932\n",
      "[1150]\ttraining's rmse: 2.29455\tvalid_1's rmse: 2.39774\n",
      "[1200]\ttraining's rmse: 2.28951\tvalid_1's rmse: 2.3962\n",
      "[1250]\ttraining's rmse: 2.28395\tvalid_1's rmse: 2.3948\n",
      "[1300]\ttraining's rmse: 2.27959\tvalid_1's rmse: 2.39339\n",
      "[1350]\ttraining's rmse: 2.27537\tvalid_1's rmse: 2.39207\n",
      "[1400]\ttraining's rmse: 2.27197\tvalid_1's rmse: 2.391\n",
      "[1450]\ttraining's rmse: 2.26719\tvalid_1's rmse: 2.38957\n",
      "[1500]\ttraining's rmse: 2.26337\tvalid_1's rmse: 2.38844\n",
      "[1550]\ttraining's rmse: 2.25916\tvalid_1's rmse: 2.38706\n",
      "[1600]\ttraining's rmse: 2.25525\tvalid_1's rmse: 2.38571\n",
      "[1650]\ttraining's rmse: 2.25215\tvalid_1's rmse: 2.38495\n",
      "[1700]\ttraining's rmse: 2.24884\tvalid_1's rmse: 2.38429\n",
      "[1750]\ttraining's rmse: 2.24449\tvalid_1's rmse: 2.38326\n",
      "[1800]\ttraining's rmse: 2.24089\tvalid_1's rmse: 2.38223\n",
      "[1850]\ttraining's rmse: 2.23755\tvalid_1's rmse: 2.38126\n",
      "[1900]\ttraining's rmse: 2.23388\tvalid_1's rmse: 2.38033\n",
      "[1950]\ttraining's rmse: 2.2305\tvalid_1's rmse: 2.37961\n",
      "[2000]\ttraining's rmse: 2.22667\tvalid_1's rmse: 2.37857\n",
      "[2050]\ttraining's rmse: 2.2232\tvalid_1's rmse: 2.37735\n",
      "[2100]\ttraining's rmse: 2.21954\tvalid_1's rmse: 2.37652\n",
      "[2150]\ttraining's rmse: 2.21615\tvalid_1's rmse: 2.37573\n",
      "[2200]\ttraining's rmse: 2.21313\tvalid_1's rmse: 2.37497\n",
      "[2250]\ttraining's rmse: 2.21104\tvalid_1's rmse: 2.37456\n",
      "[2300]\ttraining's rmse: 2.20783\tvalid_1's rmse: 2.37368\n",
      "[2350]\ttraining's rmse: 2.20483\tvalid_1's rmse: 2.37306\n",
      "[2400]\ttraining's rmse: 2.20229\tvalid_1's rmse: 2.37253\n",
      "[2450]\ttraining's rmse: 2.19921\tvalid_1's rmse: 2.37196\n",
      "[2500]\ttraining's rmse: 2.19617\tvalid_1's rmse: 2.37117\n",
      "[2550]\ttraining's rmse: 2.19345\tvalid_1's rmse: 2.37046\n",
      "[2600]\ttraining's rmse: 2.19074\tvalid_1's rmse: 2.36989\n",
      "[2650]\ttraining's rmse: 2.18852\tvalid_1's rmse: 2.36933\n",
      "[2700]\ttraining's rmse: 2.1859\tvalid_1's rmse: 2.36859\n",
      "[2750]\ttraining's rmse: 2.18318\tvalid_1's rmse: 2.3679\n",
      "[2800]\ttraining's rmse: 2.18041\tvalid_1's rmse: 2.3674\n",
      "[2850]\ttraining's rmse: 2.17727\tvalid_1's rmse: 2.3667\n",
      "[2900]\ttraining's rmse: 2.17471\tvalid_1's rmse: 2.36615\n",
      "[2950]\ttraining's rmse: 2.17204\tvalid_1's rmse: 2.36564\n",
      "[3000]\ttraining's rmse: 2.1695\tvalid_1's rmse: 2.36513\n",
      "[3050]\ttraining's rmse: 2.16694\tvalid_1's rmse: 2.36469\n",
      "[3100]\ttraining's rmse: 2.16505\tvalid_1's rmse: 2.36426\n",
      "[3150]\ttraining's rmse: 2.16297\tvalid_1's rmse: 2.3638\n",
      "[3200]\ttraining's rmse: 2.16057\tvalid_1's rmse: 2.36312\n",
      "[3250]\ttraining's rmse: 2.1584\tvalid_1's rmse: 2.36262\n",
      "[3300]\ttraining's rmse: 2.15622\tvalid_1's rmse: 2.36233\n",
      "[3350]\ttraining's rmse: 2.15388\tvalid_1's rmse: 2.36195\n",
      "[3400]\ttraining's rmse: 2.15176\tvalid_1's rmse: 2.36154\n",
      "[3450]\ttraining's rmse: 2.14957\tvalid_1's rmse: 2.36105\n",
      "[3500]\ttraining's rmse: 2.1476\tvalid_1's rmse: 2.36072\n",
      "[3550]\ttraining's rmse: 2.14498\tvalid_1's rmse: 2.36022\n",
      "[3600]\ttraining's rmse: 2.14289\tvalid_1's rmse: 2.35973\n",
      "[3650]\ttraining's rmse: 2.14077\tvalid_1's rmse: 2.35937\n",
      "[3700]\ttraining's rmse: 2.13868\tvalid_1's rmse: 2.35894\n",
      "[3750]\ttraining's rmse: 2.13684\tvalid_1's rmse: 2.35873\n",
      "[3800]\ttraining's rmse: 2.13485\tvalid_1's rmse: 2.35827\n",
      "[3850]\ttraining's rmse: 2.13269\tvalid_1's rmse: 2.35776\n",
      "[3900]\ttraining's rmse: 2.13029\tvalid_1's rmse: 2.35732\n",
      "[3950]\ttraining's rmse: 2.12872\tvalid_1's rmse: 2.35697\n",
      "[4000]\ttraining's rmse: 2.12719\tvalid_1's rmse: 2.35665\n",
      "[4050]\ttraining's rmse: 2.12505\tvalid_1's rmse: 2.35633\n",
      "[4100]\ttraining's rmse: 2.12335\tvalid_1's rmse: 2.35599\n",
      "[4150]\ttraining's rmse: 2.12147\tvalid_1's rmse: 2.35575\n",
      "[4200]\ttraining's rmse: 2.11972\tvalid_1's rmse: 2.35539\n",
      "[4250]\ttraining's rmse: 2.11749\tvalid_1's rmse: 2.35481\n",
      "[4300]\ttraining's rmse: 2.11541\tvalid_1's rmse: 2.35451\n",
      "[4350]\ttraining's rmse: 2.11376\tvalid_1's rmse: 2.35427\n",
      "[4400]\ttraining's rmse: 2.11194\tvalid_1's rmse: 2.35401\n",
      "[4450]\ttraining's rmse: 2.11032\tvalid_1's rmse: 2.35359\n",
      "[4500]\ttraining's rmse: 2.1086\tvalid_1's rmse: 2.35318\n",
      "[4550]\ttraining's rmse: 2.10719\tvalid_1's rmse: 2.35292\n",
      "[4600]\ttraining's rmse: 2.10585\tvalid_1's rmse: 2.35265\n",
      "[4650]\ttraining's rmse: 2.10412\tvalid_1's rmse: 2.35228\n",
      "[4700]\ttraining's rmse: 2.10224\tvalid_1's rmse: 2.35191\n",
      "[4750]\ttraining's rmse: 2.10082\tvalid_1's rmse: 2.35168\n",
      "[4800]\ttraining's rmse: 2.09928\tvalid_1's rmse: 2.35131\n",
      "[4850]\ttraining's rmse: 2.09784\tvalid_1's rmse: 2.35102\n",
      "[4900]\ttraining's rmse: 2.09618\tvalid_1's rmse: 2.35081\n",
      "[4950]\ttraining's rmse: 2.09473\tvalid_1's rmse: 2.35062\n",
      "[5000]\ttraining's rmse: 2.0931\tvalid_1's rmse: 2.35035\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.0931\tvalid_1's rmse: 2.35035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.59495\tvalid_1's rmse: 2.61494\n",
      "[100]\ttraining's rmse: 2.50343\tvalid_1's rmse: 2.52801\n",
      "[150]\ttraining's rmse: 2.48018\tvalid_1's rmse: 2.50889\n",
      "[200]\ttraining's rmse: 2.45832\tvalid_1's rmse: 2.49261\n",
      "[250]\ttraining's rmse: 2.4402\tvalid_1's rmse: 2.48019\n",
      "[300]\ttraining's rmse: 2.42529\tvalid_1's rmse: 2.47008\n",
      "[350]\ttraining's rmse: 2.41053\tvalid_1's rmse: 2.46077\n",
      "[400]\ttraining's rmse: 2.39838\tvalid_1's rmse: 2.45401\n",
      "[450]\ttraining's rmse: 2.38808\tvalid_1's rmse: 2.44852\n",
      "[500]\ttraining's rmse: 2.37806\tvalid_1's rmse: 2.44303\n",
      "[550]\ttraining's rmse: 2.36867\tvalid_1's rmse: 2.43835\n",
      "[600]\ttraining's rmse: 2.3604\tvalid_1's rmse: 2.43485\n",
      "[650]\ttraining's rmse: 2.35344\tvalid_1's rmse: 2.43231\n",
      "[700]\ttraining's rmse: 2.34677\tvalid_1's rmse: 2.43008\n",
      "[750]\ttraining's rmse: 2.34033\tvalid_1's rmse: 2.42765\n",
      "[800]\ttraining's rmse: 2.33456\tvalid_1's rmse: 2.42538\n",
      "[850]\ttraining's rmse: 2.32889\tvalid_1's rmse: 2.4232\n",
      "[900]\ttraining's rmse: 2.32277\tvalid_1's rmse: 2.42083\n",
      "[950]\ttraining's rmse: 2.31614\tvalid_1's rmse: 2.41827\n",
      "[1000]\ttraining's rmse: 2.3112\tvalid_1's rmse: 2.41673\n",
      "[1050]\ttraining's rmse: 2.30648\tvalid_1's rmse: 2.41491\n",
      "[1100]\ttraining's rmse: 2.30106\tvalid_1's rmse: 2.41329\n",
      "[1150]\ttraining's rmse: 2.29604\tvalid_1's rmse: 2.41176\n",
      "[1200]\ttraining's rmse: 2.29063\tvalid_1's rmse: 2.41012\n",
      "[1250]\ttraining's rmse: 2.28559\tvalid_1's rmse: 2.40833\n",
      "[1300]\ttraining's rmse: 2.2822\tvalid_1's rmse: 2.40722\n",
      "[1350]\ttraining's rmse: 2.27771\tvalid_1's rmse: 2.40562\n",
      "[1400]\ttraining's rmse: 2.27269\tvalid_1's rmse: 2.40379\n",
      "[1450]\ttraining's rmse: 2.26881\tvalid_1's rmse: 2.40256\n",
      "[1500]\ttraining's rmse: 2.26506\tvalid_1's rmse: 2.40154\n",
      "[1550]\ttraining's rmse: 2.26123\tvalid_1's rmse: 2.40045\n",
      "[1600]\ttraining's rmse: 2.25698\tvalid_1's rmse: 2.39917\n",
      "[1650]\ttraining's rmse: 2.25275\tvalid_1's rmse: 2.3981\n",
      "[1700]\ttraining's rmse: 2.24916\tvalid_1's rmse: 2.39699\n",
      "[1750]\ttraining's rmse: 2.24567\tvalid_1's rmse: 2.396\n",
      "[1800]\ttraining's rmse: 2.24251\tvalid_1's rmse: 2.39529\n",
      "[1850]\ttraining's rmse: 2.2394\tvalid_1's rmse: 2.39449\n",
      "[1900]\ttraining's rmse: 2.23609\tvalid_1's rmse: 2.3935\n",
      "[1950]\ttraining's rmse: 2.23266\tvalid_1's rmse: 2.39257\n",
      "[2000]\ttraining's rmse: 2.22869\tvalid_1's rmse: 2.3915\n",
      "[2050]\ttraining's rmse: 2.22522\tvalid_1's rmse: 2.39035\n",
      "[2100]\ttraining's rmse: 2.22217\tvalid_1's rmse: 2.38959\n",
      "[2150]\ttraining's rmse: 2.21926\tvalid_1's rmse: 2.38897\n",
      "[2200]\ttraining's rmse: 2.21617\tvalid_1's rmse: 2.38819\n",
      "[2250]\ttraining's rmse: 2.21324\tvalid_1's rmse: 2.38757\n",
      "[2300]\ttraining's rmse: 2.21042\tvalid_1's rmse: 2.38685\n",
      "[2350]\ttraining's rmse: 2.20773\tvalid_1's rmse: 2.38621\n",
      "[2400]\ttraining's rmse: 2.20531\tvalid_1's rmse: 2.38562\n",
      "[2450]\ttraining's rmse: 2.2022\tvalid_1's rmse: 2.38484\n",
      "[2500]\ttraining's rmse: 2.19959\tvalid_1's rmse: 2.3844\n",
      "[2550]\ttraining's rmse: 2.19644\tvalid_1's rmse: 2.38361\n",
      "[2600]\ttraining's rmse: 2.19353\tvalid_1's rmse: 2.38287\n",
      "[2650]\ttraining's rmse: 2.19084\tvalid_1's rmse: 2.3823\n",
      "[2700]\ttraining's rmse: 2.18805\tvalid_1's rmse: 2.38168\n",
      "[2750]\ttraining's rmse: 2.18568\tvalid_1's rmse: 2.38126\n",
      "[2800]\ttraining's rmse: 2.183\tvalid_1's rmse: 2.38081\n",
      "[2850]\ttraining's rmse: 2.18048\tvalid_1's rmse: 2.38031\n",
      "[2900]\ttraining's rmse: 2.17826\tvalid_1's rmse: 2.37975\n",
      "[2950]\ttraining's rmse: 2.17633\tvalid_1's rmse: 2.37925\n",
      "[3000]\ttraining's rmse: 2.17398\tvalid_1's rmse: 2.37864\n",
      "[3050]\ttraining's rmse: 2.17128\tvalid_1's rmse: 2.37816\n",
      "[3100]\ttraining's rmse: 2.16861\tvalid_1's rmse: 2.37755\n",
      "[3150]\ttraining's rmse: 2.16637\tvalid_1's rmse: 2.37716\n",
      "[3200]\ttraining's rmse: 2.16363\tvalid_1's rmse: 2.37652\n",
      "[3250]\ttraining's rmse: 2.1615\tvalid_1's rmse: 2.37608\n",
      "[3300]\ttraining's rmse: 2.15926\tvalid_1's rmse: 2.37552\n",
      "[3350]\ttraining's rmse: 2.15754\tvalid_1's rmse: 2.37523\n",
      "[3400]\ttraining's rmse: 2.15489\tvalid_1's rmse: 2.37474\n",
      "[3450]\ttraining's rmse: 2.15297\tvalid_1's rmse: 2.37428\n",
      "[3500]\ttraining's rmse: 2.15085\tvalid_1's rmse: 2.37399\n",
      "[3550]\ttraining's rmse: 2.14828\tvalid_1's rmse: 2.37337\n",
      "[3600]\ttraining's rmse: 2.14613\tvalid_1's rmse: 2.37292\n",
      "[3650]\ttraining's rmse: 2.14444\tvalid_1's rmse: 2.37255\n",
      "[3700]\ttraining's rmse: 2.14214\tvalid_1's rmse: 2.37199\n",
      "[3750]\ttraining's rmse: 2.13959\tvalid_1's rmse: 2.37163\n",
      "[3800]\ttraining's rmse: 2.13765\tvalid_1's rmse: 2.37136\n",
      "[3850]\ttraining's rmse: 2.13589\tvalid_1's rmse: 2.37088\n",
      "[3900]\ttraining's rmse: 2.13384\tvalid_1's rmse: 2.37053\n",
      "[3950]\ttraining's rmse: 2.13193\tvalid_1's rmse: 2.37024\n",
      "[4000]\ttraining's rmse: 2.12982\tvalid_1's rmse: 2.36986\n",
      "[4050]\ttraining's rmse: 2.12785\tvalid_1's rmse: 2.36965\n",
      "[4100]\ttraining's rmse: 2.12617\tvalid_1's rmse: 2.36943\n",
      "[4150]\ttraining's rmse: 2.1243\tvalid_1's rmse: 2.36904\n",
      "[4200]\ttraining's rmse: 2.12217\tvalid_1's rmse: 2.36863\n",
      "[4250]\ttraining's rmse: 2.12058\tvalid_1's rmse: 2.36841\n",
      "[4300]\ttraining's rmse: 2.11857\tvalid_1's rmse: 2.36807\n",
      "[4350]\ttraining's rmse: 2.11656\tvalid_1's rmse: 2.36772\n",
      "[4400]\ttraining's rmse: 2.11493\tvalid_1's rmse: 2.36743\n",
      "[4450]\ttraining's rmse: 2.1131\tvalid_1's rmse: 2.36721\n",
      "[4500]\ttraining's rmse: 2.11102\tvalid_1's rmse: 2.36695\n",
      "[4550]\ttraining's rmse: 2.10913\tvalid_1's rmse: 2.36645\n",
      "[4600]\ttraining's rmse: 2.10699\tvalid_1's rmse: 2.36609\n",
      "[4650]\ttraining's rmse: 2.10541\tvalid_1's rmse: 2.3658\n",
      "[4700]\ttraining's rmse: 2.10371\tvalid_1's rmse: 2.36545\n",
      "[4750]\ttraining's rmse: 2.1018\tvalid_1's rmse: 2.3651\n",
      "[4800]\ttraining's rmse: 2.10005\tvalid_1's rmse: 2.36497\n",
      "[4850]\ttraining's rmse: 2.09831\tvalid_1's rmse: 2.3649\n",
      "[4900]\ttraining's rmse: 2.09674\tvalid_1's rmse: 2.36469\n",
      "[4950]\ttraining's rmse: 2.09552\tvalid_1's rmse: 2.36447\n",
      "[5000]\ttraining's rmse: 2.09368\tvalid_1's rmse: 2.36414\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09368\tvalid_1's rmse: 2.36414\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.57691\tvalid_1's rmse: 2.5933\n",
      "[100]\ttraining's rmse: 2.50902\tvalid_1's rmse: 2.53021\n",
      "[150]\ttraining's rmse: 2.48583\tvalid_1's rmse: 2.51077\n",
      "[200]\ttraining's rmse: 2.46673\tvalid_1's rmse: 2.4962\n",
      "[250]\ttraining's rmse: 2.4473\tvalid_1's rmse: 2.48173\n",
      "[300]\ttraining's rmse: 2.43114\tvalid_1's rmse: 2.47067\n",
      "[350]\ttraining's rmse: 2.4152\tvalid_1's rmse: 2.46008\n",
      "[400]\ttraining's rmse: 2.40326\tvalid_1's rmse: 2.45282\n",
      "[450]\ttraining's rmse: 2.39231\tvalid_1's rmse: 2.44641\n",
      "[500]\ttraining's rmse: 2.3818\tvalid_1's rmse: 2.44103\n",
      "[550]\ttraining's rmse: 2.37288\tvalid_1's rmse: 2.43682\n",
      "[600]\ttraining's rmse: 2.36337\tvalid_1's rmse: 2.43244\n",
      "[650]\ttraining's rmse: 2.35555\tvalid_1's rmse: 2.4292\n",
      "[700]\ttraining's rmse: 2.34838\tvalid_1's rmse: 2.42616\n",
      "[750]\ttraining's rmse: 2.34109\tvalid_1's rmse: 2.42302\n",
      "[800]\ttraining's rmse: 2.33448\tvalid_1's rmse: 2.4204\n",
      "[850]\ttraining's rmse: 2.32839\tvalid_1's rmse: 2.4181\n",
      "[900]\ttraining's rmse: 2.32288\tvalid_1's rmse: 2.41632\n",
      "[950]\ttraining's rmse: 2.31791\tvalid_1's rmse: 2.41467\n",
      "[1000]\ttraining's rmse: 2.31203\tvalid_1's rmse: 2.41265\n",
      "[1050]\ttraining's rmse: 2.30665\tvalid_1's rmse: 2.41103\n",
      "[1100]\ttraining's rmse: 2.30138\tvalid_1's rmse: 2.40928\n",
      "[1150]\ttraining's rmse: 2.2965\tvalid_1's rmse: 2.40729\n",
      "[1200]\ttraining's rmse: 2.29205\tvalid_1's rmse: 2.4061\n",
      "[1250]\ttraining's rmse: 2.28627\tvalid_1's rmse: 2.40439\n",
      "[1300]\ttraining's rmse: 2.28169\tvalid_1's rmse: 2.40306\n",
      "[1350]\ttraining's rmse: 2.2771\tvalid_1's rmse: 2.40178\n",
      "[1400]\ttraining's rmse: 2.2731\tvalid_1's rmse: 2.40062\n",
      "[1450]\ttraining's rmse: 2.269\tvalid_1's rmse: 2.39932\n",
      "[1500]\ttraining's rmse: 2.26518\tvalid_1's rmse: 2.39827\n",
      "[1550]\ttraining's rmse: 2.26104\tvalid_1's rmse: 2.39695\n",
      "[1600]\ttraining's rmse: 2.25644\tvalid_1's rmse: 2.39548\n",
      "[1650]\ttraining's rmse: 2.25239\tvalid_1's rmse: 2.39411\n",
      "[1700]\ttraining's rmse: 2.24906\tvalid_1's rmse: 2.39339\n",
      "[1750]\ttraining's rmse: 2.24501\tvalid_1's rmse: 2.39207\n",
      "[1800]\ttraining's rmse: 2.24152\tvalid_1's rmse: 2.39113\n",
      "[1850]\ttraining's rmse: 2.23847\tvalid_1's rmse: 2.39027\n",
      "[1900]\ttraining's rmse: 2.23534\tvalid_1's rmse: 2.3894\n",
      "[1950]\ttraining's rmse: 2.23226\tvalid_1's rmse: 2.38863\n",
      "[2000]\ttraining's rmse: 2.22873\tvalid_1's rmse: 2.3876\n",
      "[2050]\ttraining's rmse: 2.22528\tvalid_1's rmse: 2.38661\n",
      "[2100]\ttraining's rmse: 2.22197\tvalid_1's rmse: 2.38556\n",
      "[2150]\ttraining's rmse: 2.2191\tvalid_1's rmse: 2.38465\n",
      "[2200]\ttraining's rmse: 2.21609\tvalid_1's rmse: 2.38412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.21311\tvalid_1's rmse: 2.38346\n",
      "[2300]\ttraining's rmse: 2.21032\tvalid_1's rmse: 2.3829\n",
      "[2350]\ttraining's rmse: 2.20753\tvalid_1's rmse: 2.38222\n",
      "[2400]\ttraining's rmse: 2.20499\tvalid_1's rmse: 2.38188\n",
      "[2450]\ttraining's rmse: 2.20225\tvalid_1's rmse: 2.3812\n",
      "[2500]\ttraining's rmse: 2.19907\tvalid_1's rmse: 2.38027\n",
      "[2550]\ttraining's rmse: 2.19638\tvalid_1's rmse: 2.37953\n",
      "[2600]\ttraining's rmse: 2.19355\tvalid_1's rmse: 2.37896\n",
      "[2650]\ttraining's rmse: 2.19066\tvalid_1's rmse: 2.37828\n",
      "[2700]\ttraining's rmse: 2.18816\tvalid_1's rmse: 2.37773\n",
      "[2750]\ttraining's rmse: 2.18555\tvalid_1's rmse: 2.37712\n",
      "[2800]\ttraining's rmse: 2.18306\tvalid_1's rmse: 2.37656\n",
      "[2850]\ttraining's rmse: 2.18069\tvalid_1's rmse: 2.37591\n",
      "[2900]\ttraining's rmse: 2.17812\tvalid_1's rmse: 2.37519\n",
      "[2950]\ttraining's rmse: 2.17595\tvalid_1's rmse: 2.37482\n",
      "[3000]\ttraining's rmse: 2.17335\tvalid_1's rmse: 2.37421\n",
      "[3050]\ttraining's rmse: 2.17081\tvalid_1's rmse: 2.37364\n",
      "[3100]\ttraining's rmse: 2.16836\tvalid_1's rmse: 2.37298\n",
      "[3150]\ttraining's rmse: 2.16569\tvalid_1's rmse: 2.37225\n",
      "[3200]\ttraining's rmse: 2.16338\tvalid_1's rmse: 2.3718\n",
      "[3250]\ttraining's rmse: 2.16022\tvalid_1's rmse: 2.37105\n",
      "[3300]\ttraining's rmse: 2.15799\tvalid_1's rmse: 2.37062\n",
      "[3350]\ttraining's rmse: 2.15545\tvalid_1's rmse: 2.36995\n",
      "[3400]\ttraining's rmse: 2.15349\tvalid_1's rmse: 2.3696\n",
      "[3450]\ttraining's rmse: 2.15134\tvalid_1's rmse: 2.36889\n",
      "[3500]\ttraining's rmse: 2.14924\tvalid_1's rmse: 2.3682\n",
      "[3550]\ttraining's rmse: 2.14664\tvalid_1's rmse: 2.36762\n",
      "[3600]\ttraining's rmse: 2.14403\tvalid_1's rmse: 2.36716\n",
      "[3650]\ttraining's rmse: 2.14184\tvalid_1's rmse: 2.36685\n",
      "[3700]\ttraining's rmse: 2.13959\tvalid_1's rmse: 2.36643\n",
      "[3750]\ttraining's rmse: 2.13763\tvalid_1's rmse: 2.36604\n",
      "[3800]\ttraining's rmse: 2.13519\tvalid_1's rmse: 2.36552\n",
      "[3850]\ttraining's rmse: 2.13293\tvalid_1's rmse: 2.36499\n",
      "[3900]\ttraining's rmse: 2.13102\tvalid_1's rmse: 2.36469\n",
      "[3950]\ttraining's rmse: 2.12916\tvalid_1's rmse: 2.36434\n",
      "[4000]\ttraining's rmse: 2.12683\tvalid_1's rmse: 2.36406\n",
      "[4050]\ttraining's rmse: 2.12515\tvalid_1's rmse: 2.3638\n",
      "[4100]\ttraining's rmse: 2.12343\tvalid_1's rmse: 2.36358\n",
      "[4150]\ttraining's rmse: 2.12116\tvalid_1's rmse: 2.36321\n",
      "[4200]\ttraining's rmse: 2.11921\tvalid_1's rmse: 2.36293\n",
      "[4250]\ttraining's rmse: 2.11725\tvalid_1's rmse: 2.36238\n",
      "[4300]\ttraining's rmse: 2.11529\tvalid_1's rmse: 2.36208\n",
      "[4350]\ttraining's rmse: 2.11352\tvalid_1's rmse: 2.36171\n",
      "[4400]\ttraining's rmse: 2.11195\tvalid_1's rmse: 2.36118\n",
      "[4450]\ttraining's rmse: 2.11009\tvalid_1's rmse: 2.36082\n",
      "[4500]\ttraining's rmse: 2.10814\tvalid_1's rmse: 2.36035\n",
      "[4550]\ttraining's rmse: 2.10639\tvalid_1's rmse: 2.36004\n",
      "[4600]\ttraining's rmse: 2.10433\tvalid_1's rmse: 2.35964\n",
      "[4650]\ttraining's rmse: 2.10271\tvalid_1's rmse: 2.35929\n",
      "[4700]\ttraining's rmse: 2.10088\tvalid_1's rmse: 2.35888\n",
      "[4750]\ttraining's rmse: 2.09916\tvalid_1's rmse: 2.35855\n",
      "[4800]\ttraining's rmse: 2.0975\tvalid_1's rmse: 2.35827\n",
      "[4850]\ttraining's rmse: 2.09592\tvalid_1's rmse: 2.35805\n",
      "[4900]\ttraining's rmse: 2.0941\tvalid_1's rmse: 2.35759\n",
      "[4950]\ttraining's rmse: 2.09225\tvalid_1's rmse: 2.35721\n",
      "[5000]\ttraining's rmse: 2.09006\tvalid_1's rmse: 2.35686\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09006\tvalid_1's rmse: 2.35686\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.58478\tvalid_1's rmse: 2.60605\n",
      "[100]\ttraining's rmse: 2.50907\tvalid_1's rmse: 2.53031\n",
      "[150]\ttraining's rmse: 2.48755\tvalid_1's rmse: 2.51169\n",
      "[200]\ttraining's rmse: 2.46732\tvalid_1's rmse: 2.49581\n",
      "[250]\ttraining's rmse: 2.44898\tvalid_1's rmse: 2.48233\n",
      "[300]\ttraining's rmse: 2.4333\tvalid_1's rmse: 2.47132\n",
      "[350]\ttraining's rmse: 2.41892\tvalid_1's rmse: 2.46263\n",
      "[400]\ttraining's rmse: 2.40634\tvalid_1's rmse: 2.45475\n",
      "[450]\ttraining's rmse: 2.3938\tvalid_1's rmse: 2.44802\n",
      "[500]\ttraining's rmse: 2.38324\tvalid_1's rmse: 2.44307\n",
      "[550]\ttraining's rmse: 2.37288\tvalid_1's rmse: 2.43799\n",
      "[600]\ttraining's rmse: 2.36431\tvalid_1's rmse: 2.43441\n",
      "[650]\ttraining's rmse: 2.35654\tvalid_1's rmse: 2.43122\n",
      "[700]\ttraining's rmse: 2.34951\tvalid_1's rmse: 2.42796\n",
      "[750]\ttraining's rmse: 2.34225\tvalid_1's rmse: 2.42522\n",
      "[800]\ttraining's rmse: 2.33492\tvalid_1's rmse: 2.42175\n",
      "[850]\ttraining's rmse: 2.32864\tvalid_1's rmse: 2.41947\n",
      "[900]\ttraining's rmse: 2.32244\tvalid_1's rmse: 2.41743\n",
      "[950]\ttraining's rmse: 2.31628\tvalid_1's rmse: 2.41524\n",
      "[1000]\ttraining's rmse: 2.31021\tvalid_1's rmse: 2.4133\n",
      "[1050]\ttraining's rmse: 2.30518\tvalid_1's rmse: 2.41146\n",
      "[1100]\ttraining's rmse: 2.29997\tvalid_1's rmse: 2.40965\n",
      "[1150]\ttraining's rmse: 2.29498\tvalid_1's rmse: 2.40802\n",
      "[1200]\ttraining's rmse: 2.28961\tvalid_1's rmse: 2.40629\n",
      "[1250]\ttraining's rmse: 2.28534\tvalid_1's rmse: 2.40483\n",
      "[1300]\ttraining's rmse: 2.28051\tvalid_1's rmse: 2.40299\n",
      "[1350]\ttraining's rmse: 2.27558\tvalid_1's rmse: 2.4018\n",
      "[1400]\ttraining's rmse: 2.27055\tvalid_1's rmse: 2.40008\n",
      "[1450]\ttraining's rmse: 2.26667\tvalid_1's rmse: 2.39878\n",
      "[1500]\ttraining's rmse: 2.26307\tvalid_1's rmse: 2.39759\n",
      "[1550]\ttraining's rmse: 2.25852\tvalid_1's rmse: 2.39639\n",
      "[1600]\ttraining's rmse: 2.25538\tvalid_1's rmse: 2.39558\n",
      "[1650]\ttraining's rmse: 2.25148\tvalid_1's rmse: 2.39421\n",
      "[1700]\ttraining's rmse: 2.24801\tvalid_1's rmse: 2.39324\n",
      "[1750]\ttraining's rmse: 2.2435\tvalid_1's rmse: 2.39195\n",
      "[1800]\ttraining's rmse: 2.23995\tvalid_1's rmse: 2.3911\n",
      "[1850]\ttraining's rmse: 2.23615\tvalid_1's rmse: 2.39008\n",
      "[1900]\ttraining's rmse: 2.23299\tvalid_1's rmse: 2.38931\n",
      "[1950]\ttraining's rmse: 2.2293\tvalid_1's rmse: 2.38842\n",
      "[2000]\ttraining's rmse: 2.22592\tvalid_1's rmse: 2.38769\n",
      "[2050]\ttraining's rmse: 2.22305\tvalid_1's rmse: 2.38705\n",
      "[2100]\ttraining's rmse: 2.2202\tvalid_1's rmse: 2.38635\n",
      "[2150]\ttraining's rmse: 2.21771\tvalid_1's rmse: 2.38568\n",
      "[2200]\ttraining's rmse: 2.21436\tvalid_1's rmse: 2.38478\n",
      "[2250]\ttraining's rmse: 2.21204\tvalid_1's rmse: 2.38427\n",
      "[2300]\ttraining's rmse: 2.20909\tvalid_1's rmse: 2.38378\n",
      "[2350]\ttraining's rmse: 2.20621\tvalid_1's rmse: 2.38297\n",
      "[2400]\ttraining's rmse: 2.20289\tvalid_1's rmse: 2.3822\n",
      "[2450]\ttraining's rmse: 2.19967\tvalid_1's rmse: 2.3814\n",
      "[2500]\ttraining's rmse: 2.1963\tvalid_1's rmse: 2.38033\n",
      "[2550]\ttraining's rmse: 2.19381\tvalid_1's rmse: 2.37968\n",
      "[2600]\ttraining's rmse: 2.19096\tvalid_1's rmse: 2.37889\n",
      "[2650]\ttraining's rmse: 2.18811\tvalid_1's rmse: 2.37811\n",
      "[2700]\ttraining's rmse: 2.18556\tvalid_1's rmse: 2.37757\n",
      "[2750]\ttraining's rmse: 2.18288\tvalid_1's rmse: 2.37699\n",
      "[2800]\ttraining's rmse: 2.18028\tvalid_1's rmse: 2.37615\n",
      "[2850]\ttraining's rmse: 2.17761\tvalid_1's rmse: 2.37553\n",
      "[2900]\ttraining's rmse: 2.17561\tvalid_1's rmse: 2.37518\n",
      "[2950]\ttraining's rmse: 2.17301\tvalid_1's rmse: 2.37459\n",
      "[3000]\ttraining's rmse: 2.17032\tvalid_1's rmse: 2.37388\n",
      "[3050]\ttraining's rmse: 2.16787\tvalid_1's rmse: 2.3732\n",
      "[3100]\ttraining's rmse: 2.16487\tvalid_1's rmse: 2.37254\n",
      "[3150]\ttraining's rmse: 2.16252\tvalid_1's rmse: 2.37198\n",
      "[3200]\ttraining's rmse: 2.16031\tvalid_1's rmse: 2.37155\n",
      "[3250]\ttraining's rmse: 2.15819\tvalid_1's rmse: 2.37117\n",
      "[3300]\ttraining's rmse: 2.15611\tvalid_1's rmse: 2.37072\n",
      "[3350]\ttraining's rmse: 2.15342\tvalid_1's rmse: 2.37025\n",
      "[3400]\ttraining's rmse: 2.15145\tvalid_1's rmse: 2.36989\n",
      "[3450]\ttraining's rmse: 2.14889\tvalid_1's rmse: 2.36935\n",
      "[3500]\ttraining's rmse: 2.14688\tvalid_1's rmse: 2.36886\n",
      "[3550]\ttraining's rmse: 2.14482\tvalid_1's rmse: 2.36853\n",
      "[3600]\ttraining's rmse: 2.14267\tvalid_1's rmse: 2.36824\n",
      "[3650]\ttraining's rmse: 2.14075\tvalid_1's rmse: 2.36795\n",
      "[3700]\ttraining's rmse: 2.1387\tvalid_1's rmse: 2.3677\n",
      "[3750]\ttraining's rmse: 2.13665\tvalid_1's rmse: 2.36717\n",
      "[3800]\ttraining's rmse: 2.13453\tvalid_1's rmse: 2.36668\n",
      "[3850]\ttraining's rmse: 2.1327\tvalid_1's rmse: 2.36626\n",
      "[3900]\ttraining's rmse: 2.13089\tvalid_1's rmse: 2.36591\n",
      "[3950]\ttraining's rmse: 2.12919\tvalid_1's rmse: 2.36554\n",
      "[4000]\ttraining's rmse: 2.12717\tvalid_1's rmse: 2.36524\n",
      "[4050]\ttraining's rmse: 2.1251\tvalid_1's rmse: 2.36468\n",
      "[4100]\ttraining's rmse: 2.12333\tvalid_1's rmse: 2.3643\n",
      "[4150]\ttraining's rmse: 2.12049\tvalid_1's rmse: 2.36373\n",
      "[4200]\ttraining's rmse: 2.11867\tvalid_1's rmse: 2.36353\n",
      "[4250]\ttraining's rmse: 2.11651\tvalid_1's rmse: 2.3631\n",
      "[4300]\ttraining's rmse: 2.11504\tvalid_1's rmse: 2.36298\n",
      "[4350]\ttraining's rmse: 2.11311\tvalid_1's rmse: 2.36263\n",
      "[4400]\ttraining's rmse: 2.11089\tvalid_1's rmse: 2.36204\n",
      "[4450]\ttraining's rmse: 2.10943\tvalid_1's rmse: 2.36181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500]\ttraining's rmse: 2.10765\tvalid_1's rmse: 2.36153\n",
      "[4550]\ttraining's rmse: 2.10583\tvalid_1's rmse: 2.36105\n",
      "[4600]\ttraining's rmse: 2.10431\tvalid_1's rmse: 2.36068\n",
      "[4650]\ttraining's rmse: 2.10241\tvalid_1's rmse: 2.36036\n",
      "[4700]\ttraining's rmse: 2.10068\tvalid_1's rmse: 2.36001\n",
      "[4750]\ttraining's rmse: 2.09892\tvalid_1's rmse: 2.35971\n",
      "[4800]\ttraining's rmse: 2.09701\tvalid_1's rmse: 2.35945\n",
      "[4850]\ttraining's rmse: 2.09551\tvalid_1's rmse: 2.35928\n",
      "[4900]\ttraining's rmse: 2.09363\tvalid_1's rmse: 2.35895\n",
      "[4950]\ttraining's rmse: 2.09215\tvalid_1's rmse: 2.35871\n",
      "[5000]\ttraining's rmse: 2.09034\tvalid_1's rmse: 2.3583\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09034\tvalid_1's rmse: 2.3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.57171\tvalid_1's rmse: 2.58249\n",
      "[100]\ttraining's rmse: 2.50376\tvalid_1's rmse: 2.52083\n",
      "[150]\ttraining's rmse: 2.4817\tvalid_1's rmse: 2.50369\n",
      "[200]\ttraining's rmse: 2.45948\tvalid_1's rmse: 2.48731\n",
      "[250]\ttraining's rmse: 2.44251\tvalid_1's rmse: 2.47552\n",
      "[300]\ttraining's rmse: 2.42777\tvalid_1's rmse: 2.46577\n",
      "[350]\ttraining's rmse: 2.41259\tvalid_1's rmse: 2.45582\n",
      "[400]\ttraining's rmse: 2.39919\tvalid_1's rmse: 2.44816\n",
      "[450]\ttraining's rmse: 2.38758\tvalid_1's rmse: 2.44178\n",
      "[500]\ttraining's rmse: 2.37835\tvalid_1's rmse: 2.43723\n",
      "[550]\ttraining's rmse: 2.36856\tvalid_1's rmse: 2.43226\n",
      "[600]\ttraining's rmse: 2.36134\tvalid_1's rmse: 2.4291\n",
      "[650]\ttraining's rmse: 2.35384\tvalid_1's rmse: 2.42591\n",
      "[700]\ttraining's rmse: 2.34597\tvalid_1's rmse: 2.42273\n",
      "[750]\ttraining's rmse: 2.33969\tvalid_1's rmse: 2.42003\n",
      "[800]\ttraining's rmse: 2.33349\tvalid_1's rmse: 2.4174\n",
      "[850]\ttraining's rmse: 2.327\tvalid_1's rmse: 2.41521\n",
      "[900]\ttraining's rmse: 2.32055\tvalid_1's rmse: 2.41252\n",
      "[950]\ttraining's rmse: 2.31532\tvalid_1's rmse: 2.41085\n",
      "[1000]\ttraining's rmse: 2.30932\tvalid_1's rmse: 2.4089\n",
      "[1050]\ttraining's rmse: 2.30471\tvalid_1's rmse: 2.40741\n",
      "[1100]\ttraining's rmse: 2.29963\tvalid_1's rmse: 2.40555\n",
      "[1150]\ttraining's rmse: 2.29476\tvalid_1's rmse: 2.40401\n",
      "[1200]\ttraining's rmse: 2.29069\tvalid_1's rmse: 2.403\n",
      "[1250]\ttraining's rmse: 2.28594\tvalid_1's rmse: 2.40171\n",
      "[1300]\ttraining's rmse: 2.28068\tvalid_1's rmse: 2.4001\n",
      "[1350]\ttraining's rmse: 2.27579\tvalid_1's rmse: 2.39854\n",
      "[1400]\ttraining's rmse: 2.27182\tvalid_1's rmse: 2.39734\n",
      "[1450]\ttraining's rmse: 2.26769\tvalid_1's rmse: 2.39622\n",
      "[1500]\ttraining's rmse: 2.26319\tvalid_1's rmse: 2.39533\n",
      "[1550]\ttraining's rmse: 2.25935\tvalid_1's rmse: 2.39428\n",
      "[1600]\ttraining's rmse: 2.25537\tvalid_1's rmse: 2.39319\n",
      "[1650]\ttraining's rmse: 2.25239\tvalid_1's rmse: 2.39242\n",
      "[1700]\ttraining's rmse: 2.24855\tvalid_1's rmse: 2.39122\n",
      "[1750]\ttraining's rmse: 2.24492\tvalid_1's rmse: 2.39018\n",
      "[1800]\ttraining's rmse: 2.24151\tvalid_1's rmse: 2.38929\n",
      "[1850]\ttraining's rmse: 2.23784\tvalid_1's rmse: 2.38835\n",
      "[1900]\ttraining's rmse: 2.23464\tvalid_1's rmse: 2.38766\n",
      "[1950]\ttraining's rmse: 2.23119\tvalid_1's rmse: 2.38695\n",
      "[2000]\ttraining's rmse: 2.22814\tvalid_1's rmse: 2.38617\n",
      "[2050]\ttraining's rmse: 2.22437\tvalid_1's rmse: 2.38537\n",
      "[2100]\ttraining's rmse: 2.22076\tvalid_1's rmse: 2.3844\n",
      "[2150]\ttraining's rmse: 2.21713\tvalid_1's rmse: 2.3831\n",
      "[2200]\ttraining's rmse: 2.21445\tvalid_1's rmse: 2.38233\n",
      "[2250]\ttraining's rmse: 2.21107\tvalid_1's rmse: 2.38122\n",
      "[2300]\ttraining's rmse: 2.20801\tvalid_1's rmse: 2.38048\n",
      "[2350]\ttraining's rmse: 2.20509\tvalid_1's rmse: 2.37976\n",
      "[2400]\ttraining's rmse: 2.20293\tvalid_1's rmse: 2.37927\n",
      "[2450]\ttraining's rmse: 2.19999\tvalid_1's rmse: 2.3785\n",
      "[2500]\ttraining's rmse: 2.19696\tvalid_1's rmse: 2.37761\n",
      "[2550]\ttraining's rmse: 2.19381\tvalid_1's rmse: 2.37705\n",
      "[2600]\ttraining's rmse: 2.19075\tvalid_1's rmse: 2.3764\n",
      "[2650]\ttraining's rmse: 2.18784\tvalid_1's rmse: 2.37583\n",
      "[2700]\ttraining's rmse: 2.18528\tvalid_1's rmse: 2.37535\n",
      "[2750]\ttraining's rmse: 2.18269\tvalid_1's rmse: 2.37469\n",
      "[2800]\ttraining's rmse: 2.18045\tvalid_1's rmse: 2.37417\n",
      "[2850]\ttraining's rmse: 2.17795\tvalid_1's rmse: 2.3735\n",
      "[2900]\ttraining's rmse: 2.17508\tvalid_1's rmse: 2.37287\n",
      "[2950]\ttraining's rmse: 2.17269\tvalid_1's rmse: 2.37224\n",
      "[3000]\ttraining's rmse: 2.17038\tvalid_1's rmse: 2.37192\n",
      "[3050]\ttraining's rmse: 2.16855\tvalid_1's rmse: 2.37149\n",
      "[3100]\ttraining's rmse: 2.16598\tvalid_1's rmse: 2.37101\n",
      "[3150]\ttraining's rmse: 2.16393\tvalid_1's rmse: 2.37082\n",
      "[3200]\ttraining's rmse: 2.16165\tvalid_1's rmse: 2.3704\n",
      "[3250]\ttraining's rmse: 2.15911\tvalid_1's rmse: 2.36987\n",
      "[3300]\ttraining's rmse: 2.15697\tvalid_1's rmse: 2.36934\n",
      "[3350]\ttraining's rmse: 2.15464\tvalid_1's rmse: 2.36897\n",
      "[3400]\ttraining's rmse: 2.15242\tvalid_1's rmse: 2.36841\n",
      "[3450]\ttraining's rmse: 2.15035\tvalid_1's rmse: 2.36804\n",
      "[3500]\ttraining's rmse: 2.14781\tvalid_1's rmse: 2.36748\n",
      "[3550]\ttraining's rmse: 2.14586\tvalid_1's rmse: 2.36719\n",
      "[3600]\ttraining's rmse: 2.14396\tvalid_1's rmse: 2.36665\n",
      "[3650]\ttraining's rmse: 2.14162\tvalid_1's rmse: 2.36611\n",
      "[3700]\ttraining's rmse: 2.13955\tvalid_1's rmse: 2.3658\n",
      "[3750]\ttraining's rmse: 2.13713\tvalid_1's rmse: 2.36524\n",
      "[3800]\ttraining's rmse: 2.13513\tvalid_1's rmse: 2.36493\n",
      "[3850]\ttraining's rmse: 2.13308\tvalid_1's rmse: 2.36449\n",
      "[3900]\ttraining's rmse: 2.13079\tvalid_1's rmse: 2.36416\n",
      "[3950]\ttraining's rmse: 2.12856\tvalid_1's rmse: 2.36397\n",
      "[4000]\ttraining's rmse: 2.12682\tvalid_1's rmse: 2.36352\n",
      "[4050]\ttraining's rmse: 2.12461\tvalid_1's rmse: 2.36318\n",
      "[4100]\ttraining's rmse: 2.12261\tvalid_1's rmse: 2.36289\n",
      "[4150]\ttraining's rmse: 2.12041\tvalid_1's rmse: 2.36257\n",
      "[4200]\ttraining's rmse: 2.11834\tvalid_1's rmse: 2.36246\n",
      "[4250]\ttraining's rmse: 2.11654\tvalid_1's rmse: 2.36201\n",
      "[4300]\ttraining's rmse: 2.11476\tvalid_1's rmse: 2.36182\n",
      "[4350]\ttraining's rmse: 2.11284\tvalid_1's rmse: 2.36148\n",
      "[4400]\ttraining's rmse: 2.11128\tvalid_1's rmse: 2.36115\n",
      "[4450]\ttraining's rmse: 2.10946\tvalid_1's rmse: 2.36077\n",
      "[4500]\ttraining's rmse: 2.10801\tvalid_1's rmse: 2.36041\n",
      "[4550]\ttraining's rmse: 2.10595\tvalid_1's rmse: 2.36009\n",
      "[4600]\ttraining's rmse: 2.10404\tvalid_1's rmse: 2.35979\n",
      "[4650]\ttraining's rmse: 2.10257\tvalid_1's rmse: 2.35955\n",
      "[4700]\ttraining's rmse: 2.1006\tvalid_1's rmse: 2.35923\n",
      "[4750]\ttraining's rmse: 2.09906\tvalid_1's rmse: 2.35906\n",
      "[4800]\ttraining's rmse: 2.09739\tvalid_1's rmse: 2.35882\n",
      "[4850]\ttraining's rmse: 2.09566\tvalid_1's rmse: 2.35846\n",
      "[4900]\ttraining's rmse: 2.09371\tvalid_1's rmse: 2.35805\n",
      "[4950]\ttraining's rmse: 2.09189\tvalid_1's rmse: 2.35737\n",
      "[5000]\ttraining's rmse: 2.09045\tvalid_1's rmse: 2.35731\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09045\tvalid_1's rmse: 2.35731\n",
      "1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.59054\tvalid_1's rmse: 2.61343\n",
      "[100]\ttraining's rmse: 2.50531\tvalid_1's rmse: 2.53333\n",
      "[150]\ttraining's rmse: 2.48348\tvalid_1's rmse: 2.51548\n",
      "[200]\ttraining's rmse: 2.46552\tvalid_1's rmse: 2.5024\n",
      "[250]\ttraining's rmse: 2.44758\tvalid_1's rmse: 2.48955\n",
      "[300]\ttraining's rmse: 2.4324\tvalid_1's rmse: 2.4791\n",
      "[350]\ttraining's rmse: 2.41812\tvalid_1's rmse: 2.47061\n",
      "[400]\ttraining's rmse: 2.40532\tvalid_1's rmse: 2.46311\n",
      "[450]\ttraining's rmse: 2.3926\tvalid_1's rmse: 2.45652\n",
      "[500]\ttraining's rmse: 2.38167\tvalid_1's rmse: 2.45055\n",
      "[550]\ttraining's rmse: 2.3729\tvalid_1's rmse: 2.44706\n",
      "[600]\ttraining's rmse: 2.36459\tvalid_1's rmse: 2.44365\n",
      "[650]\ttraining's rmse: 2.35623\tvalid_1's rmse: 2.43985\n",
      "[700]\ttraining's rmse: 2.34962\tvalid_1's rmse: 2.43723\n",
      "[750]\ttraining's rmse: 2.34287\tvalid_1's rmse: 2.43428\n",
      "[800]\ttraining's rmse: 2.33587\tvalid_1's rmse: 2.43174\n",
      "[850]\ttraining's rmse: 2.32982\tvalid_1's rmse: 2.42915\n",
      "[900]\ttraining's rmse: 2.32338\tvalid_1's rmse: 2.42642\n",
      "[950]\ttraining's rmse: 2.31725\tvalid_1's rmse: 2.42399\n",
      "[1000]\ttraining's rmse: 2.3114\tvalid_1's rmse: 2.42214\n",
      "[1050]\ttraining's rmse: 2.30539\tvalid_1's rmse: 2.42043\n",
      "[1100]\ttraining's rmse: 2.29987\tvalid_1's rmse: 2.41863\n",
      "[1150]\ttraining's rmse: 2.29507\tvalid_1's rmse: 2.41726\n",
      "[1200]\ttraining's rmse: 2.28947\tvalid_1's rmse: 2.41543\n",
      "[1250]\ttraining's rmse: 2.28557\tvalid_1's rmse: 2.41419\n",
      "[1300]\ttraining's rmse: 2.28019\tvalid_1's rmse: 2.41234\n",
      "[1350]\ttraining's rmse: 2.27611\tvalid_1's rmse: 2.41118\n",
      "[1400]\ttraining's rmse: 2.27232\tvalid_1's rmse: 2.41033\n",
      "[1450]\ttraining's rmse: 2.26794\tvalid_1's rmse: 2.40896\n",
      "[1500]\ttraining's rmse: 2.26379\tvalid_1's rmse: 2.40766\n",
      "[1550]\ttraining's rmse: 2.25903\tvalid_1's rmse: 2.40638\n",
      "[1600]\ttraining's rmse: 2.25486\tvalid_1's rmse: 2.40517\n",
      "[1650]\ttraining's rmse: 2.25108\tvalid_1's rmse: 2.40402\n",
      "[1700]\ttraining's rmse: 2.24726\tvalid_1's rmse: 2.40299\n",
      "[1750]\ttraining's rmse: 2.24382\tvalid_1's rmse: 2.40219\n",
      "[1800]\ttraining's rmse: 2.23997\tvalid_1's rmse: 2.4009\n",
      "[1850]\ttraining's rmse: 2.23653\tvalid_1's rmse: 2.39998\n",
      "[1900]\ttraining's rmse: 2.23275\tvalid_1's rmse: 2.39894\n",
      "[1950]\ttraining's rmse: 2.22924\tvalid_1's rmse: 2.39794\n",
      "[2000]\ttraining's rmse: 2.22596\tvalid_1's rmse: 2.39732\n",
      "[2050]\ttraining's rmse: 2.22199\tvalid_1's rmse: 2.39607\n",
      "[2100]\ttraining's rmse: 2.21916\tvalid_1's rmse: 2.3956\n",
      "[2150]\ttraining's rmse: 2.21606\tvalid_1's rmse: 2.39493\n",
      "[2200]\ttraining's rmse: 2.21316\tvalid_1's rmse: 2.39446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\ttraining's rmse: 2.2103\tvalid_1's rmse: 2.39368\n",
      "[2300]\ttraining's rmse: 2.20698\tvalid_1's rmse: 2.39275\n",
      "[2350]\ttraining's rmse: 2.20386\tvalid_1's rmse: 2.39204\n",
      "[2400]\ttraining's rmse: 2.20041\tvalid_1's rmse: 2.39156\n",
      "[2450]\ttraining's rmse: 2.19747\tvalid_1's rmse: 2.39109\n",
      "[2500]\ttraining's rmse: 2.19425\tvalid_1's rmse: 2.39029\n",
      "[2550]\ttraining's rmse: 2.19115\tvalid_1's rmse: 2.38928\n",
      "[2600]\ttraining's rmse: 2.18865\tvalid_1's rmse: 2.38862\n",
      "[2650]\ttraining's rmse: 2.18596\tvalid_1's rmse: 2.38795\n",
      "[2700]\ttraining's rmse: 2.18306\tvalid_1's rmse: 2.38757\n",
      "[2750]\ttraining's rmse: 2.18001\tvalid_1's rmse: 2.38692\n",
      "[2800]\ttraining's rmse: 2.17716\tvalid_1's rmse: 2.38639\n",
      "[2850]\ttraining's rmse: 2.17471\tvalid_1's rmse: 2.38589\n",
      "[2900]\ttraining's rmse: 2.17218\tvalid_1's rmse: 2.38544\n",
      "[2950]\ttraining's rmse: 2.16988\tvalid_1's rmse: 2.38486\n",
      "[3000]\ttraining's rmse: 2.16736\tvalid_1's rmse: 2.38453\n",
      "[3050]\ttraining's rmse: 2.16501\tvalid_1's rmse: 2.38414\n",
      "[3100]\ttraining's rmse: 2.16238\tvalid_1's rmse: 2.38351\n",
      "[3150]\ttraining's rmse: 2.16042\tvalid_1's rmse: 2.38318\n",
      "[3200]\ttraining's rmse: 2.15848\tvalid_1's rmse: 2.38282\n",
      "[3250]\ttraining's rmse: 2.15622\tvalid_1's rmse: 2.38251\n",
      "[3300]\ttraining's rmse: 2.15338\tvalid_1's rmse: 2.3818\n",
      "[3350]\ttraining's rmse: 2.1509\tvalid_1's rmse: 2.38122\n",
      "[3400]\ttraining's rmse: 2.14852\tvalid_1's rmse: 2.38063\n",
      "[3450]\ttraining's rmse: 2.14554\tvalid_1's rmse: 2.38009\n",
      "[3500]\ttraining's rmse: 2.14333\tvalid_1's rmse: 2.37957\n",
      "[3550]\ttraining's rmse: 2.14099\tvalid_1's rmse: 2.37887\n",
      "[3600]\ttraining's rmse: 2.1393\tvalid_1's rmse: 2.37858\n",
      "[3650]\ttraining's rmse: 2.13682\tvalid_1's rmse: 2.37826\n",
      "[3700]\ttraining's rmse: 2.13441\tvalid_1's rmse: 2.37791\n",
      "[3750]\ttraining's rmse: 2.13233\tvalid_1's rmse: 2.37756\n",
      "[3800]\ttraining's rmse: 2.13015\tvalid_1's rmse: 2.37727\n",
      "[3850]\ttraining's rmse: 2.12817\tvalid_1's rmse: 2.3769\n",
      "[3900]\ttraining's rmse: 2.12568\tvalid_1's rmse: 2.37672\n",
      "[3950]\ttraining's rmse: 2.12374\tvalid_1's rmse: 2.3763\n",
      "[4000]\ttraining's rmse: 2.12165\tvalid_1's rmse: 2.37597\n",
      "[4050]\ttraining's rmse: 2.11929\tvalid_1's rmse: 2.37545\n",
      "[4100]\ttraining's rmse: 2.11763\tvalid_1's rmse: 2.37512\n",
      "[4150]\ttraining's rmse: 2.11569\tvalid_1's rmse: 2.37473\n",
      "[4200]\ttraining's rmse: 2.11378\tvalid_1's rmse: 2.37443\n",
      "[4250]\ttraining's rmse: 2.11171\tvalid_1's rmse: 2.37395\n",
      "[4300]\ttraining's rmse: 2.1097\tvalid_1's rmse: 2.37358\n",
      "[4350]\ttraining's rmse: 2.10791\tvalid_1's rmse: 2.37316\n",
      "[4400]\ttraining's rmse: 2.10589\tvalid_1's rmse: 2.37278\n",
      "[4450]\ttraining's rmse: 2.10367\tvalid_1's rmse: 2.37229\n",
      "[4500]\ttraining's rmse: 2.10214\tvalid_1's rmse: 2.37211\n",
      "[4550]\ttraining's rmse: 2.10045\tvalid_1's rmse: 2.37194\n",
      "[4600]\ttraining's rmse: 2.09865\tvalid_1's rmse: 2.37166\n",
      "[4650]\ttraining's rmse: 2.09701\tvalid_1's rmse: 2.37152\n",
      "[4700]\ttraining's rmse: 2.09515\tvalid_1's rmse: 2.37093\n",
      "[4750]\ttraining's rmse: 2.09316\tvalid_1's rmse: 2.37058\n",
      "[4800]\ttraining's rmse: 2.09161\tvalid_1's rmse: 2.37042\n",
      "[4850]\ttraining's rmse: 2.09016\tvalid_1's rmse: 2.37001\n",
      "[4900]\ttraining's rmse: 2.08861\tvalid_1's rmse: 2.36971\n",
      "[4950]\ttraining's rmse: 2.08644\tvalid_1's rmse: 2.36914\n",
      "[5000]\ttraining's rmse: 2.08476\tvalid_1's rmse: 2.36889\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.08476\tvalid_1's rmse: 2.36889\n",
      "2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 2.65907\tvalid_1's rmse: 2.68275\n",
      "[100]\ttraining's rmse: 2.52665\tvalid_1's rmse: 2.54157\n",
      "[150]\ttraining's rmse: 2.50229\tvalid_1's rmse: 2.51937\n",
      "[200]\ttraining's rmse: 2.48252\tvalid_1's rmse: 2.50372\n",
      "[250]\ttraining's rmse: 2.46549\tvalid_1's rmse: 2.49125\n",
      "[300]\ttraining's rmse: 2.44867\tvalid_1's rmse: 2.48048\n",
      "[350]\ttraining's rmse: 2.43457\tvalid_1's rmse: 2.4722\n",
      "[400]\ttraining's rmse: 2.41829\tvalid_1's rmse: 2.46205\n",
      "[450]\ttraining's rmse: 2.40619\tvalid_1's rmse: 2.45472\n",
      "[500]\ttraining's rmse: 2.39473\tvalid_1's rmse: 2.4486\n",
      "[550]\ttraining's rmse: 2.38439\tvalid_1's rmse: 2.44295\n",
      "[600]\ttraining's rmse: 2.3757\tvalid_1's rmse: 2.43919\n",
      "[650]\ttraining's rmse: 2.36684\tvalid_1's rmse: 2.43443\n",
      "[700]\ttraining's rmse: 2.35907\tvalid_1's rmse: 2.43108\n",
      "[750]\ttraining's rmse: 2.35086\tvalid_1's rmse: 2.42717\n",
      "[800]\ttraining's rmse: 2.34441\tvalid_1's rmse: 2.42486\n",
      "[850]\ttraining's rmse: 2.33834\tvalid_1's rmse: 2.42274\n",
      "[900]\ttraining's rmse: 2.33152\tvalid_1's rmse: 2.41974\n",
      "[950]\ttraining's rmse: 2.32598\tvalid_1's rmse: 2.41778\n",
      "[1000]\ttraining's rmse: 2.32085\tvalid_1's rmse: 2.41595\n",
      "[1050]\ttraining's rmse: 2.3143\tvalid_1's rmse: 2.4133\n",
      "[1100]\ttraining's rmse: 2.30833\tvalid_1's rmse: 2.41095\n",
      "[1150]\ttraining's rmse: 2.30372\tvalid_1's rmse: 2.40944\n",
      "[1200]\ttraining's rmse: 2.29847\tvalid_1's rmse: 2.40738\n",
      "[1250]\ttraining's rmse: 2.29369\tvalid_1's rmse: 2.40613\n",
      "[1300]\ttraining's rmse: 2.28919\tvalid_1's rmse: 2.40473\n",
      "[1350]\ttraining's rmse: 2.28485\tvalid_1's rmse: 2.40339\n",
      "[1400]\ttraining's rmse: 2.28038\tvalid_1's rmse: 2.40225\n",
      "[1450]\ttraining's rmse: 2.27581\tvalid_1's rmse: 2.40069\n",
      "[1500]\ttraining's rmse: 2.27208\tvalid_1's rmse: 2.39949\n",
      "[1550]\ttraining's rmse: 2.26802\tvalid_1's rmse: 2.39819\n",
      "[1600]\ttraining's rmse: 2.26404\tvalid_1's rmse: 2.39692\n",
      "[1650]\ttraining's rmse: 2.25971\tvalid_1's rmse: 2.39582\n",
      "[1700]\ttraining's rmse: 2.25613\tvalid_1's rmse: 2.39479\n",
      "[1750]\ttraining's rmse: 2.25222\tvalid_1's rmse: 2.39338\n",
      "[1800]\ttraining's rmse: 2.24803\tvalid_1's rmse: 2.39218\n",
      "[1850]\ttraining's rmse: 2.24444\tvalid_1's rmse: 2.39113\n",
      "[1900]\ttraining's rmse: 2.24106\tvalid_1's rmse: 2.3901\n",
      "[1950]\ttraining's rmse: 2.23704\tvalid_1's rmse: 2.38914\n",
      "[2000]\ttraining's rmse: 2.23317\tvalid_1's rmse: 2.38775\n",
      "[2050]\ttraining's rmse: 2.22996\tvalid_1's rmse: 2.38688\n",
      "[2100]\ttraining's rmse: 2.22656\tvalid_1's rmse: 2.38574\n",
      "[2150]\ttraining's rmse: 2.2231\tvalid_1's rmse: 2.3851\n",
      "[2200]\ttraining's rmse: 2.2195\tvalid_1's rmse: 2.38403\n",
      "[2250]\ttraining's rmse: 2.21641\tvalid_1's rmse: 2.3832\n",
      "[2300]\ttraining's rmse: 2.21311\tvalid_1's rmse: 2.3822\n",
      "[2350]\ttraining's rmse: 2.20998\tvalid_1's rmse: 2.3812\n",
      "[2400]\ttraining's rmse: 2.20709\tvalid_1's rmse: 2.38027\n",
      "[2450]\ttraining's rmse: 2.20418\tvalid_1's rmse: 2.37958\n",
      "[2500]\ttraining's rmse: 2.20126\tvalid_1's rmse: 2.37884\n",
      "[2550]\ttraining's rmse: 2.19825\tvalid_1's rmse: 2.3782\n",
      "[2600]\ttraining's rmse: 2.19542\tvalid_1's rmse: 2.37759\n",
      "[2650]\ttraining's rmse: 2.19178\tvalid_1's rmse: 2.37627\n",
      "[2700]\ttraining's rmse: 2.18947\tvalid_1's rmse: 2.37572\n",
      "[2750]\ttraining's rmse: 2.18675\tvalid_1's rmse: 2.37517\n",
      "[2800]\ttraining's rmse: 2.18383\tvalid_1's rmse: 2.37469\n",
      "[2850]\ttraining's rmse: 2.18128\tvalid_1's rmse: 2.37404\n",
      "[2900]\ttraining's rmse: 2.17903\tvalid_1's rmse: 2.37349\n",
      "[2950]\ttraining's rmse: 2.1771\tvalid_1's rmse: 2.3731\n",
      "[3000]\ttraining's rmse: 2.1749\tvalid_1's rmse: 2.37259\n",
      "[3050]\ttraining's rmse: 2.1724\tvalid_1's rmse: 2.37206\n",
      "[3100]\ttraining's rmse: 2.16977\tvalid_1's rmse: 2.37132\n",
      "[3150]\ttraining's rmse: 2.16782\tvalid_1's rmse: 2.37086\n",
      "[3200]\ttraining's rmse: 2.16549\tvalid_1's rmse: 2.37041\n",
      "[3250]\ttraining's rmse: 2.16337\tvalid_1's rmse: 2.37004\n",
      "[3300]\ttraining's rmse: 2.1611\tvalid_1's rmse: 2.36954\n",
      "[3350]\ttraining's rmse: 2.15855\tvalid_1's rmse: 2.36898\n",
      "[3400]\ttraining's rmse: 2.15646\tvalid_1's rmse: 2.36861\n",
      "[3450]\ttraining's rmse: 2.15397\tvalid_1's rmse: 2.36801\n",
      "[3500]\ttraining's rmse: 2.1517\tvalid_1's rmse: 2.36744\n",
      "[3550]\ttraining's rmse: 2.14921\tvalid_1's rmse: 2.36681\n",
      "[3600]\ttraining's rmse: 2.14676\tvalid_1's rmse: 2.36612\n",
      "[3650]\ttraining's rmse: 2.14444\tvalid_1's rmse: 2.36568\n",
      "[3700]\ttraining's rmse: 2.14244\tvalid_1's rmse: 2.36522\n",
      "[3750]\ttraining's rmse: 2.14053\tvalid_1's rmse: 2.3648\n",
      "[3800]\ttraining's rmse: 2.13799\tvalid_1's rmse: 2.36406\n",
      "[3850]\ttraining's rmse: 2.13611\tvalid_1's rmse: 2.36375\n",
      "[3900]\ttraining's rmse: 2.13366\tvalid_1's rmse: 2.36303\n",
      "[3950]\ttraining's rmse: 2.13187\tvalid_1's rmse: 2.36256\n",
      "[4000]\ttraining's rmse: 2.13033\tvalid_1's rmse: 2.36232\n",
      "[4050]\ttraining's rmse: 2.12808\tvalid_1's rmse: 2.36174\n",
      "[4100]\ttraining's rmse: 2.12614\tvalid_1's rmse: 2.36137\n",
      "[4150]\ttraining's rmse: 2.1242\tvalid_1's rmse: 2.3612\n",
      "[4200]\ttraining's rmse: 2.12239\tvalid_1's rmse: 2.36089\n",
      "[4250]\ttraining's rmse: 2.12057\tvalid_1's rmse: 2.36053\n",
      "[4300]\ttraining's rmse: 2.11866\tvalid_1's rmse: 2.36022\n",
      "[4350]\ttraining's rmse: 2.11709\tvalid_1's rmse: 2.36002\n",
      "[4400]\ttraining's rmse: 2.11512\tvalid_1's rmse: 2.3597\n",
      "[4450]\ttraining's rmse: 2.11327\tvalid_1's rmse: 2.35943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500]\ttraining's rmse: 2.1112\tvalid_1's rmse: 2.359\n",
      "[4550]\ttraining's rmse: 2.10953\tvalid_1's rmse: 2.35863\n",
      "[4600]\ttraining's rmse: 2.10781\tvalid_1's rmse: 2.35834\n",
      "[4650]\ttraining's rmse: 2.1061\tvalid_1's rmse: 2.35791\n",
      "[4700]\ttraining's rmse: 2.10421\tvalid_1's rmse: 2.3575\n",
      "[4750]\ttraining's rmse: 2.10242\tvalid_1's rmse: 2.35727\n",
      "[4800]\ttraining's rmse: 2.10064\tvalid_1's rmse: 2.35696\n",
      "[4850]\ttraining's rmse: 2.09898\tvalid_1's rmse: 2.35658\n",
      "[4900]\ttraining's rmse: 2.09717\tvalid_1's rmse: 2.35627\n",
      "[4950]\ttraining's rmse: 2.09557\tvalid_1's rmse: 2.35599\n",
      "[5000]\ttraining's rmse: 2.09372\tvalid_1's rmse: 2.35583\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 2.09372\tvalid_1's rmse: 2.35583\n",
      "Wall time: 16h 59min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folds = 3\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pickle\n",
    "\n",
    "for state in states:\n",
    "    \n",
    "    seed = state\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "    for idx, (train_index, test_index) in enumerate(skf.split(df_train.index,df_train['volume'])):\n",
    "        print(idx)\n",
    "        x_train = df_train.iloc[train_index].drop(['day','volume'], axis =1)\n",
    "        y_train = df_train.iloc[train_index]['volume']\n",
    "        x_valid = df_train.iloc[test_index].drop(['day','volume'], axis =1)\n",
    "        y_valid = df_train.iloc[test_index]['volume'] \n",
    "\n",
    "        # Modeling\n",
    "        lgb_train = lgb.Dataset(x_train, y_train,categorical_feature=cat_cols)\n",
    "        lgb_eval = lgb.Dataset(x_valid, y_valid,categorical_feature=cat_cols)\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "    #                     num_boost_round=1000, \n",
    "                        valid_sets=(lgb_train, lgb_eval),\n",
    "                        early_stopping_rounds= 50,#100,\n",
    "                        verbose_eval=50) #100)\n",
    "\n",
    "        pickle.dump(gbm,open( \"20200505_model_%s_best_seed%s.pkl\"%(idx,seed), \"wb\" ))\n",
    "\n",
    "        del gbm\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "243\n",
      "1914\n",
      "rolling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yseon\\Anaconda3\\envs\\M5\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n",
      "rolling\n",
      "mean\n",
      "1916\n",
      "rolling\n",
      "mean\n",
      "1917\n",
      "rolling\n",
      "mean\n",
      "1918\n",
      "rolling\n",
      "mean\n",
      "1919\n",
      "rolling\n",
      "mean\n",
      "1920\n",
      "rolling\n",
      "mean\n",
      "1921\n",
      "rolling\n",
      "mean\n",
      "1922\n",
      "rolling\n",
      "mean\n",
      "1923\n",
      "rolling\n",
      "mean\n",
      "1924\n",
      "rolling\n",
      "mean\n",
      "1925\n",
      "rolling\n",
      "mean\n",
      "1926\n",
      "rolling\n",
      "mean\n",
      "1927\n",
      "rolling\n",
      "mean\n",
      "1928\n",
      "rolling\n",
      "mean\n",
      "1929\n",
      "rolling\n",
      "mean\n",
      "1930\n",
      "rolling\n",
      "mean\n",
      "1931\n",
      "rolling\n",
      "mean\n",
      "1932\n",
      "rolling\n",
      "mean\n",
      "1933\n",
      "rolling\n",
      "mean\n",
      "1934\n",
      "rolling\n",
      "mean\n",
      "1935\n",
      "rolling\n",
      "mean\n",
      "1936\n",
      "rolling\n",
      "mean\n",
      "1937\n",
      "rolling\n",
      "mean\n",
      "1938\n",
      "rolling\n",
      "mean\n",
      "1939\n",
      "rolling\n",
      "mean\n",
      "1940\n",
      "rolling\n",
      "mean\n",
      "1941\n",
      "rolling\n",
      "mean\n",
      "498\n",
      "1914\n",
      "rolling\n",
      "mean\n",
      "1915\n",
      "rolling\n",
      "mean\n",
      "1916\n",
      "rolling\n",
      "mean\n",
      "1917\n",
      "rolling\n",
      "mean\n",
      "1918\n",
      "rolling\n",
      "mean\n",
      "1919\n",
      "rolling\n",
      "mean\n",
      "1920\n",
      "rolling\n",
      "mean\n",
      "1921\n",
      "rolling\n",
      "mean\n",
      "1922\n",
      "rolling\n",
      "mean\n",
      "1923\n",
      "rolling\n",
      "mean\n",
      "1924\n",
      "rolling\n",
      "mean\n",
      "1925\n",
      "rolling\n",
      "mean\n",
      "1926\n",
      "rolling\n",
      "mean\n",
      "1927\n",
      "rolling\n",
      "mean\n",
      "1928\n",
      "rolling\n",
      "mean\n",
      "1929\n",
      "rolling\n",
      "mean\n",
      "1930\n",
      "rolling\n",
      "mean\n",
      "1931\n",
      "rolling\n",
      "mean\n",
      "1932\n",
      "rolling\n",
      "mean\n",
      "1933\n",
      "rolling\n",
      "mean\n",
      "1934\n",
      "rolling\n",
      "mean\n",
      "1935\n",
      "rolling\n",
      "mean\n",
      "1936\n",
      "rolling\n",
      "mean\n",
      "1937\n",
      "rolling\n",
      "mean\n",
      "1938\n",
      "rolling\n",
      "mean\n",
      "1939\n",
      "rolling\n",
      "mean\n",
      "1940\n",
      "rolling\n",
      "mean\n",
      "1941\n",
      "rolling\n",
      "mean\n",
      "45\n",
      "1914\n",
      "rolling\n",
      "mean\n",
      "1915\n",
      "rolling\n",
      "mean\n",
      "1916\n",
      "rolling\n",
      "mean\n",
      "1917\n",
      "rolling\n",
      "mean\n",
      "1918\n",
      "rolling\n",
      "mean\n",
      "1919\n",
      "rolling\n",
      "mean\n",
      "1920\n",
      "rolling\n",
      "mean\n",
      "1921\n",
      "rolling\n",
      "mean\n",
      "1922\n",
      "rolling\n",
      "mean\n",
      "1923\n",
      "rolling\n",
      "mean\n",
      "1924\n",
      "rolling\n",
      "mean\n",
      "1925\n",
      "rolling\n",
      "mean\n",
      "1926\n",
      "rolling\n",
      "mean\n",
      "1927\n",
      "rolling\n",
      "mean\n",
      "1928\n",
      "rolling\n",
      "mean\n",
      "1929\n",
      "rolling\n",
      "mean\n",
      "1930\n",
      "rolling\n",
      "mean\n",
      "1931\n",
      "rolling\n",
      "mean\n",
      "1932\n",
      "rolling\n",
      "mean\n",
      "1933\n",
      "rolling\n",
      "mean\n",
      "1934\n",
      "rolling\n",
      "mean\n",
      "1935\n",
      "rolling\n",
      "mean\n",
      "1936\n",
      "rolling\n",
      "mean\n",
      "1937\n",
      "rolling\n",
      "mean\n",
      "1938\n",
      "rolling\n",
      "mean\n",
      "1939\n",
      "rolling\n",
      "mean\n",
      "1940\n",
      "rolling\n",
      "mean\n",
      "1941\n",
      "rolling\n",
      "mean\n",
      "32\n",
      "1914\n",
      "rolling\n",
      "mean\n",
      "1915\n",
      "rolling\n",
      "mean\n",
      "1916\n",
      "rolling\n",
      "mean\n",
      "1917\n",
      "rolling\n",
      "mean\n",
      "1918\n",
      "rolling\n",
      "mean\n",
      "1919\n",
      "rolling\n",
      "mean\n",
      "1920\n",
      "rolling\n",
      "mean\n",
      "1921\n",
      "rolling\n",
      "mean\n",
      "1922\n",
      "rolling\n",
      "mean\n",
      "1923\n",
      "rolling\n",
      "mean\n",
      "1924\n",
      "rolling\n",
      "mean\n",
      "1925\n",
      "rolling\n",
      "mean\n",
      "1926\n",
      "rolling\n",
      "mean\n",
      "1927\n",
      "rolling\n",
      "mean\n",
      "1928\n",
      "rolling\n",
      "mean\n",
      "1929\n",
      "rolling\n",
      "mean\n",
      "1930\n",
      "rolling\n",
      "mean\n",
      "1931\n",
      "rolling\n",
      "mean\n",
      "1932\n",
      "rolling\n",
      "mean\n",
      "1933\n",
      "rolling\n",
      "mean\n",
      "1934\n",
      "rolling\n",
      "mean\n",
      "1935\n",
      "rolling\n",
      "mean\n",
      "1936\n",
      "rolling\n",
      "mean\n",
      "1937\n",
      "rolling\n",
      "mean\n",
      "1938\n",
      "rolling\n",
      "mean\n",
      "1939\n",
      "rolling\n",
      "mean\n",
      "1940\n",
      "rolling\n",
      "mean\n",
      "1941\n",
      "rolling\n",
      "mean\n",
      "Wall time: 2h 45min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test['id'] = train_id\n",
    "max_lag  = 120\n",
    "trn_lst = 1913\n",
    "for state in states:\n",
    "    print(state)\n",
    "    if state == 999: continue\n",
    "    #df_test 값이 Reset이 안되어 있다 !!!!!  - df test를 유지하는 것으로 하자\n",
    "    tmp_test = df_test.copy() # 추가 !!\n",
    "    for tdelta in range(1, 29):\n",
    "        f_day = trn_lst+tdelta\n",
    "        print(f_day)\n",
    "        days = [f\"d_{i}\" for i in range(trn_lst-max_lag+tdelta,f_day+1)]\n",
    "        tst = tmp_test[tmp_test['day'].isin(days)]\n",
    "\n",
    "        print(\"rolling\")\n",
    "        tst['volume_7'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(7)\n",
    "        tst['volume_28'] = tst[['id','volume']].groupby(\"id\")['volume'].shift(28)\n",
    "\n",
    "        print(\"mean\")\n",
    "        tst['rmean_7_7'] = tst[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(7).mean())\n",
    "        tst['rmean_7_28'] = tst[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(28).mean())\n",
    "        tst['rmean_7_50'] = tst[['id','volume_7']].groupby(\"id\")['volume_7'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "        tst['rmean_28_7'] = tst[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(7).mean())\n",
    "        tst['rmean_28_28'] = tst[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(28).mean())\n",
    "        tst['rmean_28_50'] = tst[['id','volume_28']].groupby(\"id\")['volume_28'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "\n",
    "        tst = tst[tst['day'] == \"d_%s\"%(f_day)]\n",
    "        t_id,t_volume,t_day = tst['id'],tst['volume'],tst['day']\n",
    "        tst = tst.drop(['id','volume','day'], axis =1)\n",
    "\n",
    "        # Crossvalidation \n",
    "        for idx in range(folds):\n",
    "            gbm = pickle.load(open( \"20200505_model_%s_best_seed%s.pkl\"%(idx,state), \"rb\" ))\n",
    "            tmp_test.loc[tmp_test.day==\"d_%s\"%(f_day),'volume'] += 1.028*gbm.predict(tst) / folds\n",
    "    \n",
    "    cols = [f\"d_{i}\" for i in range(1914,1942)]\n",
    "\n",
    "    sub = tmp_test[tmp_test['day'].isin(cols)].loc[:,['id','volume']]\n",
    "    sub['F']= [f\"F{rank}\" for rank in sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "    sub = sub.set_index([\"id\", \"F\" ]).unstack()[\"volume\"].reset_index()\n",
    "    sub.sort_values(\"id\", inplace = True)\n",
    "    sub.reset_index(drop=True, inplace = True)                                                   \n",
    "    sub =sub[['id']+[\"F%s\"% x for x in range(1,29)]]\n",
    "\n",
    "    sub = sub.fillna(0)\n",
    "\n",
    "    sub2 = sub.copy()\n",
    "    sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "    sub.to_csv(\"submission_20200506_0_seed%s.csv\"%state,index=False)\n",
    "    \n",
    "    del tmp_test\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [f\"d_{i}\" for i in range(1914,1942)]\n",
    "\n",
    "# sub = df_test[df_test['day'].isin(cols)].loc[:,['id','volume']]\n",
    "# sub['F']= [f\"F{rank}\" for rank in sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "# sub = sub.set_index([\"id\", \"F\" ]).unstack()[\"volume\"].reset_index()\n",
    "# sub.sort_values(\"id\", inplace = True)\n",
    "# sub.reset_index(drop=True, inplace = True)                                                   \n",
    "# sub =sub[['id']+[\"F%s\"% x for x in range(1,29)]]\n",
    "\n",
    "# sub = sub.fillna(0)\n",
    "\n",
    "# sub2 = sub.copy()\n",
    "# sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "# sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "# sub.to_csv(\"submission_20200505_0.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F\n",
       "id     0\n",
       "F1     0\n",
       "F2     0\n",
       "F3     0\n",
       "F4     0\n",
       "F5     0\n",
       "F6     0\n",
       "F7     0\n",
       "F8     0\n",
       "F9     0\n",
       "F10    0\n",
       "F11    0\n",
       "F12    0\n",
       "F13    0\n",
       "F14    0\n",
       "F15    0\n",
       "F16    0\n",
       "F17    0\n",
       "F18    0\n",
       "F19    0\n",
       "F20    0\n",
       "F21    0\n",
       "F22    0\n",
       "F23    0\n",
       "F24    0\n",
       "F25    0\n",
       "F26    0\n",
       "F27    0\n",
       "F28    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame, tst):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "#         train_id = train_id\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "        self.tst = tst\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        scale = np.where(scale != 0 , scale, 1)\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "\n",
    "        return np.mean(all_scores)\n",
    "    \n",
    "    \n",
    "class WRMSSEForLightGBM(WRMSSEEvaluator):\n",
    "\n",
    "    def feval(self, preds, dtrain):\n",
    "#         print(preds.shape, self.tst.shape)\n",
    "#         tst= self.df[self.df['day'].isin(valid_target_columns)]\n",
    "#         tst['id'] = train_id.loc[tst.index]\n",
    "        tmp = self.tst.copy()\n",
    "        tmp['preds'] = preds\n",
    "        tmp=  tmp.set_index(['id',\"day\"]).unstack()[\"preds\"].reset_index()\n",
    "        tmp =  tmp.fillna(0)\n",
    "\n",
    "        val = pd.DataFrame()\n",
    "        val['id'] = self.train_df['id']\n",
    "        pred = pd.merge(val, tmp, how = 'left')\n",
    "        pred = pred.fillna(0)\n",
    "#         print(pred.columns)\n",
    "#         print(self.valid_target_columns)\n",
    "        pred = pred.loc[:,self.valid_target_columns ]\n",
    "#         cv_scores.append(evaluator.score(pred))\n",
    "#         preds = preds.reshape(self.valid_df[self.valid_target_columns].shape)\n",
    "        score = self.score(pred)\n",
    "        return 'WRMSSE', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f867ecd58d004208b8b00c13d6c5b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "999\n",
      "999 0 WRMSSE : 0.5308819776047743\n",
      "999 1 WRMSSE : 0.5182642052738963\n",
      "999 2 WRMSSE : 0.5358917656293912\n",
      "999 CV score 0.5283459828360205\n",
      "243\n",
      "243 0 WRMSSE : 0.5339033635461353\n",
      "243 1 WRMSSE : 0.5383453261087875\n",
      "243 2 WRMSSE : 0.5250614624912074\n",
      "243 CV score 0.530391350109032\n",
      "498\n",
      "498 0 WRMSSE : 0.5249241083925535\n",
      "498 1 WRMSSE : 0.5381342798502133\n",
      "498 2 WRMSSE : 0.5370496208798273\n",
      "498 CV score 0.5313840121974207\n",
      "45\n",
      "45 0 WRMSSE : 0.531560943707681\n",
      "45 1 WRMSSE : 0.5402675316632751\n",
      "45 2 WRMSSE : 0.5360487959897174\n",
      "45 CV score 0.532527781761455\n",
      "32\n",
      "32 0 WRMSSE : 0.5329068294848568\n",
      "32 1 WRMSSE : 0.5359323958815846\n",
      "32 2 WRMSSE : 0.5299491452827523\n",
      "32 CV score 0.5326081167857769\n"
     ]
    }
   ],
   "source": [
    "# 마지막 28일 데이터로 검증\n",
    "cv_scores =[]\n",
    "cols = [\"d_%s\"%x for x in range(1,1914)]\n",
    "days = list(df_train['day'].unique()) # 날짜 섞기\n",
    "cols = [x for x in cols if x in days]\n",
    "\n",
    "d_valid=[f\"d_{i}\" for i in range(1913-28,1914)]\n",
    "d_train = [x for x in cols if x not in d_valid]\n",
    "tmp = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "tmp_train = pd.concat([tmp.iloc[:,:6],tmp.loc[:, d_train]], axis =1)\n",
    "tmp_valid = tmp.loc[:, d_valid]\n",
    "#     evaluator = WRMSSEEvaluator(tmp_train, tmp_valid, calendar, price)\n",
    "tst= df_valid[df_valid['day'].isin(d_valid)]\n",
    "tst['id'] = train_id.loc[tst.index]\n",
    "\n",
    "\n",
    "evaluator = WRMSSEEvaluator(tmp_train, tmp_valid, calendar, price,tst)\n",
    "for state in states:\n",
    "    print(state)\n",
    "    for idx in range(folds):\n",
    "\n",
    "        # gbm = pickle.dump(gbm,open( \"20200502_model_%s_r1_5000.pkl\"%idx, \"wb\" ))\n",
    "        gbm = pickle.load(open( \"20200505_model_%s_best_seed%s.pkl\"%(idx,state), \"rb\" ))\n",
    "        preds= gbm.predict(df_valid.drop(['day','volume'], axis =1))\n",
    "        tst= df_valid[df_valid['day'].isin(d_valid)]\n",
    "        tst['id'] = train_id.loc[tst.index]\n",
    "        tst['preds'] = preds\n",
    "        tst= tst.set_index(['id',\"day\"]).unstack()[\"preds\"].reset_index()\n",
    "        tst = tst.fillna(0)\n",
    "\n",
    "        val = pd.DataFrame()\n",
    "        val['id'] = tmp_train['id']\n",
    "        pred = pd.merge(val,tst, how = 'left')\n",
    "        pred = pred.fillna(0)\n",
    "        pred = pred.loc[:,d_valid]\n",
    "        cv_scores.append(evaluator.score(pred))\n",
    "        print(state, idx,\"WRMSSE :\", evaluator.score(pred))\n",
    "    print(state, \"CV score\" , np.mean(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
