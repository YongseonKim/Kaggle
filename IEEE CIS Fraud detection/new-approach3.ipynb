{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_identity.csv', 'test_transaction.csv', 'sample_submission.csv', 'train_transaction.csv', 'train_identity.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"../input/ieee-fraud-detection/\"))\n",
    "pd.options.display.max_rows = 99\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column Type을 다시 지정해주자 \n",
    "#https://www.kaggle.com/mhviraf/reducing-memory-size-an-alternative\n",
    "# NaN 이 포함된 int value도 float으로 되어 있고, 이를 Reduce mem usuage 사용하면 데이터 손실이 발생하기도 한다.V\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    del_cols = ['TransactionAmt','dist1', 'dist2', 'C1', 'C2', 'C4', 'C6', 'C7', 'C8', 'C10', 'C11', 'C12', 'C13', 'D8', 'D9', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V150', 'V159', 'V164', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V332', 'V334', 'V335', 'V336']\n",
    "    \n",
    "    cols = [x for x in list(df.columns) if x not in del_cols]# 데이터가 변경되는 컬럼은 제외 81개\n",
    "    for col in tqdm(cols):\n",
    "#         if col in cols : continue \n",
    "        col_type = df[col].dtype\n",
    "        #print(col_type)\n",
    "#         if str(col_type)[:4]== 'date' : continue\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('../input/datas6/train.pkl')\n",
    "# test = pd.read_pickle('../input/datas6/test.pkl')\n",
    "# gc.collect()\n",
    "\n",
    "X_train = pd.read_pickle('../input/datas7/X_train.pkl')\n",
    "X_test = pd.read_pickle('../input/datas7/X_test.pkl')\n",
    "Y_train = pd.read_pickle('../input/datas7/Y_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 바로 모델링\n",
    "# X_train = train.sort_values('TransactionDT').drop(['isFraud','TransactionDT','date'],axis =1)\n",
    "# Y_train = train.sort_values('TransactionDT')['isFraud']\n",
    "# X_test = test.sort_values('TransactionDT').drop(['TransactionDT','date'],axis =1 )\n",
    "# train, test = [], [] \n",
    "# del train, test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_columns = list(X_train.select_dtypes(include=['float16','float32','float64','int8','int16','int64']).columns)\n",
    "# numerical_columns=list(X_train[numerical_columns].isnull().sum()[X_train[numerical_columns].isnull().sum()>0].index)\n",
    "# #list(test.select_dtypes(exclude=['object']).columns)\n",
    "# print(X_train.shape)\n",
    "# X_train[numerical_columns] =X_train[numerical_columns].fillna(-999)\n",
    "# X_test[numerical_columns] =X_test[numerical_columns].fillna(-999)\n",
    "# print(\"filling numerical columns null values done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features = ['ProductCD','M4',\n",
    "#                         'card1','card2','card3','card4','card5','card6',\n",
    "#                         'addr1','addr2','dist1','dist2',\n",
    "#                         'P_emaildomain','R_emaildomain',\n",
    "#                        ]\n",
    "# categorical_features = list(set(categorical_features+X_train.select_dtypes(include='category').columns.tolist()+X_test.select_dtypes(include='category').columns.tolist()))\n",
    "# # categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features +=['M1_9', 'uid1', 'uid2', 'uid3', 'uid4', 'uid6', 'uid7', 'uid8', 'uid9', 'uid10', 'V1_11', 'V35_52', 'V75_94',\n",
    "#                          'card1_addr1','id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_13',\n",
    "#                          'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'id_32', 'id_03_04', 'id_05_06',\n",
    "#                      'id_07_08', 'id_35_38', 'id_17_20_ip1', 'id_21_22_ip2', 'id_24_26_ip3','TransactionAmt_grouping', 'dayofyear_block', 'hours_block',\n",
    "#                     ]\n",
    "# categorical_features = list(set(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for col in categorical_features :\n",
    "#     try :\n",
    "#         X_train[col] = X_train[col].cat.add_categories(-999).fillna(-999)\n",
    "#         X_test[col] = X_test[col].cat.add_categories(-999).fillna(-999)\n",
    "#     except :\n",
    "#         X_train[col] = X_train[col].fillna(-999)\n",
    "#         X_test[col] = X_test[col].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical feature로 변경\n",
    "# X_train[categorical_features] = X_train[categorical_features].astype('category')\n",
    "# X_test[categorical_features] = X_test[categorical_features].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# for f in tqdm(X_train.select_dtypes(include='category').columns.tolist() + X_train.select_dtypes(include='object').columns.tolist()):\n",
    "# #     print(f)\n",
    "#     lbl = preprocessing.LabelEncoder()\n",
    "#     lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "#     X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "#     X_test[f] = lbl.transform(list(X_test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['ProductCD','M4',\n",
    "                        'card1','card2','card3','card4','card5','card6',\n",
    "                        'addr1','addr2','dist1','dist2',\n",
    "                        'P_emaildomain','R_emaildomain',\n",
    "                       ]\n",
    "categorical_features +=['M1_9', 'uid1', 'uid2', 'uid3', 'uid4', 'uid6', 'uid7', 'uid8', 'uid9', 'uid10', 'V1_11', 'V35_52', 'V75_94',\n",
    "                         'card1_addr1','id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_13',\n",
    "                         'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'id_32', 'id_03_04', 'id_05_06',\n",
    "                     'id_07_08', 'id_35_38', 'id_17_20_ip1', 'id_21_22_ip2', 'id_24_26_ip3','TransactionAmt_grouping', 'dayofyear_block', 'hours_block',\n",
    "                   \n",
    "                       ]\n",
    "categorical_features = list(set(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Freq encoding\n",
    "# 각 컬럼의 값의 전체에서 얼만큼 빈도수를 가지는지 카운트하여 넣어줌 -> C,D는 크게 의미 없을 듯?\n",
    "i_cols = [#'card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "         # 'P_emaildomain', \n",
    "          'R_emaildomain'\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([X_train[[col]], X_test[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()    # Null 값도 계산하게하기\n",
    "    X_train[col+'_fq_enc'] = X_train[col].map(fq_encode)\n",
    "    X_test[col+'_fq_enc']  = X_test[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['day_block'] = X_train['day'].map(lambda x: 1 if x>27 else 0)\n",
    "X_test['day_block'] = X_test['day'].map(lambda x: 1 if x>27 else 0)\n",
    "categorical_features.append('day_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['C0'] = np.where(X_train['C1']==0,X_train['C2'],X_train['C1'])\n",
    "# X_train['Amt_C0'] = X_train['TransactionAmt']/X_train['C0']\n",
    "# X_train['Amt_C0'] = X_train['Amt_C0'].fillna(X_train['TransactionAmt'])\n",
    "# X_train['Amt_C0_int'] = X_train['Amt_C0'].astype('int')\n",
    "# X_train['Amt_C0_dec'] = (X_train['Amt_C0'] -X_train['Amt_C0_int'])\n",
    "# # X_train['Amt_C0_dec']\n",
    "\n",
    "# X_test['C0'] = np.where(X_test['C1']==0,X_test['C2'],X_test['C1'])\n",
    "# X_test['C0'] = np.where(X_test['C0']==0,1,X_test['C0'])\n",
    "# X_test['Amt_C0'] = X_test['TransactionAmt']/X_test['C0']\n",
    "# X_test['Amt_C0'] = X_test['Amt_C0'].fillna(X_test['TransactionAmt'])\n",
    "# X_test['Amt_C0_int'] = X_test['Amt_C0'].astype('int')\n",
    "# X_test['Amt_C0_dec'] = (X_test['Amt_C0'] -X_test['Amt_C0_int'])\n",
    "# # X_test[X_test['Amt_C0']==np.inf]['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['D15','D10','D2','D14','D12','D8']:\n",
    "    X_train[col] = np.where(X_train[col]==-999,0,X_train[col])\n",
    "    X_test[col] = np.where(X_test[col]==-999,0,X_test[col])\n",
    "    \n",
    "X_train['D16'] = X_train['D15']\n",
    "X_train['D16'] = np.where(X_train['D16']>0, X_train['D16'],X_train['D10'])\n",
    "X_train['D16'] = np.where(X_train['D16']>0, X_train['D16'],X_train['D2'])\n",
    "X_train['D16'] = np.where(X_train['D16']>0, X_train['D16'],X_train['D14'])\n",
    "X_train['D16'] = np.where(X_train['D16']>0, X_train['D16'],X_train['D12'])\n",
    "X_train['D16'] = np.where(X_train['D16']>0, X_train['D16'],X_train['D8'])\n",
    "# X_train['D16'].values[:100]\n",
    "\n",
    "X_test['D16'] = X_test['D15']\n",
    "X_test['D16'] = np.where(X_test['D16']>0, X_test['D16'],X_test['D10'])\n",
    "X_test['D16'] = np.where(X_test['D16']>0, X_test['D16'],X_test['D2'])\n",
    "X_test['D16'] = np.where(X_test['D16']>0, X_test['D16'],X_test['D14'])\n",
    "X_test['D16'] = np.where(X_test['D16']>0, X_test['D16'],X_test['D12'])\n",
    "X_test['D16'] = np.where(X_test['D16']>0, X_test['D16'],X_test['D8'])\n",
    "\n",
    "for col in ['D15','D10','D2','D14','D12','D8']:\n",
    "    X_train[col] = X_train[col].fillna(-999)\n",
    "    X_test[col] = X_test[col].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1097231/1097231 [00:00<00:00, 1414507.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# 시작점부터의 일수 계산\n",
    "fromstart = []\n",
    "tmp = 0\n",
    "x =1\n",
    "dayofyears = list(X_train['dayofyear'])+list(X_test['dayofyear']) \n",
    "for i in tqdm(range(len(dayofyears))) :\n",
    "#     print(tmp, X_train['dayofyear'].iloc[i])\n",
    "    if fromstart ==[] : fromstart.append(x)\n",
    "    else :\n",
    "        if tmp == dayofyears[i] : fromstart.append(x)\n",
    "        else : \n",
    "            x += 1\n",
    "            fromstart.append(x)\n",
    "    tmp = dayofyears[i]\n",
    "\n",
    "X_train['fromstart'] = fromstart[:X_train.shape[0]]\n",
    "X_test['fromstart'] = fromstart[X_train.shape[0]:]\n",
    "\n",
    "\n",
    "X_train['grouping'] = (X_train['D16']- X_train['fromstart'])/10\n",
    "X_train['grouping'] = np.round(X_train['grouping'])\n",
    "\n",
    "X_test['grouping'] = (X_test['D16']- X_test['fromstart'])/10\n",
    "X_test['grouping'] = np.round(X_test['grouping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P_emaildomain__C2', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'card5__P_emaildomain', 'uid10__grouping']\n"
     ]
    }
   ],
   "source": [
    "# Some arbitrary features interaction\n",
    "features = [ 'P_emaildomain__C2','D11__DeviceInfo','DeviceInfo__P_emaildomain','card5__P_emaildomain','uid10__grouping']\n",
    "#[ 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "#                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']\n",
    "# features = features[:6]\n",
    "print(features)\n",
    "from sklearn import preprocessing\n",
    "for feature in features:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    X_train[feature] = X_train[f1].astype(str) + '_' + X_train[f2].astype(str)\n",
    "    X_test[feature] = X_test[f1].astype(str) + '_' + X_test[f2].astype(str)\n",
    "    categorical_features.append(feature)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[feature].values) + list(X_test[feature].values))\n",
    "    X_train[feature] = lbl.transform(list(X_train[feature].values))\n",
    "    X_test[feature] = lbl.transform(list(X_test[feature].values))\n",
    "    \n",
    "    \n",
    "\n",
    "# for f in tqdm(X_train.select_dtypes(include='category').columns.tolist() + X_train.select_dtypes(include='object').columns.tolist()):\n",
    "# #     print(f)\n",
    "#     lbl = preprocessing.LabelEncoder()\n",
    "#     lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "#     X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "#     X_test[f] = lbl.transform(list(X_test[f].values))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['uid10__grouping_count_full'] = X_train['uid10__grouping'].map(pd.concat([X_train['uid10__grouping'], X_test['uid10__grouping']], ignore_index=True).value_counts(dropna=False))\n",
    "X_test['uid10__grouping_count_full'] = X_test['uid10__grouping'].map(pd.concat([X_train['uid10__grouping'], X_test['uid10__grouping']], ignore_index=True).value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# cardid 기준으로 최근 5일간 거래량 통계\n",
    "X_train['uid10__grouping'] = X_train['uid10__grouping'].astype('str') \n",
    "X_train['count_last_uid10__grouping'] = X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).count())\n",
    "X_train['mean_last_uid10__grouping'] = X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "X_train['min_last_uid10__grouping'] = X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).min())\n",
    "X_train['max_last_uid10__grouping'] = X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).max())\n",
    "X_train['std_last_uid10__grouping'] = X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "\n",
    "X_test['uid10__grouping'] = X_test['uid10__grouping'].astype('str') \n",
    "X_test['count_last_uid10__grouping'] = X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).count())\n",
    "X_test['mean_last_uid10__grouping'] = X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "X_test['min_last_uid10__grouping'] = X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).min())\n",
    "X_test['max_last_uid10__grouping'] = X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).max())\n",
    "X_test['std_last_uid10__grouping'] = X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "\n",
    "#최근 10일 평균 대비 큰지 작은지 \n",
    "X_train['trans_mean_last_uid10__grouping'] = X_train['TransactionAmt'] / X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "X_train['trans_std_last_uid10__grouping'] = X_train['TransactionAmt'] / X_train.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "X_test['trans_mean_last_uid10__grouping'] = X_test['TransactionAmt'] / X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "X_test['trans_std_last_uid10__grouping'] = X_test['TransactionAmt'] / X_test.groupby('uid10__grouping')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "\n",
    "\n",
    "X_train['uid10__grouping'] = X_train['uid10__grouping'].astype('float32') \n",
    "X_test['uid10__grouping'] = X_test['uid10__grouping'].astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in tqdm(main_columns):  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "                \n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "            del temp_df\n",
    "            gc.collect()\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################### D Columns\n",
    "# # From columns description we know that\n",
    "# # D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "# # 1. I can't imagine normal negative timedelta values (Let's clip Values)\n",
    "# # 2. Normalize (Min-Max, Standard score) All D columns, except D1,D2,D9\n",
    "# # 3. Do some aggregations based on uIDs\n",
    "# # 4. Freaquency encoding\n",
    "# # 5. D1,D2 are clipped by max train_df values (let's scale it)\n",
    "# i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "# i_cols.remove('D7')\n",
    "# #i_cols += ['TransactionAmt'] ## 커널이 죽는다\n",
    "# # i_cols += ['isFraud']\n",
    "# uids = ['uid10__grouping']  #'card1','card2','card3','card5' 추가하면 커널 죽음 #,'uid9','uid7'\n",
    "# aggregations = ['mean','std']\n",
    "\n",
    "# ####### uIDs aggregations\n",
    "# X_train, X_test = uid_aggregation(X_train, X_test, i_cols, uids, aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_cols = ['TransactionAmt']\n",
    "# uids = ['uid10__grouping']# ['card1','uid1','uid4','uid7','uid9']  \n",
    "# aggregations = ['mean','std']\n",
    "\n",
    "# ####### uIDs aggregations\n",
    "# X_train, X_test = uid_aggregation(X_train, X_test, i_cols, uids, aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 470/470 [01:16<00:00,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/77537\n",
    "# CV 와 LB 사이의 GAP 줄이기\n",
    "from scipy.stats import ks_2samp\n",
    "list_p_value =[]\n",
    "\n",
    "cols = [x for x in X_train.columns if x not in categorical_features]\n",
    "for i in tqdm(cols):\n",
    "    list_p_value.append(ks_2samp(X_test[i] , X_train[i])[1])\n",
    "\n",
    "Se = pd.Series(list_p_value, index = cols).sort_values() \n",
    "list_discarded = list(Se[Se == 0].index)\n",
    "# len(list_discarded)\n",
    "# if 'dayofyear' in list_discarded :list_discarded.remove('dayofyear')\n",
    "X_train = X_train.drop(list_discarded,axis =1 )\n",
    "X_test = X_test.drop(list_discarded,axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 441)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [07:50<00:00, 235.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V177', 'V121', 'V192', 'V123', 'V124', 'V118', 'V153', 'V180', 'V226', 'V251', 'V262', 'V173', 'V257', 'V229', 'V191', 'V317', 'uid4_D9_std', 'V115', 'V139', 'uid9', 'V211', 'V255', 'V242', 'V186', 'V252', 'V187', 'C6', 'id_11', 'V181', 'V196', 'V184', 'V236', 'V237', 'V110', 'addr2_fq_enc', 'V248', 'V275', 'V297', 'V157', 'V241', 'uid10_D9_std', 'V154', 'V283', 'V172', 'V174', 'uid3_count_full', 'V326', 'V199', 'V247', 'id_05', 'V300', 'C11', 'V218', 'id_29', 'id_10', 'V329', 'V261', 'V243', 'V111', 'V144', 'V213', 'V221', 'V327', 'V333', 'V302', 'V140', 'V325', 'D9_fq_enc', 'V219', 'V197', 'V231', 'TransactionAmt_grouping', 'V112', 'id_04', 'V143', 'V225', 'V210', 'Avg_V134', 'V254', 'D8_fq_enc', 'C14', 'uid4', 'V223', 'V258', 'uid8', 'V125', 'V175', 'V228', 'V244', 'V256', 'V232', 'V270', 'V152', 'V227', 'V305', 'V195', 'uid10_D9_mean', 'V176', 'R_emaildomain_fq_enc', 'V293', 'V158', 'C8', 'V323', 'V179', 'V208', 'V109', 'V245', 'V169', 'V295', 'V147', 'V116', 'V328', 'V149', 'uid4_count_full', 'V182', 'V336', 'V278', 'C2', 'V161', 'V318', 'V224', 'C10_fq_enc', 'V183', 'V339', 'V145', 'V230', 'V235', 'V321', 'Avg_V318', 'V178', 'V114', 'V260', 'V240', 'V288', 'V238', 'V113', 'V185', 'V206', 'V316', 'V188', 'V142', 'V330', 'id_09', 'V201', 'V250', 'V212', 'V301', 'V120', 'card1_addr1', 'V222', 'V190', 'addr1_fq_enc', 'uid3', 'uid6', 'V304', 'V220', 'V171', 'V200', 'uid7', 'V324', 'V168', 'V246', 'V269', 'Avg_V137', 'V162', 'id_03_04', 'card1_count_full', 'V239', 'uid10', 'V233', 'V272', 'V331', 'V271', 'V119', 'V163', 'uid10_count_full', 'V189', 'V194', 'uid2', 'V122', 'V234', 'V282', 'V117', 'V198', 'V259', 'C7_fq_enc', 'V322', 'C10', 'V156', 'V289', 'V253', 'V148', 'V151', 'V249', 'V296', 'V286', 'V141', 'V108', 'V170', 'V193', 'V155', 'V290', 'V303', 'V146', 'hours_block']\n"
     ]
    }
   ],
   "source": [
    "# # Create correlation matrix\n",
    "drop_cols = []\n",
    "for df in tqdm([X_train,X_test]):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    drop_cols += [column for column in upper.columns if any(upper[column] > 0.98)]\n",
    "drop_cols =list(set(drop_cols))\n",
    "print(drop_cols)\n",
    "gc.collect()\n",
    "# if 'dayofyear' in drop_cols :drop_cols.remove('dayofyear')\n",
    "X_train = X_train.drop(drop_cols,axis =1 )\n",
    "X_test = X_test.drop(drop_cols,axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_value_cols = [col for col in X_train.columns if X_train[col].nunique() <= 1]\n",
    "one_value_cols_test = [col for col in X_test.columns if X_test[col].nunique() <= 1]\n",
    "\n",
    "drop_cols = list(set(one_value_cols+one_value_cols_test))\n",
    "X_train = X_train.drop(drop_cols,axis =1 )\n",
    "X_test = X_test.drop(drop_cols,axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_top_value_cols = [col for col in X_train.columns if X_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.98]\n",
    "big_top_value_cols_test = [col for col in X_test.columns if X_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.98]\n",
    "\n",
    "drop_cols = list(set(big_top_value_cols+big_top_value_cols_test))\n",
    "X_train = X_train.drop(drop_cols,axis =1 )\n",
    "X_test = X_test.drop(drop_cols,axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature로 변경\n",
    "# X_train[categorical_features] = X_train[categorical_features].astype('category')\n",
    "# X_test[categorical_features] = X_test[categorical_features].astype('category')\n",
    "# from sklearn import preprocessing\n",
    "# for f in tqdm(X_train.select_dtypes(include='category').columns.tolist() + X_train.select_dtypes(include='object').columns.tolist()):\n",
    "# #     print(f)\n",
    "#     lbl = preprocessing.LabelEncoder()\n",
    "#     lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "#     X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "#     X_test[f] = lbl.transform(list(X_test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.kaggle.com/duykhanh99/hust-lgb-fe-0-9485-lb-newfeature\n",
    "\n",
    "# drop_cols = ['V300','V309','V111','V124','V106','V125','V315','V134','V102','V123','V316','V113',\n",
    "#               'V136','V305','V110','V299','V289','V286','V318','V304','V116','V284','V293',\n",
    "#               'V137','V295','V301','V104','V311','V115','V109','V119','V321','V114','V133','V122','V319',\n",
    "#               'V105','V112','V118','V117','V121','V108','V135','V320','V303','V297','V120',\n",
    "#               'V1','V14','V41','V65','V88', 'V89', 'V107', 'V68', 'V28', 'V27', 'V29', 'V241','V269',\n",
    "#               'V240', 'V325', 'V138', 'V154', 'V153', 'V330', 'V142', 'V195', 'V302', 'V328', 'V327', \n",
    "#               'V198', 'V196', 'V155']\n",
    "# drop_cols = [x for x in drop_cols if x in list(X_train.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 221)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [ x for x in categorical_features if x in list(X_train.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical_features] = X_train[categorical_features].astype('category')\n",
    "X_test[categorical_features] = X_test[categorical_features].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['uid2', 'uid3', 'uid4', 'uid6', 'uid7', 'uid8', 'uid9', 'uid10','uid10__grouping']\n",
    "#for col in cols :\n",
    "#    try :\n",
    "  #      X_train.remove(col)\n",
    "   #     X_test.remove(col)\n",
    "  #  except : continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0:\tlearn: 0.8282966\ttest: 0.7545147\tbest: 0.7545147 (0)\ttotal: 173ms\tremaining: 14m 24s\n",
      "500:\tlearn: 0.9763851\ttest: 0.9392651\tbest: 0.9392651 (500)\ttotal: 1m 16s\tremaining: 11m 27s\n",
      "1000:\tlearn: 0.9806029\ttest: 0.9426043\tbest: 0.9426275 (998)\ttotal: 2m 38s\tremaining: 10m 32s\n",
      "1500:\tlearn: 0.9837750\ttest: 0.9434239\tbest: 0.9438818 (1198)\ttotal: 3m 59s\tremaining: 9m 19s\n",
      "bestTest = 0.9438818097\n",
      "bestIteration = 1198\n",
      "Shrink model to first 1199 iterations.\n",
      "Training end\n",
      "1\n",
      "0:\tlearn: 0.8278483\ttest: 0.7874370\tbest: 0.7874370 (0)\ttotal: 146ms\tremaining: 12m 10s\n",
      "500:\tlearn: 0.9759010\ttest: 0.9372289\tbest: 0.9372373 (499)\ttotal: 1m 18s\tremaining: 11m 41s\n",
      "1000:\tlearn: 0.9800790\ttest: 0.9412803\tbest: 0.9413058 (993)\ttotal: 2m 39s\tremaining: 10m 36s\n",
      "1500:\tlearn: 0.9831250\ttest: 0.9427502\tbest: 0.9428031 (1428)\ttotal: 4m 1s\tremaining: 9m 22s\n",
      "bestTest = 0.9428031445\n",
      "bestIteration = 1428\n",
      "Shrink model to first 1429 iterations.\n",
      "Training end\n",
      "2\n",
      "0:\tlearn: 0.8223577\ttest: 0.7580749\tbest: 0.7580749 (0)\ttotal: 163ms\tremaining: 13m 37s\n",
      "500:\tlearn: 0.9745011\ttest: 0.9650787\tbest: 0.9652591 (482)\ttotal: 1m 15s\tremaining: 11m 18s\n",
      "1000:\tlearn: 0.9789991\ttest: 0.9686754\tbest: 0.9686807 (999)\ttotal: 2m 35s\tremaining: 10m 23s\n",
      "1500:\tlearn: 0.9823167\ttest: 0.9696866\tbest: 0.9697332 (1483)\ttotal: 3m 59s\tremaining: 9m 17s\n",
      "2000:\tlearn: 0.9851868\ttest: 0.9697533\tbest: 0.9697872 (1994)\ttotal: 5m 22s\tremaining: 8m 3s\n",
      "2500:\tlearn: 0.9876190\ttest: 0.9688673\tbest: 0.9698002 (2014)\ttotal: 6m 46s\tremaining: 6m 46s\n",
      "bestTest = 0.9698001742\n",
      "bestIteration = 2014\n",
      "Shrink model to first 2015 iterations.\n",
      "Training end\n",
      "3\n",
      "0:\tlearn: 0.8271048\ttest: 0.7510639\tbest: 0.7510639 (0)\ttotal: 153ms\tremaining: 12m 46s\n",
      "500:\tlearn: 0.9742929\ttest: 0.9609619\tbest: 0.9609619 (500)\ttotal: 1m 16s\tremaining: 11m 24s\n",
      "1000:\tlearn: 0.9787614\ttest: 0.9623676\tbest: 0.9626450 (881)\ttotal: 2m 36s\tremaining: 10m 24s\n",
      "1500:\tlearn: 0.9822614\ttest: 0.9617576\tbest: 0.9627881 (1061)\ttotal: 3m 58s\tremaining: 9m 15s\n",
      "bestTest = 0.962788105\n",
      "bestIteration = 1061\n",
      "Shrink model to first 1062 iterations.\n",
      "Training end\n",
      "4\n",
      "0:\tlearn: 0.8227175\ttest: 0.7647474\tbest: 0.7647474 (0)\ttotal: 147ms\tremaining: 12m 13s\n",
      "500:\tlearn: 0.9743109\ttest: 0.9677098\tbest: 0.9677098 (500)\ttotal: 1m 18s\tremaining: 11m 40s\n",
      "1000:\tlearn: 0.9789511\ttest: 0.9681390\tbest: 0.9685033 (746)\ttotal: 2m 37s\tremaining: 10m 30s\n",
      "bestTest = 0.9685032964\n",
      "bestIteration = 746\n",
      "Shrink model to first 747 iterations.\n",
      "Training end\n",
      "5\n",
      "0:\tlearn: 0.8275924\ttest: 0.7423840\tbest: 0.7423840 (0)\ttotal: 146ms\tremaining: 12m 9s\n",
      "500:\tlearn: 0.9760488\ttest: 0.9401802\tbest: 0.9402050 (495)\ttotal: 1m 16s\tremaining: 11m 26s\n",
      "1000:\tlearn: 0.9803109\ttest: 0.9417635\tbest: 0.9417635 (1000)\ttotal: 2m 36s\tremaining: 10m 23s\n",
      "1500:\tlearn: 0.9834355\ttest: 0.9414594\tbest: 0.9418759 (1036)\ttotal: 3m 58s\tremaining: 9m 15s\n",
      "bestTest = 0.9418759048\n",
      "bestIteration = 1036\n",
      "Shrink model to first 1037 iterations.\n",
      "Training end\n",
      "6\n",
      "0:\tlearn: 0.8267691\ttest: 0.7794454\tbest: 0.7794454 (0)\ttotal: 147ms\tremaining: 12m 14s\n",
      "500:\tlearn: 0.9744401\ttest: 0.9602420\tbest: 0.9602885 (499)\ttotal: 1m 17s\tremaining: 11m 37s\n",
      "1000:\tlearn: 0.9789596\ttest: 0.9612005\tbest: 0.9616814 (840)\ttotal: 2m 37s\tremaining: 10m 29s\n",
      "bestTest = 0.9616814256\n",
      "bestIteration = 840\n",
      "Shrink model to first 841 iterations.\n",
      "Training end\n",
      "7\n",
      "0:\tlearn: 0.8305994\ttest: 0.7911745\tbest: 0.7911745 (0)\ttotal: 154ms\tremaining: 12m 48s\n",
      "500:\tlearn: 0.9743841\ttest: 0.9666457\tbest: 0.9666579 (499)\ttotal: 1m 17s\tremaining: 11m 36s\n",
      "1000:\tlearn: 0.9788250\ttest: 0.9663177\tbest: 0.9668940 (578)\ttotal: 2m 38s\tremaining: 10m 34s\n",
      "bestTest = 0.9668940306\n",
      "bestIteration = 578\n",
      "Shrink model to first 579 iterations.\n",
      "Training end\n",
      "8\n",
      "0:\tlearn: 0.8192701\ttest: 0.7672793\tbest: 0.7672793 (0)\ttotal: 142ms\tremaining: 11m 50s\n",
      "500:\tlearn: 0.9749807\ttest: 0.9594913\tbest: 0.9595445 (498)\ttotal: 1m 15s\tremaining: 11m 20s\n",
      "1000:\tlearn: 0.9796644\ttest: 0.9616946\tbest: 0.9617460 (991)\ttotal: 2m 36s\tremaining: 10m 23s\n",
      "1500:\tlearn: 0.9830176\ttest: 0.9608910\tbest: 0.9621744 (1099)\ttotal: 3m 57s\tremaining: 9m 13s\n",
      "bestTest = 0.9621744156\n",
      "bestIteration = 1099\n",
      "Shrink model to first 1100 iterations.\n",
      "Training end\n",
      "9\n",
      "0:\tlearn: 0.8167504\ttest: 0.6858566\tbest: 0.6858566 (0)\ttotal: 142ms\tremaining: 11m 47s\n",
      "500:\tlearn: 0.9745187\ttest: 0.9471418\tbest: 0.9471902 (499)\ttotal: 1m 16s\tremaining: 11m 27s\n",
      "1000:\tlearn: 0.9794909\ttest: 0.9483566\tbest: 0.9484688 (867)\ttotal: 2m 37s\tremaining: 10m 28s\n",
      "1500:\tlearn: 0.9828016\ttest: 0.9483842\tbest: 0.9487224 (1431)\ttotal: 3m 58s\tremaining: 9m 16s\n",
      "bestTest = 0.9487224221\n",
      "bestIteration = 1431\n",
      "Shrink model to first 1432 iterations.\n",
      "Training end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeds =  79\n",
    "splitcounts = 10\n",
    "LGBM =  False#True\n",
    "# 앞선 Test에서의 결과로 StratifiedKFold를 사용한다. \n",
    "# folds = TimeSeriesSplit(n_splits= splitcounts)\n",
    "folds = KFold(n_splits=splitcounts, random_state = seeds) \n",
    "# folds = StratifiedKFold(n_splits=splitcounts, random_state = seeds)\n",
    "\n",
    "# params =  {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'auc',\n",
    "#         'num_threads': 4,\n",
    "#         'learning_rate': 0.01, \n",
    "#         'num_iterations' : 10000,\n",
    "#         'max_depth': -1,\n",
    "#         'reg_alpha': 0.3,\n",
    "#          'reg_lambda': 0.3,\n",
    "#         'bagging_seed' : seeds,\n",
    "#         'verbose' : -1,\n",
    "#         'seed' :seeds\n",
    "#     }\n",
    "\n",
    "if LGBM :\n",
    "\n",
    "\n",
    "    params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.007898644187072399, 'num_iterations': 10000, \n",
    "              'num_leaves': 650, 'min_data_in_leaf': 8, 'max_depth': -1, 'bagging_fraction': 0.395331230891172, \n",
    "              'feature_fraction': 0.31552276732000295, 'lambda_l1': 0.3470087563049069, 'lambda_l2': 0.503276501340582,\n",
    "              'min_child_weight': 0.03208937707510653, 'bagging_seed': seeds, 'verbose': -1, 'seed': seeds }\n",
    "             #, 'categorical_feature' :  categorical_features}\n",
    "    CVscore = []\n",
    "    predicts = []\n",
    "    X_idx = []\n",
    "    X_predicts = []\n",
    "    importance = pd.DataFrame(np.zeros((X_train.shape[1], splitcounts)), columns=['Fold_{}'.format(i) for i in range(1, splitcounts+1)], index=X_train.columns)\n",
    "    for fold_, (train_idx, test_idx) in enumerate(folds.split(X_train,Y_train)):\n",
    "#         if fold_ != 9 : continue\n",
    "        \n",
    "        X_train_, X_val_ = X_train.iloc[train_idx,:], X_train.iloc[test_idx,:]\n",
    "        Y_train_, Y_val_ = Y_train.iloc[train_idx], Y_train.iloc[test_idx]\n",
    "\n",
    "        lgb_train = lgb.Dataset(data=X_train_,label = Y_train_)\n",
    "        lgb_valid = lgb.Dataset(data=X_val_,label = Y_val_)\n",
    "\n",
    "        lgb_model = lgb.train(params, lgb_train, valid_sets =lgb_valid, verbose_eval = 200, early_stopping_rounds= 500)\n",
    "        y = lgb_model.predict(X_train, num_iteration=lgb_model.best_iteration)\n",
    "        train_score = roc_auc_score(Y_train.astype('float32'), y) \n",
    "        \n",
    "        \n",
    "        y = lgb_model.predict(X_val_, num_iteration = lgb_model.best_iteration)\n",
    "        X_predicts.append(y)\n",
    "        X_idx += list(test_idx)\n",
    "        score = roc_auc_score(Y_val_,y)\n",
    "        CVscore.append(score)\n",
    "        # 예측\n",
    "#         pd.DataFrame({'TransactionID':list(test_idx),'predict':y}).to_csv('modelprediction.csv')\n",
    "#         y = lgb_model.predict(X_train, num_iteration = lgb_model.best_iteration)\n",
    "#         X_train['predict'] = y\n",
    "#         X_train['predict'].to_csv('predict_fold9.csv')\n",
    "\n",
    "           \n",
    "        \n",
    "        print(\"Fold : \", fold_,\"train_auc : \", train_score, \"val_auc : \", score )\n",
    "        y = lgb_model.predict(X_test,num_iteration = lgb_model.best_iteration)\n",
    "        importance.iloc[:, fold_ - 1] = lgb_model.feature_importance()\n",
    "        predicts.append(y)\n",
    "        lgb_model, lgb_train, lgb_valid = None, None, None\n",
    "        del lgb_model,lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"CV Score : \" ,np.mean(CVscore))\n",
    "    submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n",
    "    submission['isFraud'] = np.mean(predicts, axis =0)\n",
    "    submission.to_csv('submission.csv')\n",
    "    importance.to_csv('importance.csv')\n",
    "#     pd.DataFrame({'TransactionID':X_idx,'predict':X_predicts}).to_csv('modelprediction_all.csv')\n",
    "\n",
    "else :\n",
    "#     X_train = reduce_mem_usage(X_train)\n",
    "#     X_test = reduce_mem_usage(X_test)\n",
    "#     X_train.to_pickle('X_train.pkl')\n",
    "#     X_test.to_pickle('X_test.pkl')\n",
    "#     Y_train.to_pickle('Y_train.pkl')\n",
    "    gc.collect()\n",
    "    params = {\n",
    "                'n_estimators': 5000,\n",
    "                'learning_rate': 0.07,\n",
    "                'eval_metric':'AUC',\n",
    "                'loss_function':'Logloss',\n",
    "                'random_seed':seeds,\n",
    "                'metric_period':500,\n",
    "                'od_wait':500, #earlystoping\n",
    "                'task_type':'GPU',\n",
    "                'depth': 8,\n",
    "                #'colsample_bylevel':0.7,\n",
    "              'max_ctr_complexity' : 2\n",
    "                } \n",
    "    \n",
    "    CVscore = []\n",
    "    predicts = []\n",
    "    X_idx = []\n",
    "    X_predicts = []\n",
    "    importance = pd.DataFrame(np.zeros((X_train.shape[1], splitcounts)), columns=['Fold_{}'.format(i) for i in range(1, splitcounts+1)], index=X_train.columns)\n",
    "#     for fold_, (train_idx, test_idx) in enumerate(folds.split(X_train,Y_train)):\n",
    "#         X_train_+fold_, Y_train_+fold_ =X_train.iloc[train_idx,:],Y_train.iloc[train_idx]\n",
    "        \n",
    "    for fold_, (train_idx, test_idx) in enumerate(folds.split(X_train,Y_train)):\n",
    "        print(fold_)\n",
    "        #if fold_ != 5: continue\n",
    "#         X_train_, X_val_ = , \n",
    "#         Y_train_, Y_val_ = , \n",
    "        X_train_, X_val_ = X_train.iloc[train_idx,:], X_train.iloc[test_idx,:]\n",
    "        Y_train_, Y_val_ = Y_train.iloc[train_idx], Y_train.iloc[test_idx]\n",
    "\n",
    "        cat_model = CatBoostClassifier(**params)        \n",
    "        cat_model.fit(\n",
    "            X_train_,Y_train_,\n",
    "            eval_set=(X_val_, Y_val_),\n",
    "            cat_features=categorical_features,\n",
    "            use_best_model=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"Training end\")\n",
    "\n",
    "#         y = cat_model.predict_proba(X_val_)[:,1]\n",
    "#         X_predicts.append(y)\n",
    "#         X_idx += list(train_idx)\n",
    "#         score = roc_auc_score(Y_val_,y)\n",
    "#         CVscore.append(score)\n",
    "#         print(\"Fold : \", fold_, \"val_auc : \", score )\n",
    "        y = cat_model.predict_proba(X_test)[:,1]\n",
    "#         importance.iloc[:, fold_ - 1] = lgb_model.feature_importance()\n",
    "        predicts.append(y)\n",
    "        cat_model = None\n",
    "        del cat_model, X_train_, X_val_,Y_train_, Y_val_\n",
    "        gc.collect()\n",
    "\n",
    "#     print(\"CV Score : \" ,np.mean(CVscore))\n",
    "    submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n",
    "    submission['isFraud'] = np.mean(predicts, axis =0)\n",
    "    submission.to_csv('submission.csv')\n",
    "#     importance.to_csv('importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LGBM :\n",
    "\n",
    "    importance['Mean_Importance'] = importance.sum(axis=1) / splitcounts\n",
    "    importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(15, 120))\n",
    "    sns.barplot(x='Mean_Importance', y=importance.index, data=importance)\n",
    "\n",
    "    plt.xlabel('')\n",
    "    plt.tick_params(axis='x', labelsize=15)\n",
    "    plt.tick_params(axis='y', labelsize=15)\n",
    "    plt.title('Mean Feature Importance Between Folds', size=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame({'TransactionID':X_idx,'predict':X_predicts})\n",
    "X_predicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
