{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/ieee-fraud-detection/discussion/103439#latest-600713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_identity.csv', 'sample_submission.csv', 'train_identity.csv', 'train_transaction.csv', 'test_transaction.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "# Standard plotly imports\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "#import cufflinks\n",
    "#import cufflinks as cf\n",
    "import plotly.figure_factory as ff\n",
    "import datetime\n",
    "# Using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "#cufflinks.go_offline(connected=True)\n",
    "\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "# from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "\n",
    "## Hyperopt modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"../input/ieee-fraud-detection\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 44.39 MB\n",
      "Memory usage of dataframe is 45.12 MB\n",
      "Memory usage after optimization is: 10.40 MB\n",
      "Decreased by 76.6%\n",
      "Memory usage after optimization is: 10.57 MB\n",
      "Decreased by 76.6%\n",
      "Memory usage of dataframe is 7.73 MB\n",
      "Memory usage after optimization is: 4.83 MB\n",
      "Decreased by 37.5%\n",
      "Memory usage of dataframe is 1519.24 MB\n",
      "Memory usage of dataframe is 1775.15 MB\n",
      "Memory usage after optimization is: 427.17 MB\n",
      "Decreased by 71.9%\n",
      "Memory usage after optimization is: 489.41 MB\n",
      "Decreased by 72.4%\n",
      "CPU times: user 956 ms, sys: 2.21 s, total: 3.17 s\n",
      "Wall time: 4min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "warnings.simplefilter('ignore')\n",
    "files = ['../input/ieee-fraud-detection/test_identity.csv', \n",
    "         '../input/ieee-fraud-detection/test_transaction.csv',\n",
    "         '../input/ieee-fraud-detection/train_identity.csv',\n",
    "         '../input/ieee-fraud-detection/train_transaction.csv',\n",
    "         '../input/ieee-fraud-detection/sample_submission.csv']\n",
    "\n",
    "def load_data(file):\n",
    "    return reduce_mem_usage(pd.read_csv(file, index_col='TransactionID'))\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    test_identity, test_transaction, train_identity, train_transaction, sample_submission = pool.map(load_data, files)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_transaction = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID'))\n",
    "# test_transaction = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID'))\n",
    "\n",
    "# train_identity = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID'))\n",
    "# test_identity = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID'))\n",
    "\n",
    "# sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corret_card_id(x): \n",
    "    x=x.replace('.0','')\n",
    "    x=x.replace('-999','nan')\n",
    "    return x\n",
    "\n",
    "def define_indexes(df):\n",
    "    \n",
    "    # create date column\n",
    "    START_DATE = '2017-12-01'\n",
    "    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "    \n",
    "    df['year'] = df['TransactionDT'].dt.year\n",
    "    df['month'] = df['TransactionDT'].dt.month\n",
    "    df['dow'] = df['TransactionDT'].dt.dayofweek\n",
    "    df['hour'] = df['TransactionDT'].dt.hour\n",
    "    df['day'] = df['TransactionDT'].dt.day\n",
    "   \n",
    "    # create card ID \n",
    "    cards_cols= ['card1', 'card2', 'card3', 'card5']\n",
    "    for card in cards_cols: \n",
    "        if '1' in card: \n",
    "            df['card_id']= df[card].map(str)\n",
    "        else : \n",
    "            df['card_id']+= ' '+df[card].map(str)\n",
    "    \n",
    "    # small correction of the Card_ID\n",
    "    df['card_id']=df['card_id'].apply(corret_card_id)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = define_indexes(train)\n",
    "test = define_indexes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n",
    "train['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n",
    "train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n",
    "test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n",
    "test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "train['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "train['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 신규로 만든 cardID에 대해 Amountamt를을 평균으로 나눠보자  : 이 건이 평균대비 많은지 적은지 - V13\n",
    "train['TransactionAmt_to_mean_card_id'] = train['TransactionAmt'] / train.groupby(['card_id'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card_id'] = train['TransactionAmt'] / train.groupby(['card_id'])['TransactionAmt'].transform('std')\n",
    "\n",
    "test['TransactionAmt_to_mean_card_id'] = test['TransactionAmt'] / test.groupby(['card_id'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card_id'] = test['TransactionAmt'] / test.groupby(['card_id'])['TransactionAmt'].transform('std')\n",
    "\n",
    "train['id_02_to_mean_card_id'] = train['id_02'] / train.groupby(['card_id'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card_id'] = train['id_02'] / train.groupby(['card_id'])['id_02'].transform('std')\n",
    "\n",
    "test['id_02_to_mean_card_id'] = test['id_02'] / test.groupby(['card_id'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card_id'] = test['id_02'] / test.groupby(['card_id'])['id_02'].transform('std')\n",
    "\n",
    "train['D15_to_mean_card_id'] = train['D15'] / train.groupby(['card_id'])['D15'].transform('mean')\n",
    "train['D15_to_std_card_id'] = train['D15'] / train.groupby(['card_id'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_card_id'] = test['D15'] / test.groupby(['card_id'])['D15'].transform('mean')\n",
    "test['D15_to_std_card_id'] = test['D15'] / test.groupby(['card_id'])['D15'].transform('std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardid 기준으로 최근 5일간 거래량 통계\n",
    "train['count_last'] = train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).count())\n",
    "train['mean_last'] = train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "train['min_last'] = train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).min())\n",
    "train['max_last'] = train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).max())\n",
    "train['std_last'] = train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "\n",
    "test['count_last'] = test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).count())\n",
    "test['mean_last'] = test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "test['min_last'] = test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).min())\n",
    "test['max_last'] = test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).max())\n",
    "test['std_last'] = test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "\n",
    "#최근 10일 평균 대비 큰지 작은지 \n",
    "train['trans_mean_last'] = train['TransactionAmt'] / train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "train['trans_std_last'] = train['TransactionAmt'] / train.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).std())\n",
    "test['trans_mean_last'] = test['TransactionAmt'] / test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "test['trans_std_last'] = test['TransactionAmt'] / test.groupby('card_id')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n"
     ]
    }
   ],
   "source": [
    "print(train['R_emaildomain'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n",
    "\n",
    "# train.loc[train['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n",
    "#                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n",
    "#                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\n",
    "# train.loc[train['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n",
    "#                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n",
    "#                                             'outlook.es', 'live.com', 'live.fr',\n",
    "#                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n",
    "# train.loc[train.R_emaildomain.isin(train.R_emaildomain\\\n",
    "#                                         .value_counts()[train.R_emaildomain.value_counts() <= 300 ]\\\n",
    "#                                         .index), 'R_emaildomain'] = \"Others\"\n",
    "# train.R_emaildomain.fillna(\"NoInf\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.loc[test['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n",
    "#\n",
    "#test.loc[test['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n",
    "#                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n",
    "#                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\n",
    "#test.loc[test['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n",
    "#                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n",
    "#                                             'outlook.es', 'live.com', 'live.fr',\n",
    "#                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n",
    "#test.loc[test.R_emaildomain.isin(test.R_emaildomain\\\n",
    " #                                        .value_counts()[test.R_emaildomain.value_counts() <= 300 ]\\\n",
    "#                                         .index), 'R_emaildomain'] = \"Others\"\n",
    "#test.R_emaildomain.fillna(\"NoInf\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Freq encoding\n",
    "# 각 컬럼의 값의 전체에서 얼만큼 빈도수를 가지는지 카운트하여 넣어줌 -> C,D는 크게 의미 없을 듯?\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "         # 'P_emaildomain', \n",
    "          'R_emaildomain'\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()    # Null 값도 계산하게하기\n",
    "    train[col+'_fq_enc'] = train[col].map(fq_encode)\n",
    "    test[col+'_fq_enc']  = test[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n",
    "def features_interaction(df, feature_1, feature_2):\n",
    "    return df[feature_1].astype(str) + '_' + df[feature_2].astype(str)\n",
    "features_interactions = [\n",
    "    'id_02__id_20',\n",
    "    'id_02__D8',\n",
    "    'D11__DeviceInfo',\n",
    "    'DeviceInfo__P_emaildomain',\n",
    "    'P_emaildomain__C2',\n",
    "    'card2__dist1',\n",
    "    'card1__card5',\n",
    "    'card2__id_20',\n",
    "    'card5__P_emaildomain',\n",
    "    'addr1__card1'\n",
    "]\n",
    "\n",
    "for new_feature in features_interactions:\n",
    "    feature_1, feature_2 = new_feature.split('__')\n",
    "    \n",
    "    train[new_feature] = features_interaction(train, feature_1, feature_2)\n",
    "    test[new_feature] = features_interaction(test, feature_1, feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount Log취하기\n",
    "train['TransactionAmt'] = np.log(train['TransactionAmt'])\n",
    "test['TransactionAmt'] = np.log(test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - V\n",
    "# mas_v = df.columns[55:]\n",
    "\n",
    "# for col in mas_v:\n",
    "#     df[col].fillna((df[col].min() - 2), inplace=True)\n",
    "#     df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "    \n",
    "# df = PCA_change(df, mas_v, prefix='PCA_V_', n_components=35)\n",
    "\n",
    "# columns = ['PCA_V_0', 'PCA_V_1', 'PCA_V_2', 'PCA_V_3', 'PCA_V_4', 'PCA_V_5', \n",
    "#            'PCA_V_6', 'PCA_V_7', 'PCA_V_8', 'PCA_V_9', 'PCA_V_10', 'PCA_V_11', \n",
    "#            'PCA_V_12', 'PCA_V_13', 'PCA_V_14', 'PCA_V_15', 'PCA_V_16', \n",
    "#            'PCA_V_17', 'PCA_V_18', 'PCA_V_19', 'PCA_V_20', 'PCA_V_21', \n",
    "#            'PCA_V_22', 'PCA_V_23', 'PCA_V_24', 'PCA_V_25', 'PCA_V_26', \n",
    "#            'PCA_V_27', 'PCA_V_28', 'PCA_V_29', 'PCA_V_30', 'PCA_V_31', \n",
    "#            'PCA_V_32', 'PCA_V_33', 'PCA_V_34']\n",
    "\n",
    "# km = KMeans(n_clusters=6)\n",
    "# km = km.fit(df[columns])\n",
    "# df['clusters_V'] = km.predict(df[columns])\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### M columns (except M4)\n",
    "# All these columns are binary encoded 1/0\n",
    "# We can have some features from it\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n",
    "    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\n",
    "one_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\n",
    "\n",
    "many_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\n",
    "many_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]\n",
    "\n",
    "big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "\n",
    "cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\n",
    "\n",
    "cols_to_drop.remove('isFraud')\n",
    "\n",
    "train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "test.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10).T.to_csv('file.csv')#'TransactionDT' 삭제\n",
    "# train.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle('train.pkl')\n",
    "test.to_pickle('test.pkl')\n",
    "sample_submission.to_pickle('sample_submission.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_transaction, train_identity, test_transaction, test_identity\n",
    "\n",
    "Y_train = train['isFraud'].copy()\n",
    "\n",
    "X_train = train.drop('isFraud', axis=1)\n",
    "X_train.drop('TransactionDT', axis=1, inplace=True)\n",
    "X_test = test.drop('TransactionDT', axis=1)\n",
    "\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductCD\n",
      "card4\n",
      "card6\n",
      "P_emaildomain\n",
      "R_emaildomain\n",
      "M1\n",
      "M2\n",
      "M3\n",
      "M4\n",
      "M5\n",
      "M6\n",
      "M7\n",
      "M8\n",
      "M9\n",
      "id_12\n",
      "id_15\n",
      "id_16\n",
      "id_28\n",
      "id_29\n",
      "id_30\n",
      "id_31\n",
      "id_33\n",
      "id_34\n",
      "id_35\n",
      "id_36\n",
      "id_37\n",
      "id_38\n",
      "DeviceType\n",
      "DeviceInfo\n",
      "card_id\n",
      "id_02__id_20\n",
      "id_02__D8\n",
      "D11__DeviceInfo\n",
      "DeviceInfo__P_emaildomain\n",
      "P_emaildomain__C2\n",
      "card2__dist1\n",
      "card1__card5\n",
      "card2__id_20\n",
      "card5__P_emaildomain\n",
      "addr1__card1\n"
     ]
    }
   ],
   "source": [
    "for f in X_train.select_dtypes(include='category').columns.tolist() + X_train.select_dtypes(include='object').columns.tolist():\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    print(f)\n",
    "    lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "    X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "    X_test[f] = lbl.transform(list(X_test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5%3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment\n",
    "#https://www.kaggle.com/niteshx2/beginner-explained-lgb-2-leaves-augment\n",
    "# Data augmentation\n",
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            if c%3 ==0:\n",
    "                np.random.shuffle(ids)\n",
    "                x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "   # for i in range(t//2):\n",
    "    #    mask = y==0\n",
    "   #     x1 = x[mask].copy()\n",
    "   #     ids = np.arange(x1.shape[0])\n",
    "   #     for c in range(x1.shape[1]):\n",
    "   #         if c%3 == 0 :\n",
    "   #             np.random.shuffle(ids)\n",
    "    #            x1[:,c] = x1[ids][:,c]\n",
    "    #    xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "   # xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "   # yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs])\n",
    "    y = np.concatenate([y,ys])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter 찾기\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  N = 5\n",
    "#     p_valid,yp = 0,0\n",
    "#     for i in range(N):\n",
    "#         X_t, y_t = augment(X_train.values, y_train.values)\n",
    "#         X_t = pd.DataFrame(X_t)\n",
    "#         X_t = X_t.add_prefix('var_')\n",
    "    \n",
    "#         trn_data = lgb.Dataset(X_t, label=y_t)\n",
    "#         val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "#         evals_result = {}\n",
    "#         lgb_clf = lgb.train(lgb_params,\n",
    "#                         trn_data,\n",
    "#                         100000,\n",
    "#                         valid_sets = [trn_data, val_data],\n",
    "#                         early_stopping_rounds=3000,\n",
    "#                         verbose_eval = 1000,\n",
    "#                         evals_result=evals_result\n",
    "#                        )\n",
    "#         p_valid += lgb_clf.predict(X_valid)\n",
    "#         yp += lgb_clf.predict(X_test)\n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "#     oof['predict'][val_idx] = p_valid/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate,num_leaves, min_data_in_leaf, bagging_fraction, feature_fraction, lambda_l1, lambda_l2,max_bin,min_child_weight):\n",
    "    print(\"############## New Run ################\")\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_threads': 4,\n",
    "        'learning_rate': learning_rate,#0.01, # learning rate\n",
    "        'num_iterations' : 200,#3000,\n",
    "        #'n_estimators' : 800,\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        'max_depth': -1,#int(max_depth),\n",
    "        'bagging_fraction' : bagging_fraction,\n",
    "        'feature_fraction' : feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'max_bin' : int(max_bin),\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'bagging_seed' : 11,\n",
    "        #'early_stopping_round' : 50,\n",
    "        'verbose' : -1,\n",
    "        'seed' :165\n",
    "    }\n",
    "    print(\"PARAMETERS: \")\n",
    "    print(f\"params  = {params}\")\n",
    "    \n",
    "    tscv =  StratifiedKFold(n_splits=5, shuffle=True, random_state=42)#KFold(n_splits=5)#StratifiedKFold(n_splits=5)#TimeSeriesSplit(n_splits=3)\n",
    "    sc = []\n",
    "#     for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "    for fold_,(train_idx, test_idx) in enumerate(tscv.split(X_train, Y_train)):\n",
    "#         print( Y_train.iloc[train_idx].head())\n",
    "        x_train, x_val = X_train.iloc[train_idx,:], X_train.iloc[test_idx,:]\n",
    "        y_train, y_val = Y_train.iloc[train_idx], Y_train.iloc[test_idx]\n",
    "        \n",
    "#         sc2 =[]\n",
    "#         N = 3\n",
    "#         for i in range(N):\n",
    "        #Data Augmentation Test\n",
    "#         X_t, y_t = augment(x_train.values, y_train.values)\n",
    "#         X_t = pd.DataFrame(X_t)\n",
    "# #             X_t = X_t.add_prefix('var_')\n",
    "#         lgb_train = lgb.Dataset(X_t.astype('float32'), label=y_t.astype('float32'))\n",
    "#         lgb_valid = lgb.Dataset(data=x_val.astype('float32'), label=y_val.astype('float32'))\n",
    "        #############################\n",
    "        lgb_train = lgb.Dataset(data=x_train.astype('float32'), label=y_train.astype('float32'))\n",
    "        lgb_valid = lgb.Dataset(data=x_val.astype('float32'), label=y_val.astype('float32'))\n",
    "        \n",
    "        \n",
    "        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=500)\n",
    "        y = lgb_model.predict(x_train.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "        train_score = roc_auc_score(y_train.astype('float32'), y)        \n",
    "        y = lgb_model.predict(x_val.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "        score = roc_auc_score(y_val.astype('float32'), y)\n",
    "        print (\"Fold : \", fold_,\"train_auc : \",train_score,\"val_auc : \", score)\n",
    "        sc.append(score)\n",
    "        \n",
    "    paralst.append(params)\n",
    "    scorelst.append(np.mean(sc))\n",
    "    df_para = pd.DataFrame({\"parameter\" :paralst,\"score\":scorelst})\n",
    "    print(df_para)\n",
    "    df_para.to_csv('para.csv',index=False)\n",
    "\n",
    "        \n",
    "    return np.mean(sc)\n",
    "\n",
    "bounds = {\n",
    "    'num_leaves': (1300,1600),#(450, 500),\n",
    "    'min_data_in_leaf': (0,100),#(100, 150),\n",
    "    #'max_depth': -1,#(-1, 50), # -> -1\n",
    "    'learning_rate': (0.014,0.019),#0.006883242363721497,\n",
    "    'bagging_fraction' : (0.3, 0.5),\n",
    "    'feature_fraction' : (0.1, 0.4),\n",
    "    'lambda_l1': (0.3, 0.5),\n",
    "    'lambda_l2': (0.3, 0.7),\n",
    "    'max_bin' : (150,255), # 추가\n",
    "    'min_child_weight': (0.01, 0.05),\n",
    "}\n",
    "\n",
    "traintime = False\n",
    "if traintime :\n",
    "    paralst,scorelst = [],[]\n",
    "    bo = BayesianOptimization(train_model, bounds, random_state= 165)\n",
    "    bo.maximize(init_points=5, n_iter=30, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_threads': 4,\n",
    "        'bagging_seed' : 11,\n",
    "        'learning_rate':bo.max['params']['learning_rate'],#0.01,\n",
    "        'n_estimators' : 200,#3000,\n",
    "        'num_leaves': int(bo.max['params']['num_leaves']),\n",
    "        'min_data_in_leaf': int(bo.max['params']['min_data_in_leaf']),\n",
    "        'max_depth': -1,#int(bo.max['params']['max_depth']),\n",
    "        'bagging_fraction' : bo.max['params']['bagging_fraction'],\n",
    "        'feature_fraction' : bo.max['params']['feature_fraction'],\n",
    "        'lambda_l1': bo.max['params']['lambda_l1'],\n",
    "        'lambda_l2': bo.max['params']['lambda_l2'],\n",
    "        'max_bin' : bo.max['params']['max_bin'],\n",
    "        'min_child_weight' :  bo.max['params']['min_child_weight'],\n",
    "       #'early_stopping_round' : 50,\n",
    "        'verbose' : -1,\n",
    "        'seed' :165\n",
    "\n",
    "    }\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.014645596881635362, 'num_iterations': 3000, 'num_leaves': 1385, 'min_data_in_leaf': 2, 'max_depth': -1, 'bagging_fraction': 0.38470656275266424, 'feature_fraction': 0.3963200069886693, 'lambda_l1': 0.3620323536611625, 'lambda_l2': 0.35373332136972424, 'max_bin': 154, 'min_child_weight': 0.04295077705493547, 'bagging_seed': 11, 'verbose': -1, 'seed': 12}\n",
      "(472431, 419)\n",
      "(505491, 419)\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tvalid_0's auc: 0.951435\n",
      "[400]\tvalid_0's auc: 0.96803\n",
      "[600]\tvalid_0's auc: 0.971613\n",
      "[800]\tvalid_0's auc: 0.972827\n",
      "[1000]\tvalid_0's auc: 0.973204\n",
      "Early stopping, best iteration is:\n",
      "[996]\tvalid_0's auc: 0.973206\n",
      "Fold :  0 train_auc :  1.0 val_auc :  0.9732063436432451\n",
      "(472431, 419)\n",
      "(505491, 419)\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tvalid_0's auc: 0.953504\n",
      "[400]\tvalid_0's auc: 0.969205\n",
      "[600]\tvalid_0's auc: 0.972865\n",
      "[800]\tvalid_0's auc: 0.973745\n",
      "[1000]\tvalid_0's auc: 0.973891\n",
      "Early stopping, best iteration is:\n",
      "[951]\tvalid_0's auc: 0.973911\n",
      "Fold :  1 train_auc :  1.0 val_auc :  0.9739096850965998\n",
      "(472432, 419)\n",
      "(505492, 419)\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tvalid_0's auc: 0.952273\n",
      "[400]\tvalid_0's auc: 0.96883\n",
      "[600]\tvalid_0's auc: 0.972537\n",
      "[800]\tvalid_0's auc: 0.973648\n",
      "[1000]\tvalid_0's auc: 0.974094\n",
      "[1200]\tvalid_0's auc: 0.974253\n",
      "[1400]\tvalid_0's auc: 0.974394\n",
      "Early stopping, best iteration is:\n",
      "[1543]\tvalid_0's auc: 0.974469\n",
      "Fold :  2 train_auc :  1.0 val_auc :  0.9744676584079469\n",
      "(472433, 419)\n",
      "(505495, 419)\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tvalid_0's auc: 0.950271\n",
      "[400]\tvalid_0's auc: 0.966693\n",
      "[600]\tvalid_0's auc: 0.97011\n",
      "[800]\tvalid_0's auc: 0.970797\n",
      "Early stopping, best iteration is:\n",
      "[891]\tvalid_0's auc: 0.970891\n",
      "Fold :  3 train_auc :  1.0 val_auc :  0.9708890130837018\n",
      "(472433, 419)\n",
      "(505495, 419)\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tvalid_0's auc: 0.952099\n",
      "[400]\tvalid_0's auc: 0.969413\n",
      "[600]\tvalid_0's auc: 0.973196\n",
      "[800]\tvalid_0's auc: 0.974099\n",
      "[1000]\tvalid_0's auc: 0.974282\n",
      "Early stopping, best iteration is:\n",
      "[1000]\tvalid_0's auc: 0.974282\n",
      "Fold :  4 train_auc :  1.0 val_auc :  0.9742808571791977\n",
      "[0.00013878 0.00070952 0.00051266 ... 0.00031159 0.00055841 0.00109544]\n"
     ]
    }
   ],
   "source": [
    "if not traintime :  \n",
    "    params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.014645596881635362, 'num_iterations': 3000, 'num_leaves': 1385, 'min_data_in_leaf': 2, 'max_depth': -1, 'bagging_fraction': 0.38470656275266424, 'feature_fraction': 0.3963200069886693, 'lambda_l1': 0.3620323536611625, 'lambda_l2': 0.35373332136972424, 'max_bin': 154, 'min_child_weight': 0.04295077705493547, 'bagging_seed': 11, 'verbose': -1, 'seed': 12}#0.96069\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.014960621900450888, 'num_iterations': 3000, 'num_leaves': 1395, 'min_data_in_leaf': 0, 'max_depth': -1, 'bagging_fraction': 0.34044595394766747, 'feature_fraction': 0.34214407482513476, 'lambda_l1': 0.4805008954626611, 'lambda_l2': 0.37118265029595326, 'max_bin': 245, 'min_child_weight': 0.029364491351868728, 'bagging_seed': 11, 'verbose': -1, 'seed': 399} #0.960682\n",
    "    \n",
    "    \n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.009833261038853328, 'num_iterations': 3000, 'num_leaves': 1149, 'min_data_in_leaf': 49, 'max_depth': -1, 'bagging_fraction': 0.3240951596739299, 'feature_fraction': 0.3498407985953037, 'lambda_l1': 0.3501312050658983, 'lambda_l2': 0.31690592587909633, 'max_bin': 161, 'min_child_weight': 0.0114896062770591, 'bagging_seed': 11, 'verbose': -1, 'seed': 99} #0.92151\n",
    "#     params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.009754414928876673, 'num_iterations': 3000, 'num_leaves': 1143, 'min_data_in_leaf': 35, 'max_depth': -1, 'bagging_fraction': 0.3554679818433634, 'feature_fraction': 0.3402175958274567, 'lambda_l1': 0.34752046110782797, 'lambda_l2': 0.37744033227254215, 'max_bin': 242, 'min_child_weight': 0.03617820556477377, 'bagging_seed': 11, 'verbose': -1, 'seed': 165} #0.921416\n",
    "    #0.9474\n",
    "   # params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.008872937600948674, 'num_iterations': 3000, 'num_leaves': 999, 'min_data_in_leaf': 10, 'max_depth': -1, 'bagging_fraction': 0.4713288697981503, 'feature_fraction': 0.39018895127170683, 'lambda_l1': 0.3510394531977163, 'lambda_l2': 0.3873703557388186, 'max_bin': 254, 'min_child_weight': 0.033084878554612146, 'bagging_seed': 11, 'verbose': -1, 'seed': 12} #0.919813\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.008976158898617054, 'num_iterations': 3000, 'num_leaves': 995, 'min_data_in_leaf': 14, 'max_depth': -1, 'bagging_fraction': 0.4682934735568483, 'feature_fraction': 0.3965931325683636, 'lambda_l1': 0.3760099829112723, 'lambda_l2': 0.3975897064215117, 'max_bin': 254, 'min_child_weight': 0.02847878240902596, 'bagging_seed': 11, 'verbose': -1, 'seed': 277} #0.919887\n",
    "    #0.9465\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.00898758545742255, 'num_iterations': 3000, 'num_leaves': 924, 'min_data_in_leaf': 14, 'max_depth': -1, 'bagging_fraction': 0.4024823598624291, 'feature_fraction': 0.3208499817534414, 'lambda_l1': 0.46115302747432046, 'lambda_l2': 0.35539565704287, 'max_bin': 250, 'min_child_weight': 0.04355005924028087, 'bagging_seed': 11, 'verbose': -1, 'seed': 399} #0.91827\n",
    "    #0.9467\n",
    "    #params ={'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.007972931513741082, 'num_iterations': 3000, 'num_leaves': 799, 'min_data_in_leaf': 1, 'max_depth': -1, 'bagging_fraction': 0.4895500948839138, 'feature_fraction': 0.37387418327918154, 'lambda_l1': 0.41482044398244905, 'lambda_l2': 0.5362091922107519, 'min_child_weight': 0.021811137046946497, 'bagging_seed': 11, 'verbose': -1, 'seed': 132} #0.916263\n",
    "    #0.9469 -> 0.9473\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.007898644187072399, 'num_iterations': 3000, 'num_leaves': 785, 'min_data_in_leaf': 8, 'max_depth': -1, 'bagging_fraction': 0.395331230891172, 'feature_fraction': 0.31552276732000295, 'lambda_l1': 0.3470087563049069, 'lambda_l2': 0.503276501340582, 'min_child_weight': 0.03208937707510653, 'bagging_seed': 11, 'verbose': -1, 'seed': 99} #0.916033\n",
    "    #0.9468\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.007899687615149507, 'num_iterations': 3000, 'num_leaves': 799, 'min_data_in_leaf': 0, 'max_depth': -1, 'bagging_fraction': 0.33041506218684263, 'feature_fraction': 0.35416575497676395, 'lambda_l1': 0.38704306154857826, 'lambda_l2': 0.5178012092942015, 'min_child_weight': 0.026728352023513757, 'bagging_seed': 11, 'verbose': -1, 'seed': 12} #0.916009\n",
    "    # 0.9467\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.007372873282822138, 'num_iterations': 3000, 'num_leaves': 599, 'min_data_in_leaf': 50, 'max_depth': -1, 'bagging_fraction': 0.38244660605863556, 'feature_fraction': 0.44257840443437824, 'lambda_l1': 0.45420352424554195, 'lambda_l2': 0.5105273366387331, 'min_child_weight': 0.04533307349626744, 'bagging_seed': 11, 'verbose': -1, 'seed': 160} #0.9131\n",
    "    # 0.9465\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.007203449399886559, 'num_iterations': 3000, 'num_leaves': 599, 'min_data_in_leaf': 50, 'max_depth': -1, 'bagging_fraction': 0.38525498446643613, 'feature_fraction': 0.380367681990148, 'lambda_l1': 0.34523297270543946, 'lambda_l2': 0.5734926679250656, 'min_child_weight': 0.02678622651731609, 'bagging_seed': 11, 'verbose': -1, 'seed': 160} #0.913067\n",
    "    #0.9460이하\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.006990904355014822, 'num_iterations': 3000, 'num_leaves': 499, 'min_data_in_leaf': 100, 'max_depth': -1, 'bagging_fraction': 0.3812981748987723, 'feature_fraction': 0.47302209023435043, 'lambda_l1': 0.4631415001252107, 'lambda_l2': 0.5618020019879593, 'min_child_weight': 0.043917717737188104, 'bagging_seed': 11, 'verbose': -1, 'seed': 105} # 0.909427\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.006787730751128605, 'num_iterations': 3000, 'num_leaves': 499, 'min_data_in_leaf': 100, 'max_depth': -1, 'bagging_fraction': 0.47560044639915244, 'feature_fraction': 0.41666150781008676, 'lambda_l1': 0.47548378689674464, 'lambda_l2': 0.9757213490246281, 'min_child_weight': 0.041972701015528587, 'bagging_seed': 11, 'verbose': -1, 'seed': 105} # 0.909053\n",
    "    #params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.006882517225047709, 'num_iterations': 3000, 'num_leaves': 499, 'min_data_in_leaf': 100, 'max_depth': -1, 'bagging_fraction': 0.4237390751101919, 'feature_fraction': 0.43926420965306257, 'lambda_l1': 0.3175126554926237, 'lambda_l2': 0.7515373479664667, 'min_child_weight': 0.017207106806066748, 'bagging_seed': 11, 'verbose': -1, 'seed': 105} #0.909397\n",
    "   # params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.006535640044021754, 'num_iterations': 3000, 'num_leaves': 450, 'min_data_in_leaf': 114, 'max_depth': -1, 'bagging_fraction': 0.3167479847792764, 'feature_fraction': 0.3667012584958002, 'lambda_l1': 0.4718711490177294, 'lambda_l2': 0.5528434176595423, 'min_child_weight': 0.01254758279001024, 'bagging_seed': 11, 'verbose': -1, 'seed': 105}\n",
    "   # params = {'objective': 'binary', 'metric': 'auc', 'num_threads': 4, 'learning_rate': 0.006536108525154421, 'num_iterations': 3000, 'num_leaves': 455, 'min_data_in_leaf': 108, 'max_depth': -1, 'bagging_fraction': 0.4637593273666464, 'feature_fraction': 0.43726611671270266, 'lambda_l1': 0.4261934244016219, 'lambda_l2': 0.6851945667053867, 'min_child_weight': 0.016926478912796333, 'bagging_seed': 11, 'verbose': -1, 'seed': 105} # Base 0.9461\n",
    "   \n",
    "\n",
    "    # params = {'num_leaves': 455,\n",
    "    #           'n_estimators' : 10000,\n",
    "    #          'min_child_weight': 0.03454472573214212,\n",
    "   #           'feature_fraction': 0.3797454081646243,\n",
    "   #           'bagging_fraction': 0.4181193142567742,\n",
    "    #          'min_data_in_leaf': 106,\n",
    "     #         'objective': 'binary',\n",
    "   #           'max_depth': -1,\n",
    "    #          'learning_rate': 0.006883242363721497,\n",
    "  #            \"boosting_type\": \"gbdt\",\n",
    "    #          \"bagging_seed\": 11,\n",
    "    #          \"metric\": 'auc',\n",
    "    #          \"verbosity\": -1,\n",
    "     #         'reg_alpha': 0.3899927210061127,\n",
    "      #        'reg_lambda': 0.6485237330340494,\n",
    "     #         'random_state': 47\n",
    "     #        }\n",
    "    print(params)\n",
    "\n",
    "\n",
    "\n",
    "    # Cross validation Score?로 제출?\n",
    "\n",
    "    tscv =  StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#KFold(n_splits=5)#(n_splits=8, shuffle=True, random_state=42)#TimeSeriesSplit(n_splits=5) #? -> \n",
    "    sc = []\n",
    "    #     for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "    for fold_,(train_idx, test_idx) in enumerate(tscv.split(X_train, Y_train)):\n",
    "    #         print( Y_train.iloc[train_idx].head())\n",
    "        x_train, x_val = X_train.iloc[train_idx,:], X_train.iloc[test_idx,:]\n",
    "        y_train, y_val = Y_train.iloc[train_idx], Y_train.iloc[test_idx]\n",
    "        print(x_train.shape)\n",
    "#         sc2 =[]\n",
    "#         N = 3\n",
    "#         for i in range(N):\n",
    "#         Data Augmentation Test\n",
    "        X_t, y_t = augment(x_train.values, y_train.values)\n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        print(X_t.shape)\n",
    "# #             X_t = X_t.add_prefix('var_')\n",
    "         # augmentation 한 것\n",
    "        lgb_train = lgb.Dataset(X_t.astype('float32'), label=y_t.astype('float32'))\n",
    "        lgb_valid = lgb.Dataset(data=x_val.astype('float32'), label=y_val.astype('float32'))\n",
    "        \n",
    "        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=200,early_stopping_rounds=50)\n",
    "        y = lgb_model.predict(x_train.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "        train_score = roc_auc_score(y_train.astype('float32'), y)        \n",
    "        y = lgb_model.predict(x_val.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "        score = roc_auc_score(y_val.astype('float32'), y)\n",
    "            \n",
    "        print (\"Fold : \", fold_,\"train_auc : \",train_score,\"val_auc : \", score)\n",
    "        y = lgb_model.predict(X_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "        sc.append(y)\n",
    "\n",
    "       # lgb_train = lgb.Dataset(data=x_train.astype('float32'), label=y_train.astype('float32'))\n",
    "        #lgb_valid = lgb.Dataset(data=x_val.astype('float32'), label=y_val.astype('float32'))\n",
    "       # lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=200,early_stopping_rounds=50)\n",
    "       # y = lgb_model.predict(x_train.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "      #  train_score = roc_auc_score(y_train.astype('float32'), y)        \n",
    "      #  y = lgb_model.predict(x_val.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "      #  score = roc_auc_score(y_val.astype('float32'), y)\n",
    "      #  print (\"Fold : \", fold_,\"train_auc : \",train_score,\"val_auc : \", score)\n",
    "       # y = lgb_model.predict(X_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "       # sc.append(y)\n",
    "\n",
    "        \n",
    "    print(np.mean(sc,axis =0))\n",
    "\n",
    "    submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n",
    "    submission['isFraud'] = np.mean(sc, axis =0)\n",
    "    submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇개 Parameter를 고정하고 Grid search\n",
    "\n",
    "# import copy\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import ParameterGrid, StratifiedKFold\n",
    "\n",
    "# N_SPLITS = 3\n",
    "# df_train = pd.read_csv('processed/df_train.csv')\n",
    "# df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "# target = pd.read_csv('processed/df_train_target.csv', header=None)\n",
    "# assert df_train.shape[0] == target.shape[0]\n",
    "# trn_data = lgb.Dataset(df_train[df_train_columns], label=target)\n",
    "# folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "# folds_generator = folds.split(df_train, df_train['outliers'].values)\n",
    "\n",
    "# default_params = {'num_leaves': 31,\n",
    "#                   'min_data_in_leaf': 30,\n",
    "#                   'objective': 'regression',\n",
    "#                   'max_depth': -1,\n",
    "#                   'learning_rate': 0.01,\n",
    "#                   \"min_child_samples\": 20,\n",
    "#                   \"boosting\": \"gbdt\",\n",
    "#                   \"feature_fraction\": 0.9,\n",
    "#                   \"bagging_freq\": 1,\n",
    "#                   \"bagging_fraction\": 0.9,\n",
    "#                   \"bagging_seed\": 11,\n",
    "#                   \"metric\": 'rmse',\n",
    "#                   \"lambda_l1\": 0.1,\n",
    "#                   \"verbosity\": -1,\n",
    "#                   \"nthread\": 4,\n",
    "#                   \"random_state\": 42,\n",
    "#                   \"device\": 'gpu'}\n",
    "# params_cv = {'min_data_in_leaf': [30, 60], \"min_child_samples\": [20, 40]}\n",
    "# param_grid = ParameterGrid(params_cv)\n",
    "# results = []\n",
    "# for param in param_grid:\n",
    "#     param_now = copy.copy(default_params)\n",
    "#     param_now.update(param)\n",
    "#     cv_results = lgb.cv(param_now, trn_data, num_boost_round=10000,\n",
    "#                         folds=folds_generator,\n",
    "#                         early_stopping_rounds=100,\n",
    "#                         stratified=False)\n",
    "#     # print(cv_results['rmse-mean'], cv_results['rmse-std'])\n",
    "#     cv_results = (param_now, cv_results)\n",
    "#     results.append(cv_results)\n",
    "\n",
    "# with open('hyper_searchs/lgb_cv.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
