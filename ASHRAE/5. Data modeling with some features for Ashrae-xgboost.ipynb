{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 types of energy to predict <br>\n",
    "    - 0 : electricity\n",
    "    - 1 : chilledwater\n",
    "    - 2 : steam\n",
    "    - 3 : hotwater\n",
    "\n",
    "Electricity and water consumption may have different behavior!<br>\n",
    "     - I will make separately train & predict the model\n",
    "\n",
    "* Reference \n",
    " - https://www.kaggle.com/corochann/ashrae-training-lgbm-by-meter-type\n",
    " - https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No leak data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 파일 읽어오기\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "weather_train_df = pd.read_csv('weather_train.csv')\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "weather_test_df = pd.read_csv('weather_test.csv')\n",
    "building_meta_df = pd.read_csv('building_metadata.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20216100, 4)\n",
      "(139773, 9)\n",
      "(277243, 9)\n",
      "(1449, 6)\n",
      "(41697600, 4)\n"
     ]
    }
   ],
   "source": [
    "# Glimpse of Data\n",
    "print(train_df.shape)\n",
    "print(weather_train_df.shape)\n",
    "print(weather_test_df.shape)\n",
    "print(building_meta_df.shape)\n",
    "\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ㅒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  3.07 Mb (68.1% reduction)\n",
      "Mem. usage decreased to  6.08 Mb (68.1% reduction)\n",
      "Mem. usage decreased to  0.03 Mb (60.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Reducing memory\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "weather_train_df = reduce_mem_usage(weather_train_df)\n",
    "weather_test_df = reduce_mem_usage(weather_test_df)\n",
    "building_meta_df = reduce_mem_usage(building_meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_date_usage(train_df, meter=0, building_id=0):\n",
    "    train_temp_df = train_df[train_df['meter'] == meter]\n",
    "    train_temp_df = train_temp_df[train_temp_df['building_id'] == building_id]    \n",
    "    train_temp_df_meter = train_temp_df.groupby('date')['meter_reading_log1p'].sum()\n",
    "    train_temp_df_meter = train_temp_df_meter.to_frame().reset_index()\n",
    "    fig = px.line(train_temp_df_meter, x='date', y='meter_reading_log1p')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "  - There are 3 parts to make features <br>\n",
    "      train_df / weather_train_df / building_meta_df\n",
    "  - and then I will merge them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df -- timestamp : 월 , 주, 일\n",
    "\n",
    "train_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])\n",
    "# train_df['meter_reading']\n",
    "# date / 월 / 주 /일 \n",
    "train_df['date'] = train_df['timestamp'].dt.date\n",
    "train_df['hour'] = train_df['timestamp'].dt.hour\n",
    "train_df['weekend'] = train_df['timestamp'].dt.weekday\n",
    "train_df['month'] = train_df['timestamp'].dt.month\n",
    "train_df['dayofweek'] = train_df['timestamp'].dt.dayofweek\n",
    "\n",
    "\n",
    "test_df['date'] = test_df['timestamp'].dt.date\n",
    "test_df['hour'] = test_df['timestamp'].dt.hour\n",
    "test_df['weekend'] = test_df['timestamp'].dt.weekday\n",
    "test_df['month'] = test_df['timestamp'].dt.month\n",
    "test_df['dayofweek'] = test_df['timestamp'].dt.dayofweek\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isholiday\n",
    "holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                \"2019-01-01\"]\n",
    "\n",
    "train_df[\"is_holiday\"] = (train_df.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "test_df[\"is_holiday\"] = (test_df.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier 제거 \n",
    "# Remove outliers\n",
    "train_df = train_df [ train_df['building_id'] != 1099 ]\n",
    "train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summer / winter\n",
    "# # summer = chilledwater 사용량이 많은 달. \n",
    "\n",
    "# train_df.head()\n",
    "# # meter ==0 이 모두 0인 building id\n",
    "# tmp = train_df[train_df['meter']==1][train_df['meter_reading']>0].groupby(['building_id','month'])['meter_reading'].count().reset_index()\n",
    "# tmp[tmp['building_id']>1020][tmp['building_id']<1030]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id                   0\n",
       "timestamp                 0\n",
       "air_temperature          55\n",
       "cloud_coverage        69173\n",
       "dew_temperature         113\n",
       "precip_depth_1_hr     50289\n",
       "sea_level_pressure    10618\n",
       "wind_direction         6268\n",
       "wind_speed              304\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weather_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather - \n",
    "# weather data has a lot of nulls \n",
    "# I tried to fill these values by interpolating data\n",
    "# df.groupby('').apply(lambda group: group.interpolate~~)\n",
    "\n",
    "weather_train_df.head()\n",
    "weather_train_df = weather_train_df.groupby('site_id').apply\\\n",
    "                    (lambda group : group.interpolate(limit_direction='both'))\n",
    "weather_test_df = weather_test_df.groupby('site_id').apply\\\n",
    "                    (lambda group : group.interpolate(limit_direction='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id                   0\n",
       "timestamp                 0\n",
       "air_temperature           0\n",
       "cloud_coverage        17228\n",
       "dew_temperature           0\n",
       "precip_depth_1_hr     26273\n",
       "sea_level_pressure     8755\n",
       "wind_direction            0\n",
       "wind_speed                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weather_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags \n",
    "# site 별로 최근 3일간의 날씨를 rolling 하기\n",
    "def add_lag_feature(weather_df, window=3):\n",
    "    group_df = weather_df.groupby('site_id')\n",
    "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "    rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "    lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "    lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "    lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "    for col in cols:\n",
    "        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "        weather_df[f'{col}_std_lag{window}'] = lag_std[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_lag_feature(weather_train_df, window=3)\n",
    "add_lag_feature(weather_train_df, window=72)\n",
    "add_lag_feature(weather_test_df, window=3)\n",
    "add_lag_feature(weather_test_df, window=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>dew_temperature</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temperature_mean_lag3</th>\n",
       "      <th>...</th>\n",
       "      <th>sea_level_pressure_min_lag72</th>\n",
       "      <th>sea_level_pressure_std_lag72</th>\n",
       "      <th>wind_direction_mean_lag72</th>\n",
       "      <th>wind_direction_max_lag72</th>\n",
       "      <th>wind_direction_min_lag72</th>\n",
       "      <th>wind_direction_std_lag72</th>\n",
       "      <th>wind_speed_mean_lag72</th>\n",
       "      <th>wind_speed_max_lag72</th>\n",
       "      <th>wind_speed_min_lag72</th>\n",
       "      <th>wind_speed_std_lag72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>24.406250</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>24.703125</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.50000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>22.796875</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.288574</td>\n",
       "      <td>23.328125</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.40625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>21.093750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.59375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.765625</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2.599609</td>\n",
       "      <td>21.296875</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.31250</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>2.599609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.188477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_id            timestamp  air_temperature  cloud_coverage  \\\n",
       "0        0  2016-01-01 00:00:00        25.000000             6.0   \n",
       "1        0  2016-01-01 01:00:00        24.406250             4.0   \n",
       "2        0  2016-01-01 02:00:00        22.796875             2.0   \n",
       "3        0  2016-01-01 03:00:00        21.093750             2.0   \n",
       "4        0  2016-01-01 04:00:00        20.000000             2.0   \n",
       "\n",
       "   dew_temperature  precip_depth_1_hr  sea_level_pressure  wind_direction  \\\n",
       "0         20.00000               -1.0              1019.5             0.0   \n",
       "1         21.09375               -1.0              1020.0            70.0   \n",
       "2         21.09375                0.0              1020.0             0.0   \n",
       "3         20.59375                0.0              1020.0             0.0   \n",
       "4         20.00000               -1.0              1020.0           250.0   \n",
       "\n",
       "   wind_speed  air_temperature_mean_lag3  ...  sea_level_pressure_min_lag72  \\\n",
       "0    0.000000                  25.000000  ...                        1019.5   \n",
       "1    1.500000                  24.703125  ...                        1019.5   \n",
       "2    0.000000                  24.062500  ...                        1019.5   \n",
       "3    0.000000                  22.765625  ...                        1019.5   \n",
       "4    2.599609                  21.296875  ...                        1019.5   \n",
       "\n",
       "   sea_level_pressure_std_lag72  wind_direction_mean_lag72  \\\n",
       "0                           NaN                   0.000000   \n",
       "1                      0.353516                  35.000000   \n",
       "2                      0.288574                  23.328125   \n",
       "3                      0.250000                  17.500000   \n",
       "4                      0.223633                  64.000000   \n",
       "\n",
       "   wind_direction_max_lag72  wind_direction_min_lag72  \\\n",
       "0                       0.0                       0.0   \n",
       "1                      70.0                       0.0   \n",
       "2                      70.0                       0.0   \n",
       "3                      70.0                       0.0   \n",
       "4                     250.0                       0.0   \n",
       "\n",
       "   wind_direction_std_lag72  wind_speed_mean_lag72  wind_speed_max_lag72  \\\n",
       "0                       NaN               0.000000              0.000000   \n",
       "1                  49.50000               0.750000              1.500000   \n",
       "2                  40.40625               0.500000              1.500000   \n",
       "3                  35.00000               0.375000              1.500000   \n",
       "4                 108.31250               0.819824              2.599609   \n",
       "\n",
       "   wind_speed_min_lag72  wind_speed_std_lag72  \n",
       "0                   0.0                   NaN  \n",
       "1                   0.0              1.060547  \n",
       "2                   0.0              0.866211  \n",
       "3                   0.0              0.750000  \n",
       "4                   0.0              1.188477  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meter reading 값에 대한 aggregation\n",
    "# # 과적합 문제를 야기할 수 있다.\n",
    "# # meter 별로\n",
    "\n",
    "# train_df['key'] =  train_df['building_id'].astype(str) + '__' + train_df['meter'].astype(str)\n",
    "\n",
    "# df_group = train_df.groupby('key')['meter_reading_log1p']\n",
    "# building_mean = df_group.mean().astype(np.float16)\n",
    "# building_median = df_group.median().astype(np.float16)\n",
    "# building_min = df_group.min().astype(np.float16)\n",
    "# building_max = df_group.max().astype(np.float16)\n",
    "# building_std = df_group.std().astype(np.float16)\n",
    "\n",
    "# train_df['building_meter_mean'] = train_df['key'].map(building_mean)\n",
    "# train_df['building_meter_median'] = train_df['key'].map(building_median)\n",
    "# train_df['building_meter_min'] = train_df['key'].map(building_min)\n",
    "# train_df['building_meter_max'] = train_df['key'].map(building_max)\n",
    "# train_df['building_meter_std'] = train_df['key'].map(building_std)\n",
    "\n",
    "\n",
    "# test_df['building_meter_mean'] = train_df['key'].map(building_mean)\n",
    "# test_df['building_meter_median'] = train_df['key'].map(building_median)\n",
    "# test_df['building_meter_min'] = train_df['key'].map(building_min)\n",
    "# test_df['building_meter_max'] = train_df['key'].map(building_max)\n",
    "# test_df['building_meter_std'] = train_df['key'].map(building_std)\n",
    "# train_df = train_df.drop('key',axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing weired data on site_id =0  \n",
    "#https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113054#656588\n",
    "# building_meta_df[building_meta_df.site_id == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter_reading_log1p</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>23.303600</td>\n",
       "      <td>3.190624</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.318163</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>175.184006</td>\n",
       "      <td>5.171529</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>91.265297</td>\n",
       "      <td>4.524668</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     building_id  meter  timestamp  meter_reading  meter_reading_log1p  \\\n",
       "103          105      0 2016-01-01      23.303600             3.190624   \n",
       "104          106      0 2016-01-01       0.374600             0.318163   \n",
       "105          106      3 2016-01-01       0.000000             0.000000   \n",
       "106          107      0 2016-01-01     175.184006             5.171529   \n",
       "107          108      0 2016-01-01      91.265297             4.524668   \n",
       "\n",
       "           date  hour  weekend  month  dayofweek  is_holiday  \n",
       "103  2016-01-01     0        4      1          4           1  \n",
       "104  2016-01-01     0        4      1          4           1  \n",
       "105  2016-01-01     0        4      1          4           1  \n",
       "106  2016-01-01     0        4      1          4           1  \n",
       "107  2016-01-01     0        4      1          4           1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나에 합치기\n",
    "\n",
    "# base + building_meta_df 합치기\n",
    "train_df = pd.merge(train_df,building_meta_df, on= ['building_id'],how='left')\n",
    "test_df = pd.merge(test_df,building_meta_df, on= ['building_id'],how='left')\n",
    "# del building_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base + weather_train_df 합치기\n",
    "weather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "weather_test_df['timestamp'] = pd.to_datetime(weather_test_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "train_df = pd.merge(train_df,weather_train_df, on= ['site_id','timestamp'],how='left')\n",
    "test_df = pd.merge(test_df,weather_test_df, on= ['site_id','timestamp'],how='left')\n",
    "# del weather_train_df, weather_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoidng variables\n",
    "le = LabelEncoder()\n",
    "# train_df['primary_use'] = train_df['primary_use'].astype(str)\n",
    "train_df['primary_use'] = le.fit_transform(train_df['primary_use']).astype(np.int8)\n",
    "\n",
    "# test_df['primary_use'] = test_df['primary_use'].astype(str)\n",
    "test_df['primary_use'] = le.fit_transform(test_df['primary_use']).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickle 저장\n",
    "\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl')\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('train_df.pkl')\n",
    "test_df = pd.read_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some feature enginnering\n",
    "\n",
    "train_df['age'] = train_df['year_built'].max()-train_df['year_built']+1\n",
    "test_df['age'] = test_df['year_built'].max() - test_df['year_built'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values\n",
    "# To streamline this though process it is useful to know the 3 categories in which missing data can be classified into:\n",
    "\n",
    "# Missing Completely at Random (MCAR)\n",
    "# Missing at Random (MAR)\n",
    "# Missing Not at Random (MNAR)\n",
    "\n",
    "train_df['floor_count'] = train_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "test_df['floor_count'] = test_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['year_built'] = train_df['year_built'].fillna(-999).astype(np.int16)\n",
    "test_df['year_built'] = test_df['year_built'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['age'] = train_df['age'].fillna(-999).astype(np.int16)\n",
    "test_df['age'] = test_df['age'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['cloud_coverage'] = train_df['cloud_coverage'].fillna(-999).astype(np.int16)\n",
    "test_df['cloud_coverage'] = test_df['cloud_coverage'].fillna(-999).astype(np.int16) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['date',\"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\"timestamp\"]\n",
    "drop_cols = ['date',\"timestamp\"]\n",
    "\n",
    "target = train_df[\"meter_reading_log1p\"]\n",
    "del train_df[\"meter_reading\"], train_df['meter_reading_log1p']\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "drop_cols += [\"row_id\"]\n",
    "# drop_cols.remove('date')\n",
    "test_df = test_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"hour\", \"weekend\",'month','is_holiday','primary_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "seed = 99 #666\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "# lightbgm\n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'regression',\n",
    "#     'metric': {'l2'},\n",
    "# #     'subsample': 0.2,\n",
    "#     'learning_rate': 0.08,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'bagging_fraction': 0.9,\n",
    "# #     'alpha': 0.1,\n",
    "# #     'lambda': 0.1,\n",
    "# #     'n_jobs' :2 \n",
    "# }\n",
    "\n",
    "# params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting\": \"gbdt\",\n",
    "#     \"num_leaves\": 1280,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"feature_fraction\": 0.85,\n",
    "#     \"reg_lambda\": 2,\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"num_threads\" : 10,\n",
    "#     'seed' : seed\n",
    "    \n",
    "# }\n",
    "\n",
    "#  n_estimators=10000,\n",
    "#         max_depth=-1,\n",
    "#         learning_rate=0.048,\n",
    "#         subsample=0.85,\n",
    "#         colsample_bytree=0.85,\n",
    "#         missing=-999,\n",
    "#         tree_method='gpu_hist',  # THE MAGICAL PARAMETER\n",
    "#         reg_alpha=0.15,\n",
    "#         reg_lamdba=0.85\n",
    "#     )\n",
    "    \n",
    "params = {\n",
    "    'colsample_bytree': 0.8,                 \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.8,\n",
    "    'reg_alpha' :0.15,\n",
    "    'reg_lamdba' : 0.85,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'missing' : -999,    \n",
    "    'objective': 'reg:squarederror',\n",
    "#     'n_jobs' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_log_error\n",
    "# from multiprocessing import Process as proc\n",
    "# def xgbfitting(n):\n",
    "#     print(2)\n",
    "#     gbm = xgb.train(params, xgb_train, num_boost_round = 3000,evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n",
    "#                     verbose_eval = 30 , early_stopping_rounds = 30)\n",
    "#     print(1)\n",
    "#     gbm.save_model('xgb.model')\n",
    "# #     model.save_model('tmp/xgb.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yseon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:4.09277\tval-rmse:4.03497\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 30 rounds.\n",
      "[30]\ttrain-rmse:1.54774\tval-rmse:1.67393\n",
      "[60]\ttrain-rmse:1.12906\tval-rmse:1.33184\n",
      "[90]\ttrain-rmse:1.00773\tval-rmse:1.24555\n",
      "[120]\ttrain-rmse:0.939901\tval-rmse:1.20075\n",
      "[150]\ttrain-rmse:0.905979\tval-rmse:1.17893\n",
      "[180]\ttrain-rmse:0.877122\tval-rmse:1.1644\n",
      "[210]\ttrain-rmse:0.855618\tval-rmse:1.15625\n",
      "[240]\ttrain-rmse:0.831678\tval-rmse:1.14511\n",
      "[270]\ttrain-rmse:0.815483\tval-rmse:1.13824\n",
      "[300]\ttrain-rmse:0.79704\tval-rmse:1.13003\n",
      "[330]\ttrain-rmse:0.780804\tval-rmse:1.12251\n",
      "[360]\ttrain-rmse:0.764842\tval-rmse:1.11536\n",
      "[390]\ttrain-rmse:0.748067\tval-rmse:1.10655\n",
      "[420]\ttrain-rmse:0.732962\tval-rmse:1.10101\n",
      "[450]\ttrain-rmse:0.72119\tval-rmse:1.09651\n",
      "[480]\ttrain-rmse:0.711523\tval-rmse:1.09302\n",
      "[510]\ttrain-rmse:0.702501\tval-rmse:1.08997\n",
      "[540]\ttrain-rmse:0.693576\tval-rmse:1.08744\n",
      "[570]\ttrain-rmse:0.685494\tval-rmse:1.08561\n",
      "[600]\ttrain-rmse:0.677518\tval-rmse:1.08266\n",
      "[630]\ttrain-rmse:0.670205\tval-rmse:1.08102\n",
      "[660]\ttrain-rmse:0.664231\tval-rmse:1.0798\n",
      "[690]\ttrain-rmse:0.657737\tval-rmse:1.07897\n",
      "[720]\ttrain-rmse:0.650486\tval-rmse:1.07732\n",
      "[750]\ttrain-rmse:0.645651\tval-rmse:1.07688\n",
      "[780]\ttrain-rmse:0.640125\tval-rmse:1.07563\n",
      "[810]\ttrain-rmse:0.63549\tval-rmse:1.07447\n",
      "[840]\ttrain-rmse:0.630764\tval-rmse:1.07433\n",
      "[870]\ttrain-rmse:0.626061\tval-rmse:1.07363\n",
      "[900]\ttrain-rmse:0.621641\tval-rmse:1.07275\n",
      "[930]\ttrain-rmse:0.617386\tval-rmse:1.07238\n",
      "[960]\ttrain-rmse:0.612236\tval-rmse:1.07091\n",
      "[990]\ttrain-rmse:0.608068\tval-rmse:1.07018\n",
      "[1020]\ttrain-rmse:0.604508\tval-rmse:1.06943\n",
      "Stopping. Best iteration:\n",
      "[1015]\ttrain-rmse:0.605288\tval-rmse:1.06931\n",
      "\n",
      "delete\n",
      "[0]\ttrain-rmse:4.04718\tval-rmse:4.11622\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 30 rounds.\n",
      "[30]\ttrain-rmse:1.48568\tval-rmse:1.5844\n",
      "[60]\ttrain-rmse:1.06594\tval-rmse:1.23155\n",
      "[90]\ttrain-rmse:0.959226\tval-rmse:1.16448\n",
      "[120]\ttrain-rmse:0.906407\tval-rmse:1.13148\n",
      "[150]\ttrain-rmse:0.876684\tval-rmse:1.11593\n",
      "[180]\ttrain-rmse:0.855767\tval-rmse:1.10506\n",
      "[210]\ttrain-rmse:0.837048\tval-rmse:1.09614\n",
      "[240]\ttrain-rmse:0.817236\tval-rmse:1.08574\n",
      "[270]\ttrain-rmse:0.801357\tval-rmse:1.07982\n",
      "[300]\ttrain-rmse:0.784905\tval-rmse:1.07305\n",
      "[330]\ttrain-rmse:0.767846\tval-rmse:1.06581\n",
      "[360]\ttrain-rmse:0.752921\tval-rmse:1.0603\n",
      "[390]\ttrain-rmse:0.739979\tval-rmse:1.05458\n",
      "[420]\ttrain-rmse:0.729288\tval-rmse:1.05069\n",
      "[450]\ttrain-rmse:0.717743\tval-rmse:1.0472\n",
      "[480]\ttrain-rmse:0.704426\tval-rmse:1.04237\n",
      "[510]\ttrain-rmse:0.695387\tval-rmse:1.0406\n",
      "[540]\ttrain-rmse:0.685457\tval-rmse:1.03676\n",
      "[570]\ttrain-rmse:0.677921\tval-rmse:1.03447\n",
      "[600]\ttrain-rmse:0.671179\tval-rmse:1.03335\n",
      "[630]\ttrain-rmse:0.665128\tval-rmse:1.0322\n",
      "[660]\ttrain-rmse:0.658518\tval-rmse:1.03055\n",
      "[690]\ttrain-rmse:0.653016\tval-rmse:1.02941\n",
      "[720]\ttrain-rmse:0.646216\tval-rmse:1.02804\n",
      "[750]\ttrain-rmse:0.640672\tval-rmse:1.02714\n",
      "[780]\ttrain-rmse:0.634312\tval-rmse:1.02494\n",
      "[810]\ttrain-rmse:0.628712\tval-rmse:1.02399\n",
      "[840]\ttrain-rmse:0.623465\tval-rmse:1.02354\n",
      "[870]\ttrain-rmse:0.619714\tval-rmse:1.02257\n",
      "[900]\ttrain-rmse:0.614919\tval-rmse:1.02234\n",
      "[930]\ttrain-rmse:0.610313\tval-rmse:1.02167\n",
      "[960]\ttrain-rmse:0.605665\tval-rmse:1.02109\n",
      "[990]\ttrain-rmse:0.601516\tval-rmse:1.02033\n",
      "[1020]\ttrain-rmse:0.598627\tval-rmse:1.01975\n",
      "[1050]\ttrain-rmse:0.593909\tval-rmse:1.01935\n",
      "[1080]\ttrain-rmse:0.589972\tval-rmse:1.01908\n",
      "[1110]\ttrain-rmse:0.586401\tval-rmse:1.01901\n",
      "Stopping. Best iteration:\n",
      "[1102]\ttrain-rmse:0.587571\tval-rmse:1.01878\n",
      "\n",
      "delete\n",
      "[0]\ttrain-rmse:4.07366\tval-rmse:4.07301\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 30 rounds.\n",
      "[30]\ttrain-rmse:1.53418\tval-rmse:1.61839\n",
      "[60]\ttrain-rmse:1.1029\tval-rmse:1.27461\n",
      "[90]\ttrain-rmse:0.97721\tval-rmse:1.20087\n",
      "[120]\ttrain-rmse:0.908808\tval-rmse:1.16921\n",
      "[150]\ttrain-rmse:0.874851\tval-rmse:1.15729\n",
      "[180]\ttrain-rmse:0.850375\tval-rmse:1.15065\n",
      "[210]\ttrain-rmse:0.825216\tval-rmse:1.14357\n",
      "[240]\ttrain-rmse:0.809887\tval-rmse:1.14026\n",
      "[270]\ttrain-rmse:0.789591\tval-rmse:1.13293\n",
      "[300]\ttrain-rmse:0.775509\tval-rmse:1.12779\n",
      "[330]\ttrain-rmse:0.760012\tval-rmse:1.12442\n",
      "[360]\ttrain-rmse:0.746315\tval-rmse:1.1239\n",
      "[390]\ttrain-rmse:0.73266\tval-rmse:1.11984\n",
      "[420]\ttrain-rmse:0.719907\tval-rmse:1.11669\n",
      "[450]\ttrain-rmse:0.709171\tval-rmse:1.11491\n",
      "[480]\ttrain-rmse:0.699253\tval-rmse:1.11249\n",
      "[510]\ttrain-rmse:0.688005\tval-rmse:1.11055\n",
      "[540]\ttrain-rmse:0.679447\tval-rmse:1.10894\n",
      "[570]\ttrain-rmse:0.673018\tval-rmse:1.10861\n",
      "[600]\ttrain-rmse:0.665926\tval-rmse:1.10729\n",
      "[630]\ttrain-rmse:0.657369\tval-rmse:1.10561\n",
      "[660]\ttrain-rmse:0.648796\tval-rmse:1.10449\n",
      "[690]\ttrain-rmse:0.643161\tval-rmse:1.10402\n",
      "[720]\ttrain-rmse:0.638331\tval-rmse:1.10365\n",
      "[750]\ttrain-rmse:0.631931\tval-rmse:1.10362\n",
      "[780]\ttrain-rmse:0.62693\tval-rmse:1.10289\n",
      "[810]\ttrain-rmse:0.621385\tval-rmse:1.10255\n",
      "[840]\ttrain-rmse:0.615526\tval-rmse:1.10224\n",
      "[870]\ttrain-rmse:0.611609\tval-rmse:1.10221\n",
      "[900]\ttrain-rmse:0.607106\tval-rmse:1.10178\n",
      "Stopping. Best iteration:\n",
      "[897]\ttrain-rmse:0.607318\tval-rmse:1.10173\n",
      "\n",
      "delete\n",
      "Wall time: 24min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "# shuffle = False\n",
    "kf = KFold(n_splits=folds, shuffle=False, random_state=seed)\n",
    "scores = [] \n",
    "models = []\n",
    "for i,(train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "    train_X = train_df.iloc[train_index]\n",
    "    val_X = train_df.iloc[val_index]\n",
    "    train_y = target.iloc[train_index]\n",
    "    val_y = target.iloc[val_index]\n",
    "#     lgb_train = lgb.Dataset(train_X, train_y,categorical_feature=categorical_features)\n",
    "#     lgb_eval = lgb.Dataset(val_X, val_y,categorical_feature=categorical_features)\n",
    "    xgb_train = xgb.DMatrix(train_X, train_y)\n",
    "    xgb_eval = xgb.DMatrix(val_X, val_y)\n",
    "#     gbm = lgb.train(params,\n",
    "#                     lgb_train,\n",
    "#                     num_boost_round=1000, #300,\n",
    "#                     valid_sets=(lgb_train, lgb_eval),\n",
    "# #                     feval=rmsle,\n",
    "#                     early_stopping_rounds= 50,#100,\n",
    "#                     verbose_eval=30) #100)\n",
    "    gbm = xgb.train(params, xgb_train, num_boost_round = 3000,evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n",
    "                    verbose_eval = 30 , early_stopping_rounds = 30)\n",
    "#     print(11)\n",
    "     \n",
    "#     fitting_process = proc( target = xgbfitting, args = (i,) )\n",
    "#     fitting_process.start()\n",
    "#     fitting_process.join()\n",
    "#     gbm.save_model('xgb_%s.model'%i)\n",
    "    # save\n",
    "    pickle.dump(gbm,open('xgb_%s.pickle'%i, \"wb\") )\n",
    "    gbm.__del__() \n",
    "    print('delete')\n",
    "#     print(gbm)\n",
    "    del xgb_train, xgb_eval\n",
    "    gc.collect()\n",
    "    # CV Score 만들기\n",
    "#     scores.append(gbm.predict(val_X))\n",
    "    #MSLE\n",
    "#scores.append( np.sqrt(mean_squared_log_error( np.expm1(val_y), np.expm1(predictions) )))\n",
    "    \n",
    "#     models.append(gbm)\n",
    "#     del gbm\n",
    "for i in range(folds):\n",
    "    models.append(pickle.load(open(\"xgb_%s.pickle\"%i, \"rb\")))²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# gbm = models[-1]\n",
    "# feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance(), gbm.feature_name()),reverse = True), columns=['Value','Feature'])\n",
    "# plt.figure(figsize=(10, 15))\n",
    "\n",
    "# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "# # plt.title('LightGBM FEATURES')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm.get_score(importance_type='gain')#,columns = ['Feature','Value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 834/834 [06:29<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "res = []\n",
    "res2 = []\n",
    "res3 = []\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test_df.shape[0] / 50000)))):\n",
    "#     # 평균을 내고 나서 exp를 취하는 코드\n",
    "#     res.append(np.expm1(sum([model.predict(test_df.iloc[i:i + step_size]) for model in models]) / folds))\n",
    "    # exp를 취하고 평균을 내는 코드 https://www.kaggle.com/rohanrao/ashrae-half-and-half\n",
    "    res2.append(sum([np.expm1(model.predict(xgb.DMatrix(test_df.iloc[i:i + step_size]))) for model in models])/ folds)\n",
    "#     res3.append([np.expm1(model.predict(test_df.iloc[i:i + step_size])) for model in models])\n",
    "    \n",
    "    i += step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>97.115349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>44.861187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.534665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>172.153442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1048.248291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.428292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>50.771851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>579.505066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>272.677948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>241.016922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  meter_reading\n",
       "0       0      97.115349\n",
       "1       1      44.861187\n",
       "2       2       3.534665\n",
       "3       3     172.153442\n",
       "4       4    1048.248291\n",
       "5       5       6.428292\n",
       "6       6      50.771851\n",
       "7       7     579.505066\n",
       "8       8     272.677948\n",
       "9       9     241.016922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "res2 = np.concatenate(res2)\n",
    "sample_submission[\"meter_reading\"] = res2\n",
    "sample_submission.loc[sample_submission['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "sample_submission.to_csv('xgb_exp_avg_sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 최근 데이터에 가중치를 더 주는 코드 \n",
    "# res4 = []\n",
    "# for i in range(len(res3)):\n",
    "#     res4.append((0.6*res3[i][0]+0.3*res3[i][1]+0.1*res3[i][2]))\n",
    "\n",
    "# res4 = np.concatenate(res4)\n",
    "# sample_submission[\"meter_reading\"] = res4\n",
    "# sample_submission.loc[sample_submission['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "# sample_submission.to_csv('exp_avg_recently_sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', index=False)\n",
    "# sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
