{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 types of energy to predict <br>\n",
    "    - 0 : electricity\n",
    "    - 1 : chilledwater\n",
    "    - 2 : steam\n",
    "    - 3 : hotwater\n",
    "\n",
    "Electricity and water consumption may have different behavior!<br>\n",
    "     - I will make separately train & predict the model\n",
    "\n",
    "* Reference \n",
    " - https://www.kaggle.com/corochann/ashrae-training-lgbm-by-meter-type\n",
    " - https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No leak data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 파일 읽어오기\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "weather_train_df = pd.read_csv('weather_train.csv')\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "weather_test_df = pd.read_csv('weather_test.csv')\n",
    "building_meta_df = pd.read_csv('building_metadata.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20216100, 4)\n",
      "(139773, 9)\n",
      "(277243, 9)\n",
      "(1449, 6)\n",
      "(41697600, 4)\n"
     ]
    }
   ],
   "source": [
    "# Glimpse of Data\n",
    "print(train_df.shape)\n",
    "print(weather_train_df.shape)\n",
    "print(weather_test_df.shape)\n",
    "print(building_meta_df.shape)\n",
    "\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ㅒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  3.07 Mb (68.1% reduction)\n",
      "Mem. usage decreased to  6.08 Mb (68.1% reduction)\n",
      "Mem. usage decreased to  0.03 Mb (60.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Reducing memory\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "weather_train_df = reduce_mem_usage(weather_train_df)\n",
    "weather_test_df = reduce_mem_usage(weather_test_df)\n",
    "building_meta_df = reduce_mem_usage(building_meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_date_usage(train_df, meter=0, building_id=0):\n",
    "    train_temp_df = train_df[train_df['meter'] == meter]\n",
    "    train_temp_df = train_temp_df[train_temp_df['building_id'] == building_id]    \n",
    "    train_temp_df_meter = train_temp_df.groupby('date')['meter_reading_log1p'].sum()\n",
    "    train_temp_df_meter = train_temp_df_meter.to_frame().reset_index()\n",
    "    fig = px.line(train_temp_df_meter, x='date', y='meter_reading_log1p')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "  - There are 3 parts to make features <br>\n",
    "      train_df / weather_train_df / building_meta_df\n",
    "  - and then I will merge them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df -- timestamp : 월 , 주, 일\n",
    "\n",
    "train_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])\n",
    "# train_df['meter_reading']\n",
    "# date / 월 / 주 /일 \n",
    "train_df['date'] = train_df['timestamp'].dt.date\n",
    "train_df['hour'] = train_df['timestamp'].dt.hour\n",
    "train_df['weekend'] = train_df['timestamp'].dt.weekday\n",
    "train_df['month'] = train_df['timestamp'].dt.month\n",
    "train_df['dayofweek'] = train_df['timestamp'].dt.dayofweek\n",
    "\n",
    "\n",
    "test_df['date'] = test_df['timestamp'].dt.date\n",
    "test_df['hour'] = test_df['timestamp'].dt.hour\n",
    "test_df['weekend'] = test_df['timestamp'].dt.weekday\n",
    "test_df['month'] = test_df['timestamp'].dt.month\n",
    "test_df['dayofweek'] = test_df['timestamp'].dt.dayofweek\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isholiday\n",
    "holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                \"2019-01-01\"]\n",
    "\n",
    "train_df[\"is_holiday\"] = (train_df.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "test_df[\"is_holiday\"] = (test_df.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier 제거 \n",
    "# Remove outliers\n",
    "train_df = train_df [ train_df['building_id'] != 1099 ]\n",
    "train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summer / winter\n",
    "# # summer = chilledwater 사용량이 많은 달. \n",
    "\n",
    "# train_df.head()\n",
    "# # meter ==0 이 모두 0인 building id\n",
    "# tmp = train_df[train_df['meter']==1][train_df['meter_reading']>0].groupby(['building_id','month'])['meter_reading'].count().reset_index()\n",
    "# tmp[tmp['building_id']>1020][tmp['building_id']<1030]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id                   0\n",
       "timestamp                 0\n",
       "air_temperature          55\n",
       "cloud_coverage        69173\n",
       "dew_temperature         113\n",
       "precip_depth_1_hr     50289\n",
       "sea_level_pressure    10618\n",
       "wind_direction         6268\n",
       "wind_speed              304\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weather_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather - \n",
    "# weather data has a lot of nulls \n",
    "# I tried to fill these values by interpolating data\n",
    "# df.groupby('').apply(lambda group: group.interpolate~~)\n",
    "\n",
    "weather_train_df.head()\n",
    "weather_train_df = weather_train_df.groupby('site_id').apply\\\n",
    "                    (lambda group : group.interpolate(limit_direction='both'))\n",
    "weather_test_df = weather_test_df.groupby('site_id').apply\\\n",
    "                    (lambda group : group.interpolate(limit_direction='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id                   0\n",
       "timestamp                 0\n",
       "air_temperature           0\n",
       "cloud_coverage        17228\n",
       "dew_temperature           0\n",
       "precip_depth_1_hr     26273\n",
       "sea_level_pressure     8755\n",
       "wind_direction            0\n",
       "wind_speed                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weather_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags \n",
    "# site 별로 최근 3일간의 날씨를 rolling 하기\n",
    "def add_lag_feature(weather_df, window=3):\n",
    "    group_df = weather_df.groupby('site_id')\n",
    "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "    rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "    lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "    lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "    lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "    for col in cols:\n",
    "        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "        weather_df[f'{col}_std_lag{window}'] = lag_std[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_lag_feature(weather_train_df, window=3)\n",
    "add_lag_feature(weather_train_df, window=72)\n",
    "add_lag_feature(weather_test_df, window=3)\n",
    "add_lag_feature(weather_test_df, window=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>dew_temperature</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temperature_mean_lag3</th>\n",
       "      <th>...</th>\n",
       "      <th>sea_level_pressure_min_lag72</th>\n",
       "      <th>sea_level_pressure_std_lag72</th>\n",
       "      <th>wind_direction_mean_lag72</th>\n",
       "      <th>wind_direction_max_lag72</th>\n",
       "      <th>wind_direction_min_lag72</th>\n",
       "      <th>wind_direction_std_lag72</th>\n",
       "      <th>wind_speed_mean_lag72</th>\n",
       "      <th>wind_speed_max_lag72</th>\n",
       "      <th>wind_speed_min_lag72</th>\n",
       "      <th>wind_speed_std_lag72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>24.406250</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>24.703125</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.50000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>22.796875</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.288574</td>\n",
       "      <td>23.328125</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.40625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>21.093750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.59375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.765625</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2.599609</td>\n",
       "      <td>21.296875</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.31250</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>2.599609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.188477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_id            timestamp  air_temperature  cloud_coverage  \\\n",
       "0        0  2016-01-01 00:00:00        25.000000             6.0   \n",
       "1        0  2016-01-01 01:00:00        24.406250             4.0   \n",
       "2        0  2016-01-01 02:00:00        22.796875             2.0   \n",
       "3        0  2016-01-01 03:00:00        21.093750             2.0   \n",
       "4        0  2016-01-01 04:00:00        20.000000             2.0   \n",
       "\n",
       "   dew_temperature  precip_depth_1_hr  sea_level_pressure  wind_direction  \\\n",
       "0         20.00000               -1.0              1019.5             0.0   \n",
       "1         21.09375               -1.0              1020.0            70.0   \n",
       "2         21.09375                0.0              1020.0             0.0   \n",
       "3         20.59375                0.0              1020.0             0.0   \n",
       "4         20.00000               -1.0              1020.0           250.0   \n",
       "\n",
       "   wind_speed  air_temperature_mean_lag3  ...  sea_level_pressure_min_lag72  \\\n",
       "0    0.000000                  25.000000  ...                        1019.5   \n",
       "1    1.500000                  24.703125  ...                        1019.5   \n",
       "2    0.000000                  24.062500  ...                        1019.5   \n",
       "3    0.000000                  22.765625  ...                        1019.5   \n",
       "4    2.599609                  21.296875  ...                        1019.5   \n",
       "\n",
       "   sea_level_pressure_std_lag72  wind_direction_mean_lag72  \\\n",
       "0                           NaN                   0.000000   \n",
       "1                      0.353516                  35.000000   \n",
       "2                      0.288574                  23.328125   \n",
       "3                      0.250000                  17.500000   \n",
       "4                      0.223633                  64.000000   \n",
       "\n",
       "   wind_direction_max_lag72  wind_direction_min_lag72  \\\n",
       "0                       0.0                       0.0   \n",
       "1                      70.0                       0.0   \n",
       "2                      70.0                       0.0   \n",
       "3                      70.0                       0.0   \n",
       "4                     250.0                       0.0   \n",
       "\n",
       "   wind_direction_std_lag72  wind_speed_mean_lag72  wind_speed_max_lag72  \\\n",
       "0                       NaN               0.000000              0.000000   \n",
       "1                  49.50000               0.750000              1.500000   \n",
       "2                  40.40625               0.500000              1.500000   \n",
       "3                  35.00000               0.375000              1.500000   \n",
       "4                 108.31250               0.819824              2.599609   \n",
       "\n",
       "   wind_speed_min_lag72  wind_speed_std_lag72  \n",
       "0                   0.0                   NaN  \n",
       "1                   0.0              1.060547  \n",
       "2                   0.0              0.866211  \n",
       "3                   0.0              0.750000  \n",
       "4                   0.0              1.188477  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meter reading 값에 대한 aggregation\n",
    "# # 과적합 문제를 야기할 수 있다.\n",
    "# # meter 별로\n",
    "\n",
    "# train_df['key'] =  train_df['building_id'].astype(str) + '__' + train_df['meter'].astype(str)\n",
    "\n",
    "# df_group = train_df.groupby('key')['meter_reading_log1p']\n",
    "# building_mean = df_group.mean().astype(np.float16)\n",
    "# building_median = df_group.median().astype(np.float16)\n",
    "# building_min = df_group.min().astype(np.float16)\n",
    "# building_max = df_group.max().astype(np.float16)\n",
    "# building_std = df_group.std().astype(np.float16)\n",
    "\n",
    "# train_df['building_meter_mean'] = train_df['key'].map(building_mean)\n",
    "# train_df['building_meter_median'] = train_df['key'].map(building_median)\n",
    "# train_df['building_meter_min'] = train_df['key'].map(building_min)\n",
    "# train_df['building_meter_max'] = train_df['key'].map(building_max)\n",
    "# train_df['building_meter_std'] = train_df['key'].map(building_std)\n",
    "\n",
    "\n",
    "# test_df['building_meter_mean'] = train_df['key'].map(building_mean)\n",
    "# test_df['building_meter_median'] = train_df['key'].map(building_median)\n",
    "# test_df['building_meter_min'] = train_df['key'].map(building_min)\n",
    "# test_df['building_meter_max'] = train_df['key'].map(building_max)\n",
    "# test_df['building_meter_std'] = train_df['key'].map(building_std)\n",
    "# train_df = train_df.drop('key',axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing weired data on site_id =0  \n",
    "#https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113054#656588\n",
    "# building_meta_df[building_meta_df.site_id == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter_reading_log1p</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>23.303600</td>\n",
       "      <td>3.190624</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.318163</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>175.184006</td>\n",
       "      <td>5.171529</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>91.265297</td>\n",
       "      <td>4.524668</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     building_id  meter  timestamp  meter_reading  meter_reading_log1p  \\\n",
       "103          105      0 2016-01-01      23.303600             3.190624   \n",
       "104          106      0 2016-01-01       0.374600             0.318163   \n",
       "105          106      3 2016-01-01       0.000000             0.000000   \n",
       "106          107      0 2016-01-01     175.184006             5.171529   \n",
       "107          108      0 2016-01-01      91.265297             4.524668   \n",
       "\n",
       "           date  hour  weekend  month  dayofweek  is_holiday  \n",
       "103  2016-01-01     0        4      1          4           1  \n",
       "104  2016-01-01     0        4      1          4           1  \n",
       "105  2016-01-01     0        4      1          4           1  \n",
       "106  2016-01-01     0        4      1          4           1  \n",
       "107  2016-01-01     0        4      1          4           1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나에 합치기\n",
    "\n",
    "# base + building_meta_df 합치기\n",
    "train_df = pd.merge(train_df,building_meta_df, on= ['building_id'],how='left')\n",
    "test_df = pd.merge(test_df,building_meta_df, on= ['building_id'],how='left')\n",
    "# del building_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base + weather_train_df 합치기\n",
    "weather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "weather_test_df['timestamp'] = pd.to_datetime(weather_test_df['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "train_df = pd.merge(train_df,weather_train_df, on= ['site_id','timestamp'],how='left')\n",
    "test_df = pd.merge(test_df,weather_test_df, on= ['site_id','timestamp'],how='left')\n",
    "# del weather_train_df, weather_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoidng variables\n",
    "le = LabelEncoder()\n",
    "# train_df['primary_use'] = train_df['primary_use'].astype(str)\n",
    "train_df['primary_use'] = le.fit_transform(train_df['primary_use']).astype(np.int8)\n",
    "\n",
    "# test_df['primary_use'] = test_df['primary_use'].astype(str)\n",
    "test_df['primary_use'] = le.fit_transform(test_df['primary_use']).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickle 저장\n",
    "\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl')\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('train_df.pkl')\n",
    "test_df = pd.read_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some feature enginnering\n",
    "\n",
    "train_df['age'] = train_df['year_built'].max()-train_df['year_built']+1\n",
    "test_df['age'] = test_df['year_built'].max() - test_df['year_built'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values\n",
    "# To streamline this though process it is useful to know the 3 categories in which missing data can be classified into:\n",
    "\n",
    "# Missing Completely at Random (MCAR)\n",
    "# Missing at Random (MAR)\n",
    "# Missing Not at Random (MNAR)\n",
    "\n",
    "train_df['floor_count'] = train_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "test_df['floor_count'] = test_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['year_built'] = train_df['year_built'].fillna(-999).astype(np.int16)\n",
    "test_df['year_built'] = test_df['year_built'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['age'] = train_df['age'].fillna(-999).astype(np.int16)\n",
    "test_df['age'] = test_df['age'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['cloud_coverage'] = train_df['cloud_coverage'].fillna(-999).astype(np.int16)\n",
    "test_df['cloud_coverage'] = test_df['cloud_coverage'].fillna(-999).astype(np.int16) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['date',\"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\"timestamp\"]\n",
    "drop_cols = ['date',\"timestamp\"]\n",
    "\n",
    "target = train_df[\"meter_reading_log1p\"]\n",
    "del train_df[\"meter_reading\"], train_df['meter_reading_log1p']\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "drop_cols += [\"row_id\"]\n",
    "# drop_cols.remove('date')\n",
    "test_df = test_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"hour\", \"weekend\",'month','is_holiday','primary_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "seed = 99 #666\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "params =    {     #'iterations' : 2000,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.05,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'loss_function': 'RMSE',\n",
    "        'random_seed': seed,\n",
    "        'metric_period': 10,\n",
    "        'task_type': 'GPU',\n",
    "        #'subsample' : 0.8,\n",
    "        'depth': 8,\n",
    "    }\n",
    "\n",
    "# params = {\n",
    "#     'colsample_bytree': 0.8,                 \n",
    "#     'learning_rate': 0.05,\n",
    "#     'max_depth': 10,\n",
    "#     'subsample': 0.8,\n",
    "#     'reg_alpha' :0.15,\n",
    "#     'reg_lamdba' : 0.85,\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'missing' : -999,    \n",
    "#     'objective': 'reg:squarederror',\n",
    "# #     'n_jobs' : 4\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0:\tlearn: 2.0121124\ttest: 2.0771303\tbest: 2.0771303 (0)\ttotal: 1.45s\tremaining: 48m 13s\n",
      "30:\tlearn: 1.2091420\ttest: 1.6573238\tbest: 1.6573238 (30)\ttotal: 39.5s\tremaining: 41m 48s\n",
      "60:\tlearn: 1.0922874\ttest: 1.5226971\tbest: 1.5226971 (60)\ttotal: 1m 14s\tremaining: 39m 24s\n",
      "90:\tlearn: 1.0452033\ttest: 1.4640413\tbest: 1.4640413 (90)\ttotal: 1m 50s\tremaining: 38m 41s\n",
      "120:\tlearn: 1.0133084\ttest: 1.4340180\tbest: 1.4340180 (120)\ttotal: 2m 26s\tremaining: 37m 52s\n",
      "150:\tlearn: 0.9891869\ttest: 1.4183901\tbest: 1.4183901 (150)\ttotal: 3m 2s\tremaining: 37m 17s\n",
      "180:\tlearn: 0.9704491\ttest: 1.4010868\tbest: 1.4010868 (180)\ttotal: 3m 39s\tremaining: 36m 50s\n",
      "210:\tlearn: 0.9543736\ttest: 1.3857823\tbest: 1.3857823 (210)\ttotal: 4m 20s\tremaining: 36m 46s\n",
      "240:\tlearn: 0.9401696\ttest: 1.3738499\tbest: 1.3738499 (240)\ttotal: 4m 58s\tremaining: 36m 21s\n",
      "270:\tlearn: 0.9288641\ttest: 1.3654981\tbest: 1.3654981 (270)\ttotal: 5m 38s\tremaining: 35m 57s\n",
      "300:\tlearn: 0.9179700\ttest: 1.3584556\tbest: 1.3584253 (298)\ttotal: 6m 16s\tremaining: 35m 23s\n",
      "330:\tlearn: 0.9080995\ttest: 1.3504610\tbest: 1.3504524 (329)\ttotal: 6m 53s\tremaining: 34m 45s\n",
      "360:\tlearn: 0.8986903\ttest: 1.3434098\tbest: 1.3434098 (360)\ttotal: 7m 32s\tremaining: 34m 13s\n",
      "390:\tlearn: 0.8901571\ttest: 1.3378007\tbest: 1.3378007 (390)\ttotal: 8m 10s\tremaining: 33m 39s\n",
      "420:\tlearn: 0.8831689\ttest: 1.3298601\tbest: 1.3298601 (420)\ttotal: 8m 48s\tremaining: 33m 2s\n",
      "450:\tlearn: 0.8761722\ttest: 1.3266592\tbest: 1.3266592 (450)\ttotal: 9m 28s\tremaining: 32m 32s\n",
      "480:\tlearn: 0.8695048\ttest: 1.3222439\tbest: 1.3222439 (480)\ttotal: 10m 7s\tremaining: 31m 58s\n",
      "510:\tlearn: 0.8634654\ttest: 1.3170016\tbest: 1.3170016 (510)\ttotal: 10m 45s\tremaining: 31m 20s\n",
      "540:\tlearn: 0.8569254\ttest: 1.3133740\tbest: 1.3133740 (540)\ttotal: 11m 24s\tremaining: 30m 45s\n",
      "570:\tlearn: 0.8511475\ttest: 1.3087144\tbest: 1.3087144 (570)\ttotal: 12m 2s\tremaining: 30m 7s\n",
      "600:\tlearn: 0.8456637\ttest: 1.3048806\tbest: 1.3035424 (598)\ttotal: 12m 39s\tremaining: 29m 29s\n",
      "630:\tlearn: 0.8411505\ttest: 1.3026650\tbest: 1.3025461 (628)\ttotal: 13m 18s\tremaining: 28m 52s\n",
      "660:\tlearn: 0.8364550\ttest: 1.2997351\tbest: 1.2992642 (657)\ttotal: 13m 56s\tremaining: 28m 14s\n",
      "690:\tlearn: 0.8318877\ttest: 1.2942035\tbest: 1.2942035 (690)\ttotal: 14m 34s\tremaining: 27m 36s\n",
      "720:\tlearn: 0.8274795\ttest: 1.2909287\tbest: 1.2909212 (719)\ttotal: 15m 12s\tremaining: 26m 58s\n",
      "750:\tlearn: 0.8231568\ttest: 1.2903688\tbest: 1.2892894 (733)\ttotal: 15m 51s\tremaining: 26m 23s\n",
      "780:\tlearn: 0.8187001\ttest: 1.2855043\tbest: 1.2855043 (780)\ttotal: 16m 31s\tremaining: 25m 47s\n",
      "810:\tlearn: 0.8150581\ttest: 1.2850418\tbest: 1.2840704 (802)\ttotal: 17m 11s\tremaining: 25m 12s\n",
      "bestTest = 1.284070427\n",
      "bestIteration = 802\n",
      "Shrink model to first 803 iterations.\n",
      "0\n",
      "1\n",
      "0:\tlearn: 2.0302448\ttest: 2.0463851\tbest: 2.0463851 (0)\ttotal: 1.38s\tremaining: 45m 57s\n",
      "30:\tlearn: 1.1830140\ttest: 1.5975121\tbest: 1.5975121 (30)\ttotal: 39.2s\tremaining: 41m 27s\n",
      "60:\tlearn: 1.0638208\ttest: 1.4961280\tbest: 1.4961280 (60)\ttotal: 1m 17s\tremaining: 41m 14s\n",
      "90:\tlearn: 1.0226271\ttest: 1.4583917\tbest: 1.4571895 (89)\ttotal: 1m 54s\tremaining: 40m 1s\n",
      "120:\tlearn: 0.9933942\ttest: 1.4176946\tbest: 1.4176946 (120)\ttotal: 2m 31s\tremaining: 39m 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# shuffle = False\n",
    "kf = KFold(n_splits=folds, shuffle=False, random_state=seed)\n",
    "scores = [] \n",
    "models = []\n",
    "for i,(train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "    train_X = train_df.iloc[train_index]\n",
    "    val_X = train_df.iloc[val_index]\n",
    "    train_y = target.iloc[train_index]\n",
    "    val_y = target.iloc[val_index]\n",
    "    \n",
    "    gbm = CatBoostRegressor(**params)\n",
    "    print(i)\n",
    "    gbm.fit(\n",
    "            train_X, train_y,\n",
    "             eval_set=(val_X, val_y),\n",
    "            early_stopping_rounds = 30,\n",
    "            cat_features=categorical_features,\n",
    "            verbose=30)\n",
    "    print(i)\n",
    "    \n",
    "#     gbm = xgb.train(params, xgb_train, num_boost_round = 3000,evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n",
    "#                     verbose_eval = 30 , early_stopping_rounds = 30)\n",
    "#     print(11)\n",
    "     \n",
    "#     fitting_process = proc( target = xgbfitting, args = (i,) )\n",
    "#     fitting_process.start()\n",
    "#     fitting_process.join()\n",
    "    pickle.dump(gbm,open('catboost_%s.pickle'%i, \"wb\") )\n",
    "    del gbm\n",
    "#     gbm.__del__() \n",
    "#     print('delete')\n",
    "#     print(gbm)\n",
    "#     del xgb_train, xgb_eval\n",
    "    gc.collect()\n",
    "    # CV Score 만들기\n",
    "#     scores.append(gbm.predict(val_X))\n",
    "    #MSLE\n",
    "#scores.append( np.sqrt(mean_squared_log_error( np.expm1(val_y), np.expm1(predictions) )))\n",
    "    \n",
    "#     models.append(gbm)\n",
    "#     del gbm\n",
    "for i in range(folds):\n",
    "    models.append(pickle.load(open(\"catboost_%s.pickle\"%i, \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# gbm = models[-1]\n",
    "# feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance(), gbm.feature_name()),reverse = True), columns=['Value','Feature'])\n",
    "# plt.figure(figsize=(10, 15))\n",
    "\n",
    "# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "# # plt.title('LightGBM FEATURES')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 834/834 [46:52<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "res = []\n",
    "res2 = []\n",
    "res3 = []\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test_df.shape[0] / 50000)))):\n",
    "#     # 평균을 내고 나서 exp를 취하는 코드\n",
    "#     res.append(np.expm1(sum([model.predict(test_df.iloc[i:i + step_size]) for model in models]) / folds))\n",
    "    # exp를 취하고 평균을 내는 코드 https://www.kaggle.com/rohanrao/ashrae-half-and-half\n",
    "    res2.append(sum([np.expm1(model.predict(test_df.iloc[i:i + step_size])) for model in models])/ folds)\n",
    "#     res3.append([np.expm1(model.predict(test_df.iloc[i:i + step_size])) for model in models])\n",
    "    \n",
    "    i += step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from datetime import datetime\n",
    "\n",
    "# res = np.concatenate(res)\n",
    "# sample_submission[\"meter_reading\"] = res\n",
    "# sample_submission.loc[sample_submission['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "# sample_submission.to_csv('avg_exp_sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', index=False)\n",
    "# sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 54s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>209.580186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>103.376081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.735707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>241.058419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>526.005096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>13.445991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>86.171745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>342.278741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>229.088741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>302.297384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  meter_reading\n",
       "0       0     209.580186\n",
       "1       1     103.376081\n",
       "2       2      12.735707\n",
       "3       3     241.058419\n",
       "4       4     526.005096\n",
       "5       5      13.445991\n",
       "6       6      86.171745\n",
       "7       7     342.278741\n",
       "8       8     229.088741\n",
       "9       9     302.297384"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "res2 = np.concatenate(res2)\n",
    "sample_submission[\"meter_reading\"] = res2\n",
    "sample_submission.loc[sample_submission['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "sample_submission.to_csv('catboost_exp_avg_sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 최근 데이터에 가중치를 더 주는 코드 \n",
    "# res4 = []\n",
    "# for i in range(len(res3)):\n",
    "#     res4.append((0.6*res3[i][0]+0.3*res3[i][1]+0.1*res3[i][2]))\n",
    "\n",
    "# res4 = np.concatenate(res4)\n",
    "# sample_submission[\"meter_reading\"] = res4\n",
    "# sample_submission.loc[sample_submission['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "# sample_submission.to_csv('exp_avg_recently_sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', index=False)\n",
    "# sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
